[{"authors":["group"],"categories":null,"content":"We are a collocation of collaborators working on a diverse range of topics in computational linguistics, natural language processing and machine learning.\nLab Motto: We put the fun in funicular!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3d4d5e14ee766925debc0cd6588cd11b","permalink":"https://rycolab.io/authors/group/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/group/","section":"authors","summary":"We are a collocation of collaborators working on a diverse range of topics in computational linguistics, natural language processing and machine learning.\nLab Motto: We put the fun in funicular!","tags":null,"title":"","type":"authors"},{"authors":["adina"],"categories":null,"content":"Animal Form: Numbat\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bfc0fd43f26e67f5f46c4bc0baf09b8e","permalink":"https://rycolab.io/authors/adina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/adina/","section":"authors","summary":"Animal Form: Numbat\n-- ","tags":null,"title":"Adina Williams","type":"authors"},{"authors":["afra"],"categories":null,"content":"Afra is a first-year doctoral fellow at ETH AI center, where she is advised by Ryan and Elliott Ash. She is broadly interested in a variety of topics related to machine learning for NLP and social sciences. Before starting her Ph.D., she received her master’s in computer science at ETH Zürich and bachelor\u0026rsquo;s in software engineering at Sharif University of Technology. In her spare time, she enjoys digital painting, reading and playing the piano.\nNative Language: Persian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2091cc68cd468c5542742f222e5c4e2a","permalink":"https://rycolab.io/authors/afra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/afra/","section":"authors","summary":"Afra is a first-year doctoral fellow at ETH AI center, where she is advised by Ryan and Elliott Ash. She is broadly interested in a variety of topics related to machine learning for NLP and social sciences.","tags":null,"title":"Afra Amini","type":"authors"},{"authors":["aleximmer"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5eb0414be8e932db3af72dc685eed986","permalink":"https://rycolab.io/authors/aleximmer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/aleximmer/","section":"authors","summary":"","tags":null,"title":"Alex Immer","type":"authors"},{"authors":["alexwarstadt"],"categories":null,"content":"Alex Warstadt is a Postdoctoral Fellow at ETH Zürich in the Computer Science department affiliated with Rycolab and Mrinmaya’s Lab. He completed a PhD in linguistics at New York University supervised by Sam Bowman. As a computational linguist, he uses language models to study human language acquisition and to test the learnability of grammatical phenomena. He also does work in theoretical and experimental pragmatics, studying gradient phenomena in presupposition and relevance.\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2714b5a67c3c75a1e093c9e29c9cf781","permalink":"https://rycolab.io/authors/alexwarstadt/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alexwarstadt/","section":"authors","summary":"Alex Warstadt is a Postdoctoral Fellow at ETH Zürich in the Computer Science department affiliated with Rycolab and Mrinmaya’s Lab. He completed a PhD in linguistics at New York University supervised by Sam Bowman.","tags":null,"title":"Alex Warstadt","type":"authors"},{"authors":["alex"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d7149a99f41440e55ea517c3fb5d3c99","permalink":"https://rycolab.io/authors/alex/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alex/","section":"authors","summary":"","tags":null,"title":"Alexander Hoyle","type":"authors"},{"authors":["alexandra"],"categories":null,"content":"Alexandra is a MSc Data Science student at ETH Zürich. Prior to joining ETH, she did a bachelor’s degree in Software Engineering at The University of Sheffield, United Kingdom, where she focused mostly on theoretical computer science and machine learning. She is currently working on formalisms for mildly context-sensitive languages for her MSc thesis. She enjoys a variety of topics, including formal language theory, algorithms, machine translation, stats and is trying to learn more about others as well, in search of a primary research interest.\nNative Language: Romanian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"66960bc565a80e3a472ac6720957f252","permalink":"https://rycolab.io/authors/alexandra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alexandra/","section":"authors","summary":"Alexandra is a MSc Data Science student at ETH Zürich. Prior to joining ETH, she did a bachelor’s degree in Software Engineering at The University of Sheffield, United Kingdom, where she focused mostly on theoretical computer science and machine learning.","tags":null,"title":"Alexandra Butoi","type":"authors"},{"authors":["amit"],"categories":null,"content":"Amit is a fourth-year PhD student at Bar-Ilan University who is visiting ETH.\nNative Language: Hebrew\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"591850d0d6a4f0d13e6df8ce96b8e76d","permalink":"https://rycolab.io/authors/amit/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/amit/","section":"authors","summary":"Amit is a fourth-year PhD student at Bar-Ilan University who is visiting ETH.\nNative Language: Hebrew","tags":null,"title":"Amit Moryossef","type":"authors"},{"authors":["andreas"],"categories":null,"content":"Andreas is a second-year PhD student at ETH and a fellow at the Max Planck ETH Center for Learning Systems. He obtained a BSc in Industrial Engineering from Chalmers University of Technology and an MSc in Data Science from ETH. He enjoys learning about formal models of language, computational tools for cognitive modeling, and NLP algorithms in general. Currently, his research focuses broadly on parsing and reasoning.\nNative Language: Swedish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"040fe87c0b036f4e659fac698e83c5b3","permalink":"https://rycolab.io/authors/andreas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/andreas/","section":"authors","summary":"Andreas is a second-year PhD student at ETH and a fellow at the Max Planck ETH Center for Learning Systems. He obtained a BSc in Industrial Engineering from Chalmers University of Technology and an MSc in Data Science from ETH.","tags":null,"title":"Andreas Opedal","type":"authors"},{"authors":["anej"],"categories":null,"content":"Anej is a first-year PhD fellow at the ETH AI Center, where he is co-advised by Ryan and prof. Valentina Boeva. He completed his Bachelor\u0026rsquo;s degree in Computer Science and Mathematics in Ljubljana, Slovenia, and obtained his Master\u0026rsquo;s degree in Data Science at ETH Zürich. He now helps with teaching the Large Language Models, Advanced Formal Language Theory, and Natural Language Processing courses.\nAnej\u0026rsquo;s main research interests lie in the intersection of formal language theory and modern language models, where he is working on improving our understanding of the formal properties of architectures such as recurrent neural networks and Transformers with weighted formalisms from formal language theory. He is also interested in representation learning and its interpretability.\nIf you ask him about his hobbies you will find out he likes to read (you may also learn a lot about John Kennedy Toole and Christopher Moore), run, hike, and aquascape. As a true Slovene, he can also bake potica.\nNative Language: Slovene\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b99e1610c7ea1dbff34a3b8d0c7e725f","permalink":"https://rycolab.io/authors/anej/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/anej/","section":"authors","summary":"Anej is a first-year PhD fellow at the ETH AI Center, where he is co-advised by Ryan and prof. Valentina Boeva. He completed his Bachelor\u0026rsquo;s degree in Computer Science and Mathematics in Ljubljana, Slovenia, and obtained his Master\u0026rsquo;s degree in Data Science at ETH Zürich.","tags":null,"title":"Anej Svete","type":"authors"},{"authors":["anton"],"categories":null,"content":"Anton received his BSc at Ecole Polytechnique (France). He is currently an MSc student in Data Science at ETH.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"007c0c7232240c091e8a22f5d15226f6","permalink":"https://rycolab.io/authors/anton/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/anton/","section":"authors","summary":"Anton received his BSc at Ecole Polytechnique (France). He is currently an MSc student in Data Science at ETH.","tags":null,"title":"Anton Raël","type":"authors"},{"authors":["aryaman"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8b2ea8bf00bfd1611eac9e685b200925","permalink":"https://rycolab.io/authors/aryaman/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/aryaman/","section":"authors","summary":"","tags":null,"title":"Aryaman Arora","type":"authors"},{"authors":["ayca"],"categories":null,"content":"Ayça is a computer vision research engineer at the ETH Media Technology Center. She completed her MSc at ETH in Electrical Engineering and Information Technology.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7e74f14ffb3b1f0dd0bf0ecc442ec976","permalink":"https://rycolab.io/authors/ayca/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ayca/","section":"authors","summary":"Ayça is a computer vision research engineer at the ETH Media Technology Center. She completed her MSc at ETH in Electrical Engineering and Information Technology.","tags":null,"title":"Ayça Takmaz","type":"authors"},{"authors":["benjamin"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ec73f435f528c2504c3b3945bc1d1423","permalink":"https://rycolab.io/authors/benjamin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/benjamin/","section":"authors","summary":"","tags":null,"title":"Benjamin Li Dayan","type":"authors"},{"authors":["brian"],"categories":null,"content":"Brian is a postdoc in Rycolab. He completed his PhD at the University of Notre Dame, supervised by David Chiang. Brian\u0026rsquo;s dissertation was about incorporating pushdown automata into neural nets for NLP. Brian also taught the Theory of Computing course at Notre Dame in the spring of 2022. Brian is particularly interested in areas of NLP that are connected to formal language theory, grammars, parsing, and machine translation.\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b7bd119042681ace0111e2c236476166","permalink":"https://rycolab.io/authors/brian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/brian/","section":"authors","summary":"Brian is a postdoc in Rycolab. He completed his PhD at the University of Notre Dame, supervised by David Chiang. Brian\u0026rsquo;s dissertation was about incorporating pushdown automata into neural nets for NLP.","tags":null,"title":"Brian DuSell","type":"authors"},{"authors":["carl"],"categories":null,"content":"I am a post doc in Machine Learning at the AI Centre, ETH Zurich. I completed my PhD in 2022 at the University of Edinburgh as a member of the Centre for Doctoral Training in Data Science, supervised by Profs Tim Hospedales and Iain Murray. My PhD focused on developing a theoretical understanding of how words are represented: as word embeddings learned from huge text corpora (e.g. by word2vec or GloVe); or as entity embeddings learned from the facts of a knowledge graph. During my PhD I spent 6 months on internship at Samsung AI Centre, Cambridge looking into the interestection of representation learning and logical reasoning.\nNative Language: English\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cdd9bfbdc98cd2eb12c329195a32902e","permalink":"https://rycolab.io/authors/carl/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/carl/","section":"authors","summary":"I am a post doc in Machine Learning at the AI Centre, ETH Zurich. I completed my PhD in 2022 at the University of Edinburgh as a member of the Centre for Doctoral Training in Data Science, supervised by Profs Tim Hospedales and Iain Murray.","tags":null,"title":"Carl Allen","type":"authors"},{"authors":["carlos"],"categories":null,"content":"Carlos is a lecturer at ETH Zürich. He is interested in applications of ML into medicine and information security.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8d0972dd4ff800cd6d19f264079261ef","permalink":"https://rycolab.io/authors/carlos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/carlos/","section":"authors","summary":"Carlos is a lecturer at ETH Zürich. He is interested in applications of ML into medicine and information security.","tags":null,"title":"Carlos Cotrini","type":"authors"},{"authors":["carmen"],"categories":null,"content":"Carmen is a Postdoctoral Fellow at the ETH AI Center, affiliated with Melanie Zeilinger, Ryan Cotterell and Florian Dorfler. She completed her PhD in control theory at Caltech, and is very interested in exploring how control theoretical tools can be used to improve language technologies, as well as to enhance our understanding of their behavior.\nNative Language: Spanish\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e1d638788b1833e935cdbb7084971dd5","permalink":"https://rycolab.io/authors/carmen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/carmen/","section":"authors","summary":"Carmen is a Postdoctoral Fellow at the ETH AI Center, affiliated with Melanie Zeilinger, Ryan Cotterell and Florian Dorfler. She completed her PhD in control theory at Caltech, and is very interested in exploring how control theoretical tools can be used to improve language technologies, as well as to enhance our understanding of their behavior.","tags":null,"title":"Carmen Amo Alonso","type":"authors"},{"authors":["ce"],"categories":null,"content":"Ce is an Assistant Professor in Computer Science at ETH Zurich. The mission of his research is to make machine learning techniques widely accessible-​​-​-while being cost-​efficient and trustworthy-​​-​-to everyone who wants to use them to make our world a better place. He believes in a system approach to enabling this goal, and his current research focuses on building next-​generation machine learning platforms and systems that are data-​centric, human-​centric, and declaratively scalable. Before joining ETH, Ce finished his PhD at the University of Wisconsin-​​Madison and spent another year as a postdoctoral researcher at Stanford, both advised by Christopher Ré. His work has received recognitions such as the SIGMOD Best Paper Award, SIGMOD Research Highlight Award, Google Focused Research Award, an ERC Starting Grant, and has been featured and reported by Science, Nature, the Communications of the ACM, and a various media outlets such as Atlantic, WIRED, Quanta Magazine, etc.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"31169efd524f11d74449b532ab60eae8","permalink":"https://rycolab.io/authors/ce/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ce/","section":"authors","summary":"Ce is an Assistant Professor in Computer Science at ETH Zurich. The mission of his research is to make machine learning techniques widely accessible-​​-​-while being cost-​efficient and trustworthy-​​-​-to everyone who wants to use them to make our world a better place.","tags":null,"title":"Ce Zhang","type":"authors"},{"authors":["clara"],"categories":null,"content":"Clara is a third year PhD with Ryan at ETH Zürich. She received her B.S. and M.S. from Stanford University in Computational and Mathematical Engineering. Her research interests include language generation and psycholinguistics. In her free time, she likes to rock climb, trail run, and make snarky comments (directed predominantly at Ryan).\nNative Language: English\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3de996f9586f710ca1ed8c97792251c","permalink":"https://rycolab.io/authors/clara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/clara/","section":"authors","summary":"Clara is a third year PhD with Ryan at ETH Zürich. She received her B.S. and M.S. from Stanford University in Computational and Mathematical Engineering. Her research interests include language generation and psycholinguistics.","tags":null,"title":"Clara Meister","type":"authors"},{"authors":["clement"],"categories":null,"content":"Clément is a Research Assistant with Ryan at ETH Zürich. He completed his Bachelor’s degree in Mathematical Economics and Government at Wesleyan University and his Master’s degree in Data Science at ETH Zurich. His research interests include interpretability and language generation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0a58b1c043fd50490aaba2253085be3d","permalink":"https://rycolab.io/authors/clement/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/clement/","section":"authors","summary":"Clément is a Research Assistant with Ryan at ETH Zürich. He completed his Bachelor’s degree in Mathematical Economics and Government at Wesleyan University and his Master’s degree in Data Science at ETH Zurich.","tags":null,"title":"Clément Guerner","type":"authors"},{"authors":["clemente"],"categories":null,"content":"Clemente is a Research Assistant at Rycolab. Clemente did his Bachelor’s degree in Mathematical Engineering at Politecnico in Milano, Italy, and his master’s in Informatics at Università della Svizzera Italian in Lugano, Switzerland. He spent the last year of his master’s at ETH.\nClemente is interested in many areas of NLP, but he is currently focused on formal languages, and particularly in developing parsing algorithms. Some of his other interests are semantics, neuro-symbolic AI, and theory of computation.\nIn his free time Clemente likes to go rowing on the lake and read philosophy books.\nNative Language: Italian Favourite Language: Ancient Greek\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2ab8bf2d1048c91f046494a0a4317a55","permalink":"https://rycolab.io/authors/clemente/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/clemente/","section":"authors","summary":"Clemente is a Research Assistant at Rycolab. Clemente did his Bachelor’s degree in Mathematical Engineering at Politecnico in Milano, Italy, and his master’s in Informatics at Università della Svizzera Italian in Lugano, Switzerland.","tags":null,"title":"Clemente Pasti","type":"authors"},{"authors":["damian"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"915cffd0a6d0547272831667bc5c6bb1","permalink":"https://rycolab.io/authors/damian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/damian/","section":"authors","summary":"","tags":null,"title":"Damián Blasi","type":"authors"},{"authors":["daniel"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"13fb944df2e35b4baa2c6664c8ff164a","permalink":"https://rycolab.io/authors/daniel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/daniel/","section":"authors","summary":"","tags":null,"title":"Daniel Vera","type":"authors"},{"authors":["david"],"categories":null,"content":"David is a PhD student shared between the BoevaLab at ETH and the Robinson Group at UZH. He is interested broadly in survival analysis and the analysis of long-read sequencing data.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a188323511e886023c2ed76753cf8aaf","permalink":"https://rycolab.io/authors/david/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/david/","section":"authors","summary":"David is a PhD student shared between the BoevaLab at ETH and the Robinson Group at UZH. He is interested broadly in survival analysis and the analysis of long-read sequencing data.","tags":null,"title":"David Wissel","type":"authors"},{"authors":["edo"],"categories":null,"content":"Edoardo is a prospective postdoctoral fellow at Mila in Montreal, Canada. He is expected to complete his Ph.D. at the University of Cambridge in September 2020. In Cambridge, he was affiliated with St John\u0026rsquo;s College and supervised by Prof. Anna Korhonen and Ivan Vulić. As an undergrad, he studied modern literature at the University of Pavia, Italy. He works closely with Ryan. He previously interned as an AI/ML researcher at Apple in Cupertino. His research has been supported by a Google Research Faculty Award and an ERC PoC grant for projects co-written with his supervisors. His research focuses on few-shot multilingual learning and on text-based commonsense reasoning.\nNative Language: Italian\nAnimal Form: Water bear (Tardigrade)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0d9d7c8635f56e85ab0ca3b09a1442fb","permalink":"https://rycolab.io/authors/edo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/edo/","section":"authors","summary":"Edoardo is a prospective postdoctoral fellow at Mila in Montreal, Canada. He is expected to complete his Ph.D. at the University of Cambridge in September 2020. In Cambridge, he was affiliated with St John\u0026rsquo;s College and supervised by Prof.","tags":null,"title":"Edoardo M. Ponti","type":"authors"},{"authors":["kat"],"categories":null,"content":"Ekaterina Vylomova is a Postdoctoral Fellow at the University of Melbourne. She holds a PhD in Computer Science obtained from the University of Melbourne. Her research is focused on compositionality modelling for morphology, models for derivational morphology, neural machine translation, and diachronic language modeling.\nNative Language: Russian\nAnimal Form: Mimus polyglottos\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"77b0a8f9ddff4ab3e5c7552c687132ed","permalink":"https://rycolab.io/authors/kat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kat/","section":"authors","summary":"Ekaterina Vylomova is a Postdoctoral Fellow at the University of Melbourne. She holds a PhD in Computer Science obtained from the University of Melbourne. Her research is focused on compositionality modelling for morphology, models for derivational morphology, neural machine translation, and diachronic language modeling.","tags":null,"title":"Ekaterina Vylomova","type":"authors"},{"authors":["eleftheria"],"categories":null,"content":"Eleftheria is a PhD student in Computer Science at ETH Zurich. She is advised by Ryan and Elliott Ash. Previously, she was a Research Engineer at Disney Research where she worked on natural language understanding and emotion classification. Eleftheria received an MSc in Artificial Intelligence with specialization in NLP from the University of Edinburgh where her thesis was on machine translation. Before that, she received a BA in English Language and Literature with specialization in Linguistics from the University of Athens. Her research interests include computational narratology (particularly NLP for literary and screenwriting analysis), and formal language theory. Outside of research, Eleftheria likes tea, TV Tropes, wholesome memes, and playing soundtracks by ear on the piano.\nNative Language: Greek\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ca5fef1fb2e39756e74f4ff22d43656f","permalink":"https://rycolab.io/authors/eleftheria/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/eleftheria/","section":"authors","summary":"Eleftheria is a PhD student in Computer Science at ETH Zurich. She is advised by Ryan and Elliott Ash. Previously, she was a Research Engineer at Disney Research where she worked on natural language understanding and emotion classification.","tags":null,"title":"Eleftheria Tsipidi","type":"authors"},{"authors":["liz"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fc7f296399b51eca9f04506227a81c4e","permalink":"https://rycolab.io/authors/liz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liz/","section":"authors","summary":"","tags":null,"title":"Elizabeth Salesky","type":"authors"},{"authors":["ema"],"categories":null,"content":"Emanuele is a first-year PhD student at the University of Copenhagen, supervised by Desmond Elliott and closely collaborating with Ryan. His research is funded through a Marie Skłodowska-Curie TALENT fellowship and focuses on language grounding from multimodal and multilingual contexts. He also likes neural machine translation, typology and ramen.\nBefore his PhD, Emanuele received his master’s degree from the School of Computer and Communication Sciences at EPFL, where he was a member of the Data Science Lab led by Robert West. He also holds two bachelor’s degrees in Information Engineering from Tongji University and in Telecommunications Engineering from Politecnico di Torino.\nNative Language: Italian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bbe9321070c19534ca5cfe036a68e756","permalink":"https://rycolab.io/authors/ema/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ema/","section":"authors","summary":"Emanuele is a first-year PhD student at the University of Copenhagen, supervised by Desmond Elliott and closely collaborating with Ryan. His research is funded through a Marie Skłodowska-Curie TALENT fellowship and focuses on language grounding from multimodal and multilingual contexts.","tags":null,"title":"Emanuele Bugliarello","type":"authors"},{"authors":["ethan"],"categories":null,"content":"Ethan is a postdoc in Rycolab and the Language Reasoning and Education lab. He is a computational psycholinguist, using the tools from computer science to build models of language processing and language acquisition. He is particularly interested in how people process language as they read and how they make inferences about language structure during language learning.\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c20d8716151b9b830514afcc70e5aa4f","permalink":"https://rycolab.io/authors/ethan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ethan/","section":"authors","summary":"Ethan is a postdoc in Rycolab and the Language Reasoning and Education lab. He is a computational psycholinguist, using the tools from computer science to build models of language processing and language acquisition.","tags":null,"title":"Ethan Wilcox","type":"authors"},{"authors":["floriantramer"],"categories":null,"content":"Florian Tramèr is an assistant professor of computer science at ETH Zurich. His research interests lie in Computer Security, Cryptography and Machine Learning security. In his current work, he studies the worst-case behavior of Deep Learning systems from an adversarial perspective, to understand and mitigate long-term threats to the safety and privacy of users.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b494302bb572063b3799240ccba31643","permalink":"https://rycolab.io/authors/floriantramer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/floriantramer/","section":"authors","summary":"Florian Tramèr is an assistant professor of computer science at ETH Zurich. His research interests lie in Computer Security, Cryptography and Machine Learning security. In his current work, he studies the worst-case behavior of Deep Learning systems from an adversarial perspective, to understand and mitigate long-term threats to the safety and privacy of users.","tags":null,"title":"Florian Tramèr","type":"authors"},{"authors":["franz"],"categories":null,"content":"Franz is a MSc Computer Science student at ETH Zürich, majoring in Machine Intelligence. Before joining ETH, he did a bachelor\u0026rsquo;s degree in Computer Science at Cambridge, doing his final project on machine learning applied to database indexing. His hobbies include jazz piano, chess, and free diving.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"79aa2a605090ca1f8205c5bec82bc1ee","permalink":"https://rycolab.io/authors/franz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/franz/","section":"authors","summary":"Franz is a MSc Computer Science student at ETH Zürich, majoring in Machine Intelligence. Before joining ETH, he did a bachelor\u0026rsquo;s degree in Computer Science at Cambridge, doing his final project on machine learning applied to database indexing.","tags":null,"title":"Franz Nowak","type":"authors"},{"authors":["giacomo"],"categories":null,"content":"Giacomo is second-year a Computer Science Master student at ETH Zürich. Before joining ETH, he completed his bachelor’s degree in Computer Engineering in Padova, Italy. His hobbies include swimming, cooking and hiking.\nNative language: Italian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"84e75426c39d03c10d74bb36aeadb134","permalink":"https://rycolab.io/authors/giacomo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/giacomo/","section":"authors","summary":"Giacomo is second-year a Computer Science Master student at ETH Zürich. Before joining ETH, he completed his bachelor’s degree in Computer Engineering in Padova, Italy. His hobbies include swimming, cooking and hiking.","tags":null,"title":"Giacomo Camposampiero","type":"authors"},{"authors":["gian"],"categories":null,"content":"Gian is a second year Data Science Student at ETH Zürich. Prior to that, he obtained a BSc in mathematics from ETHZ. His research interests include Natural Language Generation, with a focus on decoding schemes. In his freetime, he likes to mountain bike, play videogames and ride his motorbike around the alps.\nNative Language: Swiss German\nAnimal Form: Jellyfish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6a25e7f4891be9b1b8ffac4d9553ae26","permalink":"https://rycolab.io/authors/gian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gian/","section":"authors","summary":"Gian is a second year Data Science Student at ETH Zürich. Prior to that, he obtained a BSc in mathematics from ETHZ. His research interests include Natural Language Generation, with a focus on decoding schemes.","tags":null,"title":"Gian Wiher","type":"authors"},{"authors":["giovanni"],"categories":null,"content":"Giovanni is a Master’s student in Data Science at ETH Zurich. He completed his Bachelor’s degree in Mathematics at University of Bologna. His research interests include surprisal theory, and he is still looking for the right compromise between mathematics and computer science. In his free time, he likes bouldering and cooking.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"cd5771a06165b12ddc7a4ecf9aa75e23","permalink":"https://rycolab.io/authors/giovanni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/giovanni/","section":"authors","summary":"Giovanni is a Master’s student in Data Science at ETH Zurich. He completed his Bachelor’s degree in Mathematics at University of Bologna. His research interests include surprisal theory, and he is still looking for the right compromise between mathematics and computer science.","tags":null,"title":"Giovanni Acampa","type":"authors"},{"authors":["irene"],"categories":null,"content":"Irene is an MPhil student at the University of Cambridge, where she is supervised by Ryan. She received her BSc in Computer Science with a minor in mathematics and statistics from the University of Helsinki. Her primary interests are machine learning and anything in the intersection of computer science and statistics. During her free time she likes running (away from responsibilities) and correcting people who say that Finland is part of Scandinavia.\nNative Language: Finnish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89af9b35ecd8250fa5b24fe3dfe7d186","permalink":"https://rycolab.io/authors/irene/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/irene/","section":"authors","summary":"Irene is an MPhil student at the University of Cambridge, where she is supervised by Ryan. She received her BSc in Computer Science with a minor in mathematics and statistics from the University of Helsinki.","tags":null,"title":"Irene Nikkarinen","type":"authors"},{"authors":["isabelle"],"categories":null,"content":"Isabelle is an associate professor at the University of Copenhagen, Department of Computer Science and works in the general areas of Statistical Natural Language Processing and Machine Learning\nAnimal Form: Owl\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"76b480c4a09bc036d7176e3210f31a66","permalink":"https://rycolab.io/authors/isabelle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/isabelle/","section":"authors","summary":"Isabelle is an associate professor at the University of Copenhagen, Department of Computer Science and works in the general areas of Statistical Natural Language Processing and Machine Learning\nAnimal Form: Owl","tags":null,"title":"Isabelle Augenstein","type":"authors"},{"authors":["jen"],"categories":null,"content":"Jennifer is a first-year PhD student at the University of Cambridge, supervised by Ryan. She earned an MMathPhys in Mathematics and Physics at the University of Warwick and an MPhil in Advanced Computer Science at the University of Cambridge where she was also supervised by Ryan. She is interested in techniques for low-resource NLP, gender bias in NLP and annotation of linguistic data, among other things. Her background is in Algebraic Geometry, so she is always happy to get a chance to put her mathematical skills to good use. In her spare time she enjoys learning languages, reading, eating chicken katsu curry and travelling as much as possible.\nNative Language: English\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5d240d57e5fd9bed1d26eb181ebe2842","permalink":"https://rycolab.io/authors/jen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jen/","section":"authors","summary":"Jennifer is a first-year PhD student at the University of Cambridge, supervised by Ryan. She earned an MMathPhys in Mathematics and Physics at the University of Warwick and an MPhil in Advanced Computer Science at the University of Cambridge where she was also supervised by Ryan.","tags":null,"title":"Jennifer C. White","type":"authors"},{"authors":["jiaoda"],"categories":null,"content":"Jiaoda Li is an ETH AI Center Doctoral Fellow. He is part of the groups of Ryan Cotterell and Stefan Feuerriegel. He also works closely with Mrinmaya Sachan and Rico Sennrich. He received his Master degree in Data Science from ETH Zürich in 2021. Prior to that, he was an undergraduate student in City University of Hong Kong, majoring in Electronic and Communication Engineering.\nNative Language: Mandarin\nAnimal Form: Mouse\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2f5d73b305f66523f6084d93add0fbdf","permalink":"https://rycolab.io/authors/jiaoda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiaoda/","section":"authors","summary":"Jiaoda Li is an ETH AI Center Doctoral Fellow. He is part of the groups of Ryan Cotterell and Stefan Feuerriegel. He also works closely with Mrinmaya Sachan and Rico Sennrich.","tags":null,"title":"Jiaoda Li","type":"authors"},{"authors":["john"],"categories":null,"content":"John is a final-year PhD in computer science at Stanford University with Chris Manning and Percy Liang. His interests are in neural representations of language, models of language, and interpretability thereof. His long-term goals are to design systems that learn many of the world\u0026rsquo;s languages and provide interfaces for controlling and understanding their behavior.\nNative Language: English\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8898c868207f087bc0b8ca6b49975fdd","permalink":"https://rycolab.io/authors/john/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/john/","section":"authors","summary":"John is a final-year PhD in computer science at Stanford University with Chris Manning and Percy Liang. His interests are in neural representations of language, models of language, and interpretability thereof.","tags":null,"title":"John Hewitt","type":"authors"},{"authors":["josef"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9d3fdfe8719fded59b20776fb11d2964","permalink":"https://rycolab.io/authors/josef/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/josef/","section":"authors","summary":"","tags":null,"title":"Josef Valvoda","type":"authors"},{"authors":["gianni"],"categories":null,"content":"Juan Luis Gastaldi (Gianni) is a philosopher of science, specialized in the philosophy of language and formal sciences (mathematics, logic, and computer science). After being a Professor in Philosophy and History of Ideas at MOCO.ESBA (Montpellier, France) and a Marie Skłodowska-Curie Postdoctoral Fellow at ETH (D-GESS), he is now pursuing a second PhD in Natural Language Processing at the Computer Science Department of ETH.\nGianni\u0026rsquo;s interests revolve around the linguistic aspects of formal sciences and the formal aspects of language and other sign systems, from a technical, philosophical, and historical perspective. He is also interested in the relationship between language and thought, the nature of meaning, and the possibility that a theory of language can inform both a theory of culture and a theory of science. His current research focuses on the computational and conceptual aspects of segmentation (and tokenization) and their relation to linguistic structure and meaning.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"164bfd3f5eea775c75dbd4bce7a05bff","permalink":"https://rycolab.io/authors/gianni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gianni/","section":"authors","summary":"Juan Luis Gastaldi (Gianni) is a philosopher of science, specialized in the philosophy of language and formal sciences (mathematics, logic, and computer science). After being a Professor in Philosophy and History of Ideas at MOCO.","tags":null,"title":"Juan Luis Gastaldi","type":"authors"},{"authors":["jun"],"categories":null,"content":"Jun is a video game programmer at Electric Square. They are also interested in computational linguistics and completed their MPhil in Advanced Computer Science at the University of Cambridge in June 2020. Prior to this, they studied Computer Science at the University of Southern California. In another life, Jun would like to be either a musician or a whale.\nNative Languages: English and Cantonese\nAnimal Form: Orca\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7d339d8f30d803efeaeb243a5fe6af73","permalink":"https://rycolab.io/authors/jun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jun/","section":"authors","summary":"Jun is a video game programmer at Electric Square. They are also interested in computational linguistics and completed their MPhil in Advanced Computer Science at the University of Cambridge in June 2020.","tags":null,"title":"Jun Yen Leung","type":"authors"},{"authors":["karolina"],"categories":null,"content":"-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89a3e957bf68ea773109c28337cfefd1","permalink":"https://rycolab.io/authors/karolina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/karolina/","section":"authors","summary":"-- ","tags":null,"title":"Karolina Stańczak","type":"authors"},{"authors":["kevin"],"categories":null,"content":"Kevin is a PhD student at ETH Zürich. He is interested in developing explainability methods for NLP. Previously, he graduated with his Bachelor\u0026rsquo;s + Master\u0026rsquo;s degrees from Brown University and worked as an applied ML engineer at Flatiron Health.\nNative Language: English\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"75fa14a39d8cd5050b2b641d745313cb","permalink":"https://rycolab.io/authors/kevin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kevin/","section":"authors","summary":"Kevin is a PhD student at ETH Zürich. He is interested in developing explainability methods for NLP. Previously, he graduated with his Bachelor\u0026rsquo;s + Master\u0026rsquo;s degrees from Brown University and worked as an applied ML engineer at Flatiron Health.","tags":null,"title":"Kevin Du","type":"authors"},{"authors":["kumar"],"categories":null,"content":"I am a second year PhD student at ETH Zürich in Switzerland, supervised by Prof. Mrinmaya Sachan and Nicholas Monath from Google. I work on understanding the reasoning capabilities of generative LLMs. I also work closely with Carl Allen (Post doc at ETH AI Center) and Manzil Zaheer (Research Scientist at Deepmind). Before that, I was a Masters student at the University of Kaiserslautern, Germany and worked with Prof. Marcus Liwicki on Bayesian CNNs with variational inference.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"aa94db0d790156451e2314f072774a37","permalink":"https://rycolab.io/authors/kumar/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kumar/","section":"authors","summary":"I am a second year PhD student at ETH Zürich in Switzerland, supervised by Prof. Mrinmaya Sachan and Nicholas Monath from Google. I work on understanding the reasoning capabilities of generative LLMs.","tags":null,"title":"Kumar Shridhar","type":"authors"},{"authors":["richard"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b875188fbc18f4f241c1881bc428b190","permalink":"https://rycolab.io/authors/kyle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kyle/","section":"authors","summary":"","tags":null,"title":"Kyle Mahowald","type":"authors"},{"authors":["laura"],"categories":null,"content":"Laura is a Postdoctoral Researcher at the ETH Media Technology Center, where she is currently doing research on abstractive Summarization. She is also interested in low-resource settings. She has a Computer Science background and holds a Ph.D. in Computational Linguistics from the University of Zürich. Her dissertation was on improving lexical choice in Statistical Machine Translation with discourse-aware approaches.\nNative Language: Catalan\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f67bd8c08f9dcbd326d839820d910860","permalink":"https://rycolab.io/authors/laura/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/laura/","section":"authors","summary":"Laura is a Postdoctoral Researcher at the ETH Media Technology Center, where she is currently doing research on abstractive Summarization. She is also interested in low-resource settings. She has a Computer Science background and holds a Ph.","tags":null,"title":"Laura Mascarell","type":"authors"},{"authors":["leo"],"categories":null,"content":"Leo is a PhD student at Johns Hopkins University currently advised by Jason Eisner. He received his B.S. from University of Washington in Mathematics and Computer Science. His interests include statistical and mathematical problems in NLP and machine learning, sometimes with methods inspired from physics.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8f09dff652e04b0d9031d9d42901e9d1","permalink":"https://rycolab.io/authors/leo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/leo/","section":"authors","summary":"Leo is a PhD student at Johns Hopkins University currently advised by Jason Eisner. He received his B.S. from University of Washington in Mathematics and Computer Science. His interests include statistical and mathematical problems in NLP and machine learning, sometimes with methods inspired from physics.","tags":null,"title":"Leo Du","type":"authors"},{"authors":["leonardo"],"categories":null,"content":"Leonardo is a Master\u0026rsquo;s student in mathematics at ETH with application area Machine Learning. He is currently working on a formal language theory-based project with the group.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0313e6ee7498693302dc7ba17dcc6523","permalink":"https://rycolab.io/authors/leonardo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/leonardo/","section":"authors","summary":"Leonardo is a Master\u0026rsquo;s student in mathematics at ETH with application area Machine Learning. He is currently working on a formal language theory-based project with the group.","tags":null,"title":"Leonardo Nevali","type":"authors"},{"authors":["luca"],"categories":null,"content":"Luca is a Research Assistant with Ryan at ETH Zurich. He completed his Bachelor’s degree in Computer Engineering at the Polytechnic University of Milan and his Master’s degree in Computer Science at ETH Zurich.\nHis research interests include decoding and training techniques for language models. He has recently finished writing his Master’s Thesis titled ‘Divergence Functions for Natural Language Generation’ under the supervision of Clara.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0274d5dfc9b30c878e9ba77604bc74cf","permalink":"https://rycolab.io/authors/luca/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/luca/","section":"authors","summary":"Luca is a Research Assistant with Ryan at ETH Zurich. He completed his Bachelor’s degree in Computer Engineering at the Polytechnic University of Milan and his Master’s degree in Computer Science at ETH Zurich.","tags":null,"title":"Luca Malagutti","type":"authors"},{"authors":["lucas"],"categories":null,"content":"Lucas is a PhD student at MIT supervised by Yoon Kim. Previously, he completed the MPhil in Advanced Computer Science at the University of Cambridge. He wrote his MPhil thesis with Ryan on probing pre-trained neural language models for morpho-syntax. Before then he was a BSc Artificial Intelligence and Computer Science student at the University of Edinburgh.\nNative Languages: Spanish and Portuguese\nAnimal Form: Llama\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f4eeb2051886e69b374435203391df51","permalink":"https://rycolab.io/authors/lucas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lucas/","section":"authors","summary":"Lucas is a PhD student at MIT supervised by Yoon Kim. Previously, he completed the MPhil in Advanced Computer Science at the University of Cambridge. He wrote his MPhil thesis with Ryan on probing pre-trained neural language models for morpho-syntax.","tags":null,"title":"Lucas Torroba Hennigen","type":"authors"},{"authors":["mans"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bcd9a2ac5f3d13f165b3f46243c9f942","permalink":"https://rycolab.io/authors/mans/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mans/","section":"authors","summary":"","tags":null,"title":"Mans Hulden","type":"authors"},{"authors":["marinela"],"categories":null,"content":"Marinela started her PhD in October 2019 in the Language Technology Lab at the University of Cambridge under the supervision of Prof. Anna Korhonen and Ryan. She is a member of Trinity College and is funded by Trinity Overseas Bursary.\nBefore starting her PhD, Marinela obtained MSc and BSc degrees in Mathematics, both from the University of Belgrade in Serbia.\nNative Language: Serbian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7cbd738aeab27bddcfebd1c698fee92d","permalink":"https://rycolab.io/authors/marinela/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/marinela/","section":"authors","summary":"Marinela started her PhD in October 2019 in the Language Technology Lab at the University of Cambridge under the supervision of Prof. Anna Korhonen and Ryan. She is a member of Trinity College and is funded by Trinity Overseas Bursary.","tags":null,"title":"Marinela Parović","type":"authors"},{"authors":["mario"],"categories":null,"content":"Mario is a postdoctoral researcher at ETH Zurich in the Institute for Machine Learning, Department of Computer Science, and a member of the ELLIS Society. He completed a PhD in computational linguistics at the University of Amsterdam, in the Institute for Logic, Language and Computation. Mario studies language use and evolution using tools from computer science, linguistics, and cognitive science. His topics of interest include language modelling, NLP interpretability and evaluation, natural language generation, computational semantics and pragmatics, language variation and change in communities of speakers.\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"454acc1cf8bead111374b203a9357c0b","permalink":"https://rycolab.io/authors/mario/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mario/","section":"authors","summary":"Mario is a postdoctoral researcher at ETH Zurich in the Institute for Machine Learning, Department of Computer Science, and a member of the ELLIS Society. He completed a PhD in computational linguistics at the University of Amsterdam, in the Institute for Logic, Language and Computation.","tags":null,"title":"Mario Giulianelli","type":"authors"},{"authors":["martina"],"categories":null,"content":"Martina is writing her Master\u0026rsquo;s thesis at ETH Zürich, supervised by Clara and Ryan. Her academic interests are neural machine translation, morphology and cross-lingual learning. Also, she likes learning new languages and eating lots of chocolate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b08b9d0bebfd9788e31d72c0fa38ce6f","permalink":"https://rycolab.io/authors/martina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/martina/","section":"authors","summary":"Martina is writing her Master\u0026rsquo;s thesis at ETH Zürich, supervised by Clara and Ryan. Her academic interests are neural machine translation, morphology and cross-lingual learning. Also, she likes learning new languages and eating lots of chocolate.","tags":null,"title":"Martina Forster","type":"authors"},{"authors":["maximilian"],"categories":null,"content":"Maximilian is an Applied Mathematics master\u0026rsquo;s student at ETHZ. He did his BSc in mathematics at the University of Vienna. Currently he is writing his master thesis about the metric entropy optimality of CTRNNs.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"10604489ebb1ae8bae8376a28d7acacf","permalink":"https://rycolab.io/authors/maximilian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/maximilian/","section":"authors","summary":"Maximilian is an Applied Mathematics master\u0026rsquo;s student at ETHZ. He did his BSc in mathematics at the University of Vienna. Currently he is writing his master thesis about the metric entropy optimality of CTRNNs.","tags":null,"title":"Maximilian Schneiderbauer","type":"authors"},{"authors":["mian"],"categories":null,"content":"Mian studies MSc Data Science at ETH and graduated from Applied Mathematics at UC Berkeley. She is very grateful for being part of DAAD RiSE, USC ISI and UCSF, where she gets to know what interdisciplinary research is like. Her current research interests are open-domain question answering, bias \u0026amp; fairness, and NLP applications with real-world data. Now She is working on her master thesis with Niklas and Shehzaad, and seeking for PhD positions. While many people loathe, she could spend many hours on reading, collecting and visualizing data. Then, if not browsing data, she is likely sitting there to listen/watch new podcasts, film trailers, bibliographies or TvN shows. Meanwhile, her mind is drifted away with making (4-song) Spotify playlists, reading\u0026amp;re-starting blogs, and taking street photos.\nAnimal Form: Octopus\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6cac6af3d7ef86ad99b6c1fca6c2e3fa","permalink":"https://rycolab.io/authors/mian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mian/","section":"authors","summary":"Mian studies MSc Data Science at ETH and graduated from Applied Mathematics at UC Berkeley. She is very grateful for being part of DAAD RiSE, USC ISI and UCSF, where she gets to know what interdisciplinary research is like.","tags":null,"title":"Mian Zhong","type":"authors"},{"authors":["mirko"],"categories":null,"content":"Mirko is a second-year Computer Science Master’s student at ETH. I have completed my Bachelor in Computer Engineering at Politecnico di Milano. My main interests are Machine Learning, Theoretical Computer Science, Bioinformatics and NLP. When I do not study I mostly make EDM music.\nNative language: Italian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3259b2b53adf699533417ccbca798e2b","permalink":"https://rycolab.io/authors/mirko/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mirko/","section":"authors","summary":"Mirko is a second-year Computer Science Master’s student at ETH. I have completed my Bachelor in Computer Engineering at Politecnico di Milano. My main interests are Machine Learning, Theoretical Computer Science, Bioinformatics and NLP.","tags":null,"title":"Mirko De Vita","type":"authors"},{"authors":["mrinmaya"],"categories":null,"content":"Mrinmaya Sachan is an Assistant Professor in the Department of Computer Science at ETH Zurich. He received his PhD from Carnegie Mellon University. His research spans natural language processing, knowledge representation and reasoning. He is also interested in problems in the intersection of NLP and Education.. He received an outstanding paper award at ACL 2015, multiple fellowships (IBM and Facebook fellowships, Siebel scholarship and CMU CMLH fellowship) during his PhD. His research group is supported by the Swiss National Science Foundation, ETH Zurich and Haslerstiftung.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"af7a0ca59577e6610c0cc73cb5e9eb06","permalink":"https://rycolab.io/authors/mrinmaya/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mrinmaya/","section":"authors","summary":"Mrinmaya Sachan is an Assistant Professor in the Department of Computer Science at ETH Zurich. He received his PhD from Carnegie Mellon University. His research spans natural language processing, knowledge representation and reasoning.","tags":null,"title":"Mrinmaya Sachan","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rycolab.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":["niklas"],"categories":null,"content":"Niklas is a doctoral student interested in latent intensity scales. Combining methods from Natural Language Processing, Computational Social Science and Network Science, his interests are particularly centred around numbers in text, conflict intensity and sentiment analysis. After completing a MSc in Data Science at UCL, he worked at the interface of these fields at the IBM AI Core Team, Microsoft Research Cambridge and the German Federal Foreign Office in Shanghai. During his BSc in Information Management Systems at TU Berlin, Niklas spent 6 months at the University of Oxford and 12 months at Tsinghua University in Beijing funded by the German Academic Scholarship Foundation / Studienstiftung.\nNative Language: German\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3980a18dc1184eb009e3646396b5d2a3","permalink":"https://rycolab.io/authors/niklas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/niklas/","section":"authors","summary":"Niklas is a doctoral student interested in latent intensity scales. Combining methods from Natural Language Processing, Computational Social Science and Network Science, his interests are particularly centred around numbers in text, conflict intensity and sentiment analysis.","tags":null,"title":"Niklas Stoehr","type":"authors"},{"authors":["patrizia"],"categories":null,"content":"Patrizia is the assistant to Prof. Ryan Cotterell and his group and provides administrative support.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ff9e68237715d4029ad47562910dde7a","permalink":"https://rycolab.io/authors/patrizia/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/patrizia/","section":"authors","summary":"Patrizia is the assistant to Prof. Ryan Cotterell and his group and provides administrative support.","tags":null,"title":"Patrizia Napoli","type":"authors"},{"authors":["paula"],"categories":null,"content":"Paula is a third year PhD student at the University of Cambridge, where she is advised by Prof Ann Copestake and Ryan. She is supported by the Vice-Chancellor\u0026rsquo;s and Selwyn College Scholarship. Prior to starting her PhD she did the BSc in Computer Science at the University of St Andrews and completed the MPhil in Advanced Computer Science at Cambridge. During those degrees she was generously supported by the G. D. Fahrenheit Scholarship, awarded by the City Council of Gdańsk.\nHer main research interests include cross-lingual learning and computational approaches to linguistic typology and morphology. She is fascinated by how world’s languages differ in their means of encoding meaning and hopes that investigating those differences, alongside language similarities can facilitate building better, less biased NLP systems.\nNative Language: Polish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"74eae12507749d073766d39a5b14ab4e","permalink":"https://rycolab.io/authors/paula/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/paula/","section":"authors","summary":"Paula is a third year PhD student at the University of Cambridge, where she is advised by Prof Ann Copestake and Ryan. She is supported by the Vice-Chancellor\u0026rsquo;s and Selwyn College Scholarship.","tags":null,"title":"Paula Czarnowska","type":"authors"},{"authors":["philippe"],"categories":null,"content":"Philippe is an NLP research engineer at the ETH Media Technology Center (currently working on automatic post-editing of MT). He completed MSc in Computer Science at ETH.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8bc0fd568ea288f7cbed9e16f11af304","permalink":"https://rycolab.io/authors/philippe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/philippe/","section":"authors","summary":"Philippe is an NLP research engineer at the ETH Media Technology Center (currently working on automatic post-editing of MT). He completed MSc in Computer Science at ETH.","tags":null,"title":"Philippe Schlattner","type":"authors"},{"authors":["pouya"],"categories":null,"content":"Pouya is a Master\u0026rsquo;s student in Computer Science at ETH. He is interested in a variety of topics related to Machine Learning and NLP including Parsing, Information Extraction, and Multilingual NLP. In his free time, he likes to play Basketball, read about psychotherapy, get some REM sleep, and dream about how he can apply NLP to those areas.\nNative Language: Persian\nAnimal Form: Panthera tigris virgata\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ca209308c144c9f381a15bc865a34a02","permalink":"https://rycolab.io/authors/pouya/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/pouya/","section":"authors","summary":"Pouya is a Master\u0026rsquo;s student in Computer Science at ETH. He is interested in a variety of topics related to Machine Learning and NLP including Parsing, Information Extraction, and Multilingual NLP.","tags":null,"title":"Pouya Pourjafar","type":"authors"},{"authors":["ran"],"categories":null,"content":"Ran is a second-year PhD student at the University of Cambridge advised by Ryan. Previously, Ran completed the Computer Science Tripos at the University of Cambridge and continued on to Part III where he was also advised by Ryan. He is primarily interested in research regarding algorithms in NLP and parsing, and is picking up new interests as he goes along. While he waits for his systems to train he likes to play tennis, lacrosse and do stand-up comedy.\nNative Language: Hebrew\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6eba7e88ca7bbc6ce8119664e70023ed","permalink":"https://rycolab.io/authors/ran/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ran/","section":"authors","summary":"Ran is a second-year PhD student at the University of Cambridge advised by Ryan. Previously, Ran completed the Computer Science Tripos at the University of Cambridge and continued on to Part III where he was also advised by Ryan.","tags":null,"title":"Ran Zmigrod","type":"authors"},{"authors":["richard"],"categories":null,"content":"Richard Futrell is an Assistant Professor in the Department of Language Science at the University of California, Irvine. His research focuses on language processing in humans and machines.\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ed7eae801e1f7fcbe6a9d5e7cc088611","permalink":"https://rycolab.io/authors/richard/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/richard/","section":"authors","summary":"Richard Futrell is an Assistant Professor in the Department of Language Science at the University of California, Irvine. His research focuses on language processing in humans and machines.\n-- ","tags":null,"title":"Richard Futrell","type":"authors"},{"authors":["robin"],"categories":null,"content":"Robin is a recent ETH Data Science MSc graduate and an NLP TA. He wrote his thesis at IBM Research under the supervision of Ryan, working on out-of-distribution generalization of LLMs and constrained generation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"69b81a0066894916592506306a6e6258","permalink":"https://rycolab.io/authors/robin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/robin/","section":"authors","summary":"Robin is a recent ETH Data Science MSc graduate and an NLP TA. He wrote his thesis at IBM Research under the supervision of Ryan, working on out-of-distribution generalization of LLMs and constrained generation.","tags":null,"title":"Robin Chan","type":"authors"},{"authors":["rowan"],"categories":null,"content":"Please refer to his bio on the University of Cambridge website.\nNative Language: English\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d3750635a3b8746ef003d5679b039abb","permalink":"https://rycolab.io/authors/rowan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rowan/","section":"authors","summary":"Please refer to his bio on the University of Cambridge website.\nNative Language: English","tags":null,"title":"Rowan Hall Maudslay","type":"authors"},{"authors":["ryan"],"categories":null,"content":"I was born and raised in the city of Baltimore, Maryland—the greatest city in America. But you don’t have to take my word for it, it’s spray-painted on the city’s benches: I completed my undergraduate degree at Johns Hopkins University (also in Baltimore, Maryland) in Cognitive Science (with focal areas in Linguistics and Computational Methods) under the tutelage of Colin Wilson. I was then recruited by Jason Eisner to do a Ph.D. in Computer Science at Johns Hopkins University where I was a member of the Center for Language and Speech Processing; I also took quite a few courses in mathematics, statistics and linguistics along the way. I am currently a tenure-track assistant professor at ETH Zürich in the Department of Computer Science where I am a member of the Institut für maschinelles Lernen. I was previously a Lecturer (that’s British for tenure-track assistant professor) at the Computer Laboratory (the oldest computer science department in the world) at the University of Cambridge in the United Kingdom where I am still affiliated. I have also done research stints at Google AI, Facebook AI Research and the Ludwig Maximilian University of Munich.\nI am curious about most things: Simone Teufel calls me a “research hippie”—I’ve never been quite sure what that means, though. It is my greatest dream to achieve near-native competence in an agglutinative language, but here’s hoping. Recent scholarly work that bears my name may be perused on my lab’s publication list.\nNote: I am not taking students at the moment—my lab has reached critical mass. I spend roughly 3–4 hours a week with my primary (and secondary) advisees and a similar amount of time with many of my close collaborators as well. I am fresh out of cycles 😞\nNote Note: Despite the above note, please feel free to email anyway—about anything. But, if you do email me, please address me by my first name: Ryan. I strongly disprefer academic titles.\nFamily on the internet: My uncle John Cotterell is a painter; check him out here. My cousin Jake Velker does amazing work at the One Acre Fund. My parents (Thomas Cotterell and Gina Scarinzi) are not on the internet, but here’s a recent picture of us in London, England: -- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"65d4d588cef9fb8886b394cbe1e95b85","permalink":"https://rycolab.io/authors/ryan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ryan/","section":"authors","summary":"I was born and raised in the city of Baltimore, Maryland—the greatest city in America. But you don’t have to take my word for it, it’s spray-painted on the city’s benches: I completed my undergraduate degree at Johns Hopkins University (also in Baltimore, Maryland) in Cognitive Science (with focal areas in Linguistics and Computational Methods) under the tutelage of Colin Wilson.","tags":null,"title":"Ryan Cotterell","type":"authors"},{"authors":["samuel"],"categories":null,"content":"Samuel is a Ph.D. student at Roy Wagner\u0026rsquo;s Chair for the History and Philosophy of Mathematics. He works on the history of type theory and its relation to programming errors.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c0b58ee0cb59fefd52b1283d9f3b41db","permalink":"https://rycolab.io/authors/samuel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/samuel/","section":"authors","summary":"Samuel is a Ph.D. student at Roy Wagner\u0026rsquo;s Chair for the History and Philosophy of Mathematics. He works on the history of type theory and its relation to programming errors.","tags":null,"title":"Samuel Hunziker","type":"authors"},{"authors":["selena"],"categories":null,"content":"Selena is a second -year MSc student in Computer Science at ETH Zurich. Previously, she received her BSc from University of Novi Sad in Serbia. Currently, she is working on a research project with Ryan on cognitively plausible morphological inflection models. Apart from that, she is interested in theoretical computer science and in her spare time enjoys playing bridge.\nNative Language: Serbian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"19bd63ab7c994c5a2aea5fd2b3cecab4","permalink":"https://rycolab.io/authors/selena/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/selena/","section":"authors","summary":"Selena is a second -year MSc student in Computer Science at ETH Zurich. Previously, she received her BSc from University of Novi Sad in Serbia. Currently, she is working on a research project with Ryan on cognitively plausible morphological inflection models.","tags":null,"title":"Selena Pepić","type":"authors"},{"authors":["shijie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"be2d1967e64e4ed029b3e9f673940c75","permalink":"https://rycolab.io/authors/shijie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shijie/","section":"authors","summary":"","tags":null,"title":"Shijie Wu","type":"authors"},{"authors":["simone"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"561fdecbc989c73616aba426ed25861c","permalink":"https://rycolab.io/authors/simone/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/simone/","section":"authors","summary":"","tags":null,"title":"Simone Teufel","type":"authors"},{"authors":["stefan"],"categories":null,"content":"Stefan holds an MPhil degree in Advanced Computer Science from the University of Cambridge and BA degrees in Mathematics and Computer Science from the American University in Bulgaria. His interests include explainability of deep learning models, neural machine translation, and natural language generation. He likes spending his free time with friends and whenever he does not have free time, he likes working with friends.\nNative Language: Bulgarian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1fee0630ebf691a0b4eb76303184b35c","permalink":"https://rycolab.io/authors/stefan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/stefan/","section":"authors","summary":"Stefan holds an MPhil degree in Advanced Computer Science from the University of Cambridge and BA degrees in Mathematics and Computer Science from the American University in Bulgaria. His interests include explainability of deep learning models, neural machine translation, and natural language generation.","tags":null,"title":"Stefan Lazov","type":"authors"},{"authors":["tiago"],"categories":null,"content":"Tiago is a Postdoc at ETH Zürich. At the moment, Tiago is mainly interested in information theory and its application to the study of machine learning models and linguistics. To this end, he has recently been dabbling in information-theoretic linguistics and probing.\nNative Language: Portuguese\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fdbf9f3b62c6aa81ec87c8656829ba69","permalink":"https://rycolab.io/authors/tiago/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tiago/","section":"authors","summary":"Tiago is a Postdoc at ETH Zürich. At the moment, Tiago is mainly interested in information theory and its application to the study of machine learning models and linguistics. To this end, he has recently been dabbling in information-theoretic linguistics and probing.","tags":null,"title":"Tiago Pimentel","type":"authors"},{"authors":["tianyu"],"categories":null,"content":"Tianyu is a second-year PhD student at ETH Zurich. He is advised by Ryan and Mrinmaya Sachan. He received his BSc in computer science from Peking University. He is currently interested in structured prediction, parsing, and natural language generation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"24aa23770a64f329ed6e3676915264ca","permalink":"https://rycolab.io/authors/tianyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianyu/","section":"authors","summary":"Tianyu is a second-year PhD student at ETH Zurich. He is advised by Ryan and Mrinmaya Sachan. He received his BSc in computer science from Peking University. He is currently interested in structured prediction, parsing, and natural language generation.","tags":null,"title":"Tianyu Liu","type":"authors"},{"authors":["tim"],"categories":null,"content":"I develop machine learning algorithms for tough problems—tending toward applications in natural language processing and programming languages. I am currently a postdoctoral researcher working with Ryan Cotterell. I did my PhD with Jason Eisner at Johns Hopkins University. I\u0026rsquo;ve also worked with Andrew McCallum on Rexa and Factorie; Dan Roth on Textual Entailment. I did my undergraduate degree at the University of Illinois Urbana-Champaign.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"07d172c41c903101879fc5085cda4837","permalink":"https://rycolab.io/authors/tim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tim/","section":"authors","summary":"I develop machine learning algorithms for tough problems—tending toward applications in natural language processing and programming languages. I am currently a postdoctoral researcher working with Ryan Cotterell. I did my PhD with Jason Eisner at Johns Hopkins University.","tags":null,"title":"Tim Vieira","type":"authors"},{"authors":["vicky"],"categories":null,"content":"Vasiliki is a MSc Data Science student at ETH Zürich. She received her Diploma in Electrical and Computer Engineering in National Technical University of Athens, where her diploma thesis focused on the semantic enrichment of pre-trained models with knowledge graph representation. She is interested in a variety of topics around NLP, including formal language theory and modern language models. In her spare time, she enjoys reading books, traveling and painting, while in summer she likes island hopping in Greece.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ef296f827f65ff94c8a69eff1ac64857","permalink":"https://rycolab.io/authors/vicky/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vicky/","section":"authors","summary":"Vasiliki is a MSc Data Science student at ETH Zürich. She received her Diploma in Electrical and Computer Engineering in National Technical University of Athens, where her diploma thesis focused on the semantic enrichment of pre-trained models with knowledge graph representation.","tags":null,"title":"Vasiliki Xefteri","type":"authors"},{"authors":["vesteinn"],"categories":null,"content":"Vésteinn is a PhD student at the University of Copenhagen. His thesis project spans the boundaries of computer vision and natural language processing where he hopes to analyse images using language. His advisors are Serge Belongie at the University of Copenhagen and Ryan Cotterell at ETH Zürich. He is also affiliated with Icelandic language technology company Miðeind ehf. In the last couple of years his work has focused on question answering for Icelandic (dataset, OpenQA), neural machine translation and language modeling for Icelandic.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7223a1d37b0c7cad06358ef2294ac4a9","permalink":"https://rycolab.io/authors/vesteinn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vesteinn/","section":"authors","summary":"Vésteinn is a PhD student at the University of Copenhagen. His thesis project spans the boundaries of computer vision and natural language processing where he hopes to analyse images using language.","tags":null,"title":"Vésteinn Snæbjarnarson","type":"authors"},{"authors":["vilem"],"categories":null,"content":"Vilém Zouhar (Vilda) is a PhD student in Mrinmaya Sachan\u0026rsquo;s lab. In the past he worked primarily on machine translation but is now studying a variety of NLP topics, especially those with ties to compression and optimization theory. After finishing his undergraduate at Charles University, he obtained his master degrees at Groningen University and Saarland University. While waiting for the models to converge, he enjoys board games and amateur singing and playing instruments.\nNative Language: Czech\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5ecf27b5796498be6b3e9726e7a79637","permalink":"https://rycolab.io/authors/vilem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vilem/","section":"authors","summary":"Vilém Zouhar (Vilda) is a PhD student in Mrinmaya Sachan\u0026rsquo;s lab. In the past he worked primarily on machine translation but is now studying a variety of NLP topics, especially those with ties to compression and optimization theory.","tags":null,"title":"Vilém Zouhar","type":"authors"},{"authors":["yufei"],"categories":null,"content":"Yufei is an incoming PhD student and a third-year undergraduate student in the École Polytechnique, France, double-majoring in mathematics and computer science. She is supervised by Ryan for the ETH Zürich Summer Student Research Fellowship. She is currently working on removing gender bias from contextual word representations.\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"32f3ce7af08bb902238d53f01fad01cd","permalink":"https://rycolab.io/authors/yufei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yufei/","section":"authors","summary":"Yufei is an incoming PhD student and a third-year undergraduate student in the École Polytechnique, France, double-majoring in mathematics and computer science. She is supervised by Ryan for the ETH Zürich Summer Student Research Fellowship.","tags":null,"title":"Yufei Liu","type":"authors"},{"authors":["zhijing"],"categories":null,"content":"Zhijing Jin (she/her) is a Ph.D. at Max Planck Institute \u0026amp; ETH Zürich. Her research goals are two-fold: (1) to expand the impact of NLP by promoting NLP for social good, and (2) to improve NLP models by connecting NLP with causal inference. She is also co-supervised by Prof Bernhard Schoelkopf. Previously, she was a research intern at Amazon AI (2019-2020) supervised by Prof Zheng Zhang, David Wipf, and Prof Xipeng Qiu. She has published at many NLP and AI venues (e.g., AAAI, ACL, EMNLP, NAACL, COLING, AISTATS), and NLP for healthcare venues (e.g., AAHPM, JPSM). Her work has been cited in MIT News, ACM TechNews, WeVolver, VentureBeat, and Synced. She is actively involved in AI for social good, as the organizer of NLP for Positive Impact Workshop at ACL 2021, and RobustML workshop at ICLR 2021. To support the NLP research community, she organizes the ACL Year-Round Mentorship Program. To foster the causality research community, she is the Publications Chair for the 1st conference on Causal Learning and Reasoning (CLeaR).\nNative language: Mandarin Chinese, Shanghainese\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e1535db07c72c6729cb46dc38e37c50c","permalink":"https://rycolab.io/authors/zhijing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhijing/","section":"authors","summary":"Zhijing Jin (she/her) is a Ph.D. at Max Planck Institute \u0026amp; ETH Zürich. Her research goals are two-fold: (1) to expand the impact of NLP by promoting NLP for social good, and (2) to improve NLP models by connecting NLP with causal inference.","tags":null,"title":"Zhijing Jin","type":"authors"},{"authors":["Ethan Wilcox","Clara Meister","Ryan Cotterell","Tiago Pimentel"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112806,"objectID":"a27495066ef9e0c071c6e35cfb31049c","permalink":"https://rycolab.io/publication/wilcoxal-emnlp-23/","publishdate":"2023-12-20T23:16:44.920286Z","relpermalink":"/publication/wilcoxal-emnlp-23/","section":"publication","summary":"Surprisal theory (Hale, 2001; Levy, 2008) posits that a word’s reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word’s ground-truth probability, surprisal theory has been empirically tested using surprisal estimates from language models (LMs). Under the premise that surprisal theory holds, we would expect that higher quality language models provide more powerful predictors of human reading behavior---a conjecture we dub the quality--power (QP) hypothesis. Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations. In this work, we conduct a systematic crosslinguistic assessment of the QP hypothesis. We train LMs from scratch on small- and medium-sized datasets from 13 languages (across five language families) and assess their ability to predict eye tracking data. We find correlations between LM quality and power in eleven of these thirteen languages, suggesting that, within the range of model classes and sizes tested, better language models are indeed better predictors of human language processing behaviors.","tags":[],"title":"Language Model Quality Correlates with Psychometric Predictive Power in Multiple Languages","type":"publication"},{"authors":["Tianyu Liu","Afra Amini","Mrinmaya Sachan","Ryan Cotterell"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112813,"objectID":"b95778673414a9702a4ab6efa7eb1137","permalink":"https://rycolab.io/publication/liual-emnlp-23/","publishdate":"2023-12-20T23:16:52.799018Z","relpermalink":"/publication/liual-emnlp-23/","section":"publication","summary":"Tasks that model the relation between pairs of tokens in a string are a vital part of understanding natural language. Such tasks, in general, require exhaustive pair-wise comparisons of tokens, thus having a quadratic runtime complexity in the length of the string. We show that these exhaustive comparisons can be avoided, and, moreover, the complexity of such tasks can be reduced to linear by casting the relation between tokens as a partial order over the string. Our method predicts real numbers for each token in a string in parallel and sorts the tokens accordingly, resulting in total orders of the tokens in the string. Each total order implies a set of arcs oriented from smaller to greater tokens, sorted by their predicted numbers. The intersection of total orders results in a partial order over the set of tokens in the string, which is then decoded into a directed graph representing the desired linguistic structure. Our experiments on dependency parsing and coreference resolution show that our method achieves state-of-the-art or comparable performance. Moreover, the linear complexity and parallelism of our method double the speed of graph-based coreference resolution models, and bring a 10-times speed-up over graph-based dependency parsers.","tags":[],"title":"Linear-Time Modeling of Linguistic Structure: An Order-Theoretic Perspective","type":"publication"},{"authors":["Tiago Pimentel","Clara Meister","Ethan Wilcox","Kyle Mahowald","Ryan Cotterell"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112805,"objectID":"50ad82450f4f824c6c6c3c56cc2c9484","permalink":"https://rycolab.io/publication/pimentelal-emnlp-23-a/","publishdate":"2023-12-20T23:16:44.662923Z","relpermalink":"/publication/pimentelal-emnlp-23-a/","section":"publication","summary":"","tags":[],"title":"On the Optimality of Word Lengths","type":"publication"},{"authors":["Franz Nowak","Anej Svete","Li Du","Ryan Cotterell"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112813,"objectID":"97e088871c63db0d014abd8d498a4826","permalink":"https://rycolab.io/publication/nowakal-emnlp-23/","publishdate":"2023-12-20T23:16:52.538349Z","relpermalink":"/publication/nowakal-emnlp-23/","section":"publication","summary":"This work investigates the computational expressivity of language models (LMs) based on recurrent neural networks (RNNs). Siegelmann and Sontag (1992) famously showed that RNNs with rational weights and hidden states and unbounded computation time are Turing complete. However, LMs define weightings over strings in addition to just (unweighted) language membership and the analysis of the computational power of RNN LMs (RLMs) should reflect this. We extend the Turing completeness result to the probabilistic case, showing how a rationally weighted RLM with unbounded computation time can simulate any probabilistic Turing machine (PTM). Since, in practice, RLMs work in real-time, processing a symbol at every time step, we treat the above result as an upper bound on the expressivity of RLMs. We also provide a lower bound by showing that under the restriction to real-time computation, such models can simulate deterministic real-time rational PTMs.","tags":[],"title":"On the Representational Capacity of Recurrent Neural Language Models","type":"publication"},{"authors":["Lukas Wolf","Tiago Pimentel","Evelina Fedorenko","Ryan Cotterell","Alex Warstadt","Ethan Wilcox","Tamar Regev"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112813,"objectID":"8263449d1cc5fd8aed24a14d82ae0264","permalink":"https://rycolab.io/publication/wolfal-emnlp-23/","publishdate":"2023-12-20T23:16:52.028059Z","relpermalink":"/publication/wolfal-emnlp-23/","section":"publication","summary":"","tags":[],"title":"Quantifying the redundancy between prosody and text","type":"publication"},{"authors":["Afra Amini","Li Du","Ryan Cotterell"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112813,"objectID":"aa90a0da42d38d508c198c15e91bd518","permalink":"https://rycolab.io/publication/aminial-neurips-23/","publishdate":"2023-12-20T23:16:52.281764Z","relpermalink":"/publication/aminial-neurips-23/","section":"publication","summary":"","tags":[],"title":"Structured Voronoi Sampling","type":"publication"},{"authors":["Andreas Opedal","Eleftheria Tsipidi","Tiago Pimentel","Ryan Cotterell","Tim Vieira"],"categories":[],"content":"","date":1701388800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112812,"objectID":"c88740ca4580959bcecc7c2bb8883fcb","permalink":"https://rycolab.io/publication/opedalal-emnlp-23/","publishdate":"2023-12-20T23:16:51.771222Z","relpermalink":"/publication/opedalal-emnlp-23/","section":"publication","summary":"","tags":[],"title":"The Generalized Left-Corner Transformation","type":"publication"},{"authors":["Jaap Jumelet"],"categories":null,"content":" Bio Jaap Jumelet is a PhD candidate in the group of Jelle Zuidema at the ILLC, University of Amsterdam. The topic of his PhD lies at the intersection of explainable AI and natural language processing. He is interested in uncovering the linguistic capacities of current NLP models, and developing new techniques that allow to gain these insights in a robust and faithful way.\n","date":1700643600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1700643600,"objectID":"0f38bef852b51a2d67794643a9b31562","permalink":"https://rycolab.io/talk/jumelet-nov-22-23/","publishdate":"2023-06-14T14:20:00+02:00","relpermalink":"/talk/jumelet-nov-22-23/","section":"talk","summary":"We present a setup for training, evaluating and interpreting neural language models, that uses artificial, language-like data. The data is generated using a massive probabilistic grammar (based on state-split PCFGs), that is itself derived from a large natural language corpus, but also provides us complete control over the generative process. We describe and release both grammar and corpus, and test for the naturalness of our generated data. This approach allows us to define closed-form expressions to efficiently compute exact lower bounds on obtainable perplexity using both causal and masked language modelling. Our results show striking differences between neural language modelling architectures and training objectives in how closely they allow approximating the lower bound on perplexity. Our approach also allows us to directly compare learned representations to symbolic rules in the underlying source. We experiment with various techniques for interpreting model behaviour and learning dynamics. With access to the underlying true source, our results show striking differences and outcomes in learning dynamics between different classes of words.","tags":[],"title":"Transparency at the Source: Evaluating and Interpreting Language Models With Access to the True Distribution","type":"talk"},{"authors":["John Terilla"],"categories":null,"content":" Bio John Terilla is Professor of Mathematics at Queens College and on the Doctoral Faculty at the City University of New York Graduate Center. His research is in algebraic topology, quantum physics, and artificial intelligence, and he is interested in understanding the mathematics at work in natural language.\n","date":1699432200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699432200,"objectID":"08e001464c133120d57ee16c1f30b1a0","permalink":"https://rycolab.io/talk/terilla-nov-08-23/","publishdate":"2023-06-14T14:20:00+02:00","relpermalink":"/talk/terilla-nov-08-23/","section":"talk","summary":"Classical probability distributions on sets of sequences can be modeled using quantum states. Here, we do so with a quantum state that is pure and entangled. Because it is entangled, the reduced densities that describe subsystems also carry information about the complementary subsystem. This is in contrast to the classical marginal distributions on a subsystem in which information about the complementary system has been integrated out and lost. A training algorithm based on the density matrix renormalization group (DMRG) procedure uses the extra information contained in the reduced densities and organizes it into a tensor network model. An understanding of the extra information contained in the reduced densities allow us to examine the mechanics of this DMRG algorithm and study the generalization error of the resulting model. As an illustration, we work with the even-parity dataset and produce an estimate for the generalization error as a function of the fraction of the dataset used in training.","tags":[],"title":"Modeling Sequences with Quantum States","type":"talk"},{"authors":["Franz Nowak","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913025,"objectID":"fed51facd614ca0e143e838dfa478dab","permalink":"https://rycolab.io/publication/nowakcotterell-acl-23/","publishdate":"2023-12-20T23:16:48.077145Z","relpermalink":"/publication/nowakcotterell-acl-23/","section":"publication","summary":"Multiple algorithms are known for efficiently calculating the prefix probability of a string under a probabilistic context-free grammar (PCFG). Good algorithms for the problem have a runtime cubic in the length of the input string. However, some proposed algorithms are suboptimal with respect to the size of the grammar.This paper proposes a new speed-up of Jelinek and Lafferty’s (1991) algorithm, which runs in O(n3|N|3 + |N|4), where n is the input length and |N| is the number of non-terminals in the grammar. In contrast, our speed-up runs in O(n2|N|3 + n3|N|2).","tags":[],"title":"A Fast Algorithm for Computing Prefix Probabilities","type":"publication"},{"authors":["Vilém Zouhar","Clara Meister","Juan Gastaldi","Li Du","Tim Vieira","Mrinmaya Sachan","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913023,"objectID":"39990b437ae8cc811ec94579f26ab4a7","permalink":"https://rycolab.io/publication/zouharal-acl-findings-23/","publishdate":"2023-12-20T23:16:46.506206Z","relpermalink":"/publication/zouharal-acl-findings-23/","section":"publication","summary":"Byte-Pair Encoding (BPE) is a popular algorithm used for tokenizing data in NLP, despite being devised initially as a compression method. BPE appears to be a greedy algorithm at face value, but the underlying optimization problem that BPE seeks to solve has not yet been laid down. We formalize BPE as a combinatorial optimization problem. Via submodular functions, we prove that the iterative greedy version is a 1σ(μ⋆)(1−e−σ(μ⋆))-approximation of an optimal merge sequence, where σ(μ⋆) is the total backward curvature with respect to the optimal merge sequence μ⋆. Empirically the lower bound of the approximation is ≈0.37.     We provide a faster implementation of BPE which improves the runtime complexity from O(NM) to O(NlogM), where N is the sequence length and M is the merge count. Finally, we optimize the brute-force algorithm for optimal BPE using memoization.","tags":[],"title":"A Formal Perspective on Byte-Pair Encoding","type":"publication"},{"authors":["Li Du","Lucas Torroba Hennigen","Tiago Pimentel","Clara Meister","Jason Eisner","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688918378,"objectID":"04948714bc508235bb6d88c216b9aa21","permalink":"https://rycolab.io/publication/dual-acl-23-a/","publishdate":"2023-12-20T23:16:48.341353Z","relpermalink":"/publication/dual-acl-23-a/","section":"publication","summary":"Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can “leak” onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.","tags":[],"title":"A Measure-theoretic Characterization of Tight Language Model","type":"publication"},{"authors":["Niklas Stoehr","Lucas Torroba Hennigen","Josef Valvoda","Robert West","Ryan Cotterell","Aaron Schein"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913026,"objectID":"4ae8770e94e731b73f1aa854aafec1d0","permalink":"https://rycolab.io/publication/stoehral-acl-23/","publishdate":"2023-12-20T23:16:49.684314Z","relpermalink":"/publication/stoehral-acl-23/","section":"publication","summary":"Measuring the intensity of events is crucial for monitoring and tracking armed conflict. Advances in automated event extraction have yielded massive data sets of “who did what to whom” micro-records that enable data-driven approaches to monitoring conflict. The Goldstein scale is a widely-used expert-based measure that scores events on a conflictual–cooperative scale. It is based only on the action category (“what”) and disregards the subject (“who”) and object (“to whom”) of an event, as well as contextual information, like associated casualty count, that should contribute to the perception of an event’s “intensity”. This paper takes a latent variable-based approach to measuring conflict intensity. We introduce a probabilistic generative model that assumes each observed event is associated with a latent intensity class. A novel aspect of this model is that it imposes an ordering on the classes, such that higher-valued classes denote higher levels of intensity. The ordinal nature of the latent variable is induced from naturally ordered aspects of the data (e.g., casualty counts) where higher values naturally indicate higher intensity. We evaluate the proposed model both intrinsically and extrinsically, showing that it obtains comparatively good held-out predictive performance.","tags":[],"title":"An Ordinal Latent Variable Model of Conflict Intensity","type":"publication"},{"authors":["Alexandra Butoi","Ryan Cotterell","David Chiang"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913026,"objectID":"29f348c714481fa9ba0c9c04cc37884d","permalink":"https://rycolab.io/publication/butoial-acl-23/","publishdate":"2023-12-20T23:16:49.153117Z","relpermalink":"/publication/butoial-acl-23/","section":"publication","summary":"Weir has defined a hierarchy of language classes whose second member ($mathcalL_2$) is generated by tree-adjoining grammars (TAG), linear indexed grammars (LIG), combinatory categorial grammars, and head grammars. The hierarchy is obtained using the mechanism of control, and $mathcalL_2$ is obtained using a context-free grammar (CFG) whose derivations are controlled by another CFG. We adapt Weir's definition of a controllable CFG to give a definition of controllable pushdown automata (PDAs). This yields three new characterizations of $mathcalL_2$ as the class of languages generated by PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs. We show that these four formalisms are not only weakly equivalent but equivalent in a stricter sense that we call d-weak equivalence. Furthermore, using an even stricter notion of equivalence called d-strong equivalence, we make precise the intuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an embedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this family, a CFG controlling a PDA, does not correspond to any formalism we know of, so we invent one and call it a Pushdown Adjoining Automaton.","tags":[],"title":"Convergence and Diversity in the Control Hierarchy","type":"publication"},{"authors":["Yuchen Eleanor Jiang","Tianyu Liu","Shuming Ma","Dongdong Zhang","Ryan Cotterell","Mrinmaya Sachan"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112808,"objectID":"c2ff513e394bdd891e5a483727a912bc","permalink":"https://rycolab.io/publication/jiangal-acl-23/","publishdate":"2023-12-20T23:16:47.28731Z","relpermalink":"/publication/jiangal-acl-23/","section":"publication","summary":"Several recent papers claim to have achieved human parity at sentence-level machine translation (MT)—especially between high-resource language pairs. In response, the MT community has, in part, shifted its focus to document-level translation. Translating documents requires a deeper understanding of the structure and meaning of text, which is often captured by various kinds of discourse phenomena such as consistency, coherence, and cohesion. However, this renders conventional sentence-level MT evaluation benchmarks inadequate for evaluating the performance of context-aware MT systems. This paperpresents a new dataset with rich discourse annotations, built upon the large-scale parallel corpus BWB introduced in Jiang et al. (2022a). The new BWB annotation introduces four extra evaluation aspects, i.e., entity, terminology, coreference, and quotation, covering 15,095 entity mentions in both languages. Using these annotations, we systematically investigate the similarities and differences between the discourse structures of source and target languages, and the challenges they pose to MT. We discover that MT outputs differ fundamentally from human translations in terms of their latent discourse structures. This gives us a new perspective on the challenges and opportunities in document-level MT. We make our resource publicly available to spur future research in document-level MT and its generalization to other language translation tasks.","tags":[],"title":"Discourse-Centric Evaluation of Document-level Machine Translation with a New Densely Annotated Parallel Corpus of Novels","type":"publication"},{"authors":["Andreas Opedal","Ran Zmigrod","Tim Vieira","Ryan Cotterell","Jason Eisner"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913025,"objectID":"fe218a3889c6aedeb39c2d064c9a99d1","permalink":"https://rycolab.io/publication/opedalal-acl-23/","publishdate":"2023-12-20T23:16:48.607182Z","relpermalink":"/publication/opedalal-acl-23/","section":"publication","summary":"We present Earley’s (1970) context-free parsing algorithm as a deduction system, incorporating various known and new speed-ups. In particular, our presentation supports a known worst-case runtime improvement from Earley’s (1970) O(N3|G||R|), which is unworkable for the large grammars that arise in natural language processing, to O(N3|G|), which matches the complexity of CKY on a binarized version of the grammar G. Here N is the length of the sentence, |R| is the number of productions in G, and |G| is the total length of those productions. We also provide a version that achieves runtime of O(N3|M|) with |M| leq |G| when the grammar is represented compactly as a single finite-state automaton M (this is partly novel). We carefully treat the generalization to semiring-weighted deduction, preprocessing the grammar like Stolcke (1995) to eliminate the possibility of deduction cycles, and further generalize Stolcke’s method to compute the weights of sentence prefixes. We also provide implementation details for efficient execution, ensuring that on a preprocessed grammar, the semiring-weighted versions of our methods have the same asymptotic runtime and space requirements as the unweighted methods, including sub-cubic runtime on some grammars.","tags":[],"title":"Efficient Semiring-Weighted Earley Parsing","type":"publication"},{"authors":["Kevin Du","Lucas Torroba Hennigen","Niklas Stoehr","Alex Warstadt","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112808,"objectID":"8b932f2cbc6c207d0f68995e46c15832","permalink":"https://rycolab.io/publication/dual-acl-23-b/","publishdate":"2023-12-20T23:16:47.553279Z","relpermalink":"/publication/dual-acl-23-b/","section":"publication","summary":"Many popular feature-attribution methods for interpreting deep neural networks rely on computing the gradients of a model's output with respect to its inputs. While these methods can indicate which input features may be important for the model's prediction, they reveal little about the inner workings of the model itself. In this paper, we observe that the gradient computation of a model is a special case of a more general formulation using semirings.  This observation allows us to generalize the backpropagation algorithm to efficiently compute other interpretable statistics about the gradient graph of a neural network, such as the highest-weighted path and entropy.  We implement this generalized algorithm, evaluate it on synthetic datasets to better understand the statistics it computes, and apply it to study BERT's behavior on the subject--verb number agreement task (SVA).  With this method, we (a) validate that the amount of gradient flow through a component of a model reflects its importance to a prediction and (b) for SVA, identify which pathways of the self-attention mechanism are most important.","tags":[],"title":"Generalizing Backpropagation for Gradient-Based Interpretability","type":"publication"},{"authors":["Afra Amini$^*$","Tianyu Liu$^*$","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913026,"objectID":"f3d5ebe0352cf681f065f2d1f2c0b830","permalink":"https://rycolab.io/publication/aminial-acl-23/","publishdate":"2023-12-20T23:16:48.885003Z","relpermalink":"/publication/aminial-acl-23/","section":"publication","summary":"We introduce a novel dependency parser, the hexatagger, that constructs dependency trees by tagging the words in a sentence with elements from a finite set of possible tags. In contrast to many approaches to dependency parsing, our approach is fully parallelizable at training time, i.e., the structure-building actions needed to build a dependency parse can be predicted in parallel to each other. Additionally, exact decoding is linear in time and space complexity. Furthermore, we derive a probabilistic dependency parser that predicts hexatags using no more than a linear model with features from a pretrained language model, i.e., we forsake a bespoke architecture explicitly designed for the task. Despite the generality and simplicity of our approach, we achieve state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test set. Additionally, our parser’s linear time complexity and parallelism significantly improve computational efficiency, with a roughly 10-times speed-up over previous state-of-the-art models during decoding.","tags":[],"title":"Hexatagging: Projective Dependency Parsing as Tagging","type":"publication"},{"authors":["Clara Meister","Tiago Pimentel","Gian Wiher","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986972,"objectID":"af28acb7a468ecd25a2c0054280f2134","permalink":"https://rycolab.io/publication/meisteral-tacl-22/","publishdate":"2023-12-20T23:16:51.250618Z","relpermalink":"/publication/meisteral-tacl-22/","section":"publication","summary":"Today's probabilistic language generators fall short when it comes to producing coherent and fluent text, despite the fact that the underlying models perform incredibly well in terms of standard metrics such as perplexity.   This dichotomy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process can provide new insights into the behavior of probabilistic language generators, e.g., why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: those for which each word has an information content close to the emphexpected information content, i.e., the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.","tags":[],"title":"Locally Typical Sampling","type":"publication"},{"authors":["Shauli Ravfogel","Yoav Goldberg","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913024,"objectID":"25c71e770fab3c9680270a64062490ce","permalink":"https://rycolab.io/publication/ravfogelal-acl-23/","publishdate":"2023-12-20T23:16:47.030627Z","relpermalink":"/publication/ravfogelal-acl-23/","section":"publication","summary":"Methods for erasing human-interpretable concepts from neural representations that assume linearity have been found to be tractable and useful. However, the impact of this removal on the behavior of downstream classifiers trained on the modified representations is not fully understood. In this work, we formally define the notion of log-linear guardedness as the inability of an adversary to predict the concept directly from the representation, and study its implications. We show that, in the binary case, under certain assumptions, a downstream log-linear model cannot recover the erased concept. However, we demonstrate that a multiclass log-linear model emphcan be constructed that indirectly recovers the concept in some cases, pointing to the inherent limitations of log-linear guardedness as a downstream bias mitigation technique. These findings shed light on the theoretical limitations of linear erasure methods and highlight the need for further research on the connections between intrinsic and extrinsic bias in neural models.","tags":[],"title":"Log-Linear Guardedness and Its Implications","type":"publication"},{"authors":["Afra Amini","Tiago Pimentel","Clara Meister","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986972,"objectID":"1144d4708c8baf1d9ecceb86ae209e7f","permalink":"https://rycolab.io/publication/aminial-tacl-22/","publishdate":"2023-12-20T23:16:50.992935Z","relpermalink":"/publication/aminial-tacl-22/","section":"publication","summary":"Probing has become a go-to methodology for interpreting and analyzing deep neural models in natural language processing. However, there is still a lack of understanding of the limitations and weaknesses of various types of probes. In this work, we suggest a strategy for input-level intervention on naturalistic sentences. Using our approach, we intervene on the morpho-syntactic features of a sentence, while keeping the rest of the sentence unchanged. Such an intervention allows us to causally probe pre-trained models. We apply our naturalistic causal probing framework to analyze the effects of grammatical gender and number on contextualized representations extracted from three pre-trained models in Spanish: the multilingual versions of BERT, RoBERTa, and GPT-2. Our experiments suggest that naturalistic interventions lead to stable estimates of the causal effects of various linguistic properties. Moreover, our experiments demonstrate the importance of naturalistic causal probing when analyzing pre-trained models.","tags":[],"title":"Naturalistic Causal Probing for Morpho-Syntax","type":"publication"},{"authors":["Tiago Pimentel","Clara Meister","Ethan G. Wilcox","Roger Levy","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112811,"objectID":"0f55157ae253d37b296f1321e5551147","permalink":"https://rycolab.io/publication/pimentelal-tacl-23/","publishdate":"2023-12-20T23:16:50.472437Z","relpermalink":"/publication/pimentelal-tacl-23/","section":"publication","summary":"Over the past two decades, numerous studies have demonstrated how less predictable (i.e. higher surprisal) words take more time to read. In general, these previous studies implicitly assumed the reading process to be purely responsive: readers observe a new word and allocate time to read it as required. These results, however, are also compatible with a reading time that is anticipatory: readers could, e.g., allocate time to a future word based on their expectation about it. In this work, we examine the anticipatory nature of reading by looking at how people's predictions about upcoming material influence reading times. Specifically, we test anticipation by looking at the effects of surprisal and contextual entropy on four reading-time datasets: two self-paced and two eye-tracking. In three of four datasets tested, we find that the entropy predicts reading times as well as (or better than) the surprisal. We then hypothesise four cognitive mechanisms through which the contextual entropy could impact RTs -- three of which we design experiments to analyse. Overall, our results support a view of reading that is both anticipatory and responsive.","tags":[],"title":"On the Effect of Anticipation on Reading Times","type":"publication"},{"authors":["Clara Meister","Tiago Pimentel","Luca Malagutti","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913026,"objectID":"c523ec78e9656d582472b8518a6ef199","permalink":"https://rycolab.io/publication/meisteral-acl-23/","publishdate":"2023-12-20T23:16:49.416216Z","relpermalink":"/publication/meisteral-acl-23/","section":"publication","summary":"Sampling-based decoding strategies are widely employed for generating text from probabilistic models, yet standard ancestral sampling often results in text that is degenerate or incoherent. To alleviate this issue, various modifications to a model’s sampling distribution, such as top-p or top-k sampling, have been introduced and are now ubiquitously used in language generation systems. We propose a unified framework for understanding these techniques, which we term sampling adapters. Sampling adapters often lead to qualitatively better text, which raises the question: From a formal perspective, how are they changing the token-level distributions of language generation models? And why do these local changes lead to higher-quality text? We argue that the shift they enforce can be viewed as a trade-off between precision and recall: while the model loses its ability to produce certain strings, its precision rate on desirable text increases. While this trade-off is not reflected in standard metrics of distribution quality (such as perplexity), we find that several precision-emphasizing measures indeed indicate that sampling adapters can lead to probability distributions more aligned with the true distribution. Further, these measures correlate with higher sequence-level quality scores, specifically, Mauve.","tags":[],"title":"On the Efficacy of Sampling Adapters","type":"publication"},{"authors":["Ethan G. Wilcox","Tiago Pimentel","Clara Meister","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112811,"objectID":"2d0c22153a44b4d81a388095b4168c3a","permalink":"https://rycolab.io/publication/wilcoxal-tacl-23/","publishdate":"2023-12-20T23:16:50.733937Z","relpermalink":"/publication/wilcoxal-tacl-23/","section":"publication","summary":"A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, is predictive of reading times; (iii) and whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to-date between information theory and incremental language processing across languages.","tags":[],"title":"Testing the Predictions of Surprisal Theory in 11 Languages","type":"publication"},{"authors":["Vilém Zouhar","Clara Meister","Juan Gastaldi","Li Du","Mrinmaya Sachan","Ryan Cotterell"],"categories":[],"content":"","date":1688169600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913025,"objectID":"40d8d293763634740fb3ad754488196a","permalink":"https://rycolab.io/publication/zouharal-acl-23/","publishdate":"2023-12-20T23:16:47.813335Z","relpermalink":"/publication/zouharal-acl-23/","section":"publication","summary":"Subword tokenization is a key part of most NLP pipelines.However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution.Nevertheless, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency subwords and very short codes to high-frequency subwords.Defining efficiency in terms of Rényi entropy, on the other hand, penalizes distributions with either very high or very low-frequency subwords.We posit that (1) extremely high-frequency subwords are problematic because their meaning is not distinct and (2) that low-frequency subwords may not appear frequently enough for their meaning to be learned properly; encodings that induce unigram distributions with either can harm model performance.In machine translation, we find that across multiple tokenizers, the Rényi entropy has a very strong correlation with BLEU: 0.82 in comparison to just -0.30 for compressed length.","tags":[],"title":"Tokenization and the Noiseless Channel","type":"publication"},{"authors":["Krishna Pillutla"],"categories":null,"content":" Bio Krishna Pillutla is a visiting researcher (postdoc) at Google Research in the Federated Learning team. He is broadly interested in machine learning, artificial intelligence, optimization, robustness, and privacy, and specifically in the setting of federated learning, and text generation. He obtained his PhD from the Paul G. Allen School of Computer Science \u0026amp; Engineering at the University of Washington.\n","date":1686916800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686916800,"objectID":"4eab5fdefb7003c557544e375e49b005","permalink":"https://rycolab.io/talk/pillutla-jun-16-23/","publishdate":"2023-06-14T14:20:00+02:00","relpermalink":"/talk/pillutla-jun-16-23/","section":"talk","summary":"Generative AI has matured to a point where large-scale models can generate text that seems indistinguishable from human-written text and remarkably photorealistic images. Automatically measuring how close the distribution of generated data is to the target real data distribution is a key step in diagnosing existing models and developing better models.\nWe present MAUVE, a family of comparison measures between pairs of distributions such as those encountered in the generative modeling of text or images. These scores are statistical summaries of divergence frontiers capturing two types of errors in generative modeling. We explore the statistical estimation of these frontiers and study their rates.\nEmpirically, we find that the proposed scores paired with a range of f-divergences and statistical estimation methods can quantify the gaps between the distributions of human-written text and those of modern neural language models by correlating with human judgments and identifying known properties of the generated texts. We conclude by discussing the applications to other AI domains and some future directions.","tags":[],"title":"MAUVE Scores for Generative Models: Theory and Practice","type":"talk"},{"authors":["Anna Rogers"],"categories":null,"content":" Bio Anna Rogers is an assistant professor in the Computer Science Department at the IT University of Copenhagen. She has a wide range of research interests that span intersection of linguistics, natural language processing, and machine learning. She is also currently a co-program-chair of ACL 2023.\n","date":1686313800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1686313800,"objectID":"71ca0d2df1cf2643fd8b607496865486","permalink":"https://rycolab.io/talk/rogers-jun-8-23/","publishdate":"2023-06-08T16:10:00+02:00","relpermalink":"/talk/rogers-jun-8-23/","section":"talk","summary":"The continued growth of LLMs and their wide-scale adoption in commercial applications such as chatGPT make it increasingly important to (a) develop ways to source their training data in a more transparent way, and (b) to investigate it, both for research and for ethical issues. This talk will discuss the current state of affairs and some data governance lessons learned from Big Science, an open-source effort to train a multilingual LLM - including an ongoing effort for investigating the 1.6 Tb multilingual ROOTS corpus.","tags":[],"title":"Data Governance and Transparency for Large Language Models","type":"talk"},{"authors":["Tom Kocmi"],"categories":null,"content":" Bio Tom Kocmi is a researcher at Microsoft Translator focusing on human and automatic evaluation of machine translation. He coordinates the annual WMT General MT shared task where researchers from both academia and industry compete to build the best performing MT systems.\n","date":1684917000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1684917000,"objectID":"288b42e50af82a52b462fe055acb550d","permalink":"https://rycolab.io/talk/kocmi-may-24-23/","publishdate":"2023-05-22T11:20:00+02:00","relpermalink":"/talk/kocmi-may-24-23/","section":"talk","summary":"For years, the progress in modeling has outpaced the evaluation in NLP, where we relied predominantly on string-based matching metrics. In this talk, we will outline the benefits and differences among the three classes of metrics: n-gram matching (such as ChrF or BLEU), pretrained models (COMET, BLEURT), and black-box LLMs (GEMBA). We will primarily focus on the emerging evaluation based on LLMs, highlighting open questions and challenges anticipated in the upcoming era.","tags":[],"title":"The Evolution of Automatic Metrics, From String Matching to Black-Box LLM","type":"talk"},{"authors":["Mario Giulianelli"],"categories":null,"content":" Bio Mario Giulianelli is a PhD student at the Institute for Logic, Language and Computation of the University of Amsterdam. His research is mainly on human strategies of communication using computational models of language understanding and generation.\n","date":1683102600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1683102600,"objectID":"d4c238d439a2dbce561432ef497571cd","permalink":"https://rycolab.io/talk/giulianelli-may-03-23/","publishdate":"2023-05-02T23:00:00+02:00","relpermalink":"/talk/giulianelli-may-03-23/","section":"talk","summary":"Any unique language production context affords speakers with multiple plausible communicative intents, and any intent can be produced in multiple plausible ways—given the same story prompt, for example, different humans may tell very different stories. Using multiple-reference datasets, we characterise the extent to which human production varies lexically, syntactically, and semantically across four production (text generation) tasks. We then inspect the space of plausible alternative productions as shaped by a text generator’s predicted probability distribution and decoding algorithm. For each test input, we probe the system’s production variability and measure its alignment to the variability exhibited by humans. We analyse language models and decoding strategies and find that (i) models overestimate variability on open-ended tasks and underestimate it on more constrained tasks, and (ii) decoding algorithms have relatively little influence on a model’s alignment to human variability. Our study draws connections between variability and uncertainty in language production and suggests ways to exploit this link, and our methods, in future work.","tags":[],"title":"On the alignment of production variability in humans and text generators","type":"talk"},{"authors":["Clemente Pasti","Andreas Opedal","Tiago Pimentel","Tim Vieira","Jason Eisner","Ryan Cotterell"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913023,"objectID":"1533842f56237045bc663dba630bec2e","permalink":"https://rycolab.io/publication/pastial-eacl-23/","publishdate":"2023-12-20T23:16:45.978891Z","relpermalink":"/publication/pastial-eacl-23/","section":"publication","summary":"The Bar-Hillel construction is a classic result in formal language theory. It shows, by construction, that the intersection between a context-free language and a regular language is itself context-free. However, neither its original formulation (Bar-Hillel et al., 1961) nor its weighted extension (Nederhof and Satta, 2003) can handle automata with ϵ-arcs. In this short note, we generalize the Bar-Hillel construction to correctly compute the intersection even when the automaton contains ϵ-arcs. We further prove that our generalized construction leads to a grammar that encodes the structure of both the input automaton and grammar while retaining the asymptotic size of the original construction.","tags":[],"title":"On the Intersection of Context-Free and Regular Languages","type":"publication"},{"authors":["Tiago Pimentel$^*$","Clara Meister$^*$","Ryan Cotterell"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913023,"objectID":"ab021e76427cde3dfca43986ed6a6bd2","permalink":"https://rycolab.io/publication/pimentelal-iclr-23/","publishdate":"2023-12-20T23:16:46.243058Z","relpermalink":"/publication/pimentelal-iclr-23/","section":"publication","summary":"A good automatic evaluation metric for language generation ideally correlates highly with human judgements of text quality. Yet, there is a dearth of such metrics, which inhibits the rapid and efficient progress of language generators. One exception is the recently proposed Mauve. In theory, Mauve measures an information-theoretic divergence between two probability distributions over strings: one representing the language generator under evaluation; the other representing the true natural language distribution. Mauve's authors argue that its success comes from the qualitative properties of their proposed divergence. Yet in practice, as this divergence is uncomputable, Mauve approximates it by measuring the divergence between multinomial distributions over clusters instead, where cluster assignments are attained by grouping strings based on a pre-trained language model's embeddings. As we show, however, this is not a tight approximation -- in either theory or practice. This begs the question: why does Mauve work so well? In this work, we show that Mauve was right for the wrong reasons, and that its newly proposed divergence is not necessary for its high performance. In fact, classical divergences paired with its proposed cluster-based approximation may actually serve as better evaluation metrics. We finish the paper with a probing analysis; this analysis leads us to conclude that -- by encoding syntactic- and coherence-level features of text, while ignoring surface-level features -- such cluster-based substitutes to string distributions may simply be better for evaluating state-of-the-art language generators.","tags":[],"title":"On the Usefulness of Embeddings, Clusters and Strings for Text Generation Evaluation","type":"publication"},{"authors":["Niklas Stoehr","Ryan Cotterell","Aaron Schein"],"categories":[],"content":"","date":1682899200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913023,"objectID":"84d7e3f7d38038e52ec3c1583408cbff","permalink":"https://rycolab.io/publication/stoehral-eacl-23/","publishdate":"2023-12-20T23:16:45.713047Z","relpermalink":"/publication/stoehral-eacl-23/","section":"publication","summary":"Sentiment analysis has become a central tool in various disciplines outside of natural language processing. In particular in applied and domain-specific settings with strong requirements for interpretable methods, dictionary-based approaches are still a popular choice. However, existing dictionaries are often limited in coverage, static once annotation is completed and sentiment scales differ widely; some are discrete others continuous. We propose a Bayesian generative model that learns a composite sentiment dictionary as an interpolation between six existing dictionaries with different scales. We argue that sentiment is a latent concept with intrinsically ranking-based characteristics — the word “excellent” may be ranked more positive than “great” and “okay”, but it is hard to express how much more exactly. This prompts us to enforce an ordinal scale of ordered discrete sentiment values in our dictionary. We achieve this through an ordering transformation in the priors of our model. We evaluate the model intrinsically by imputing missing values in existing dictionaries. Moreover, we conduct extrinsic evaluations through sentiment classification tasks. Finally, we present two extension: first, we present a method to augment dictionary-based approaches with word embeddings to construct sentiment scales along new semantic axes. Second, we demonstrate a Latent Dirichlet Allocation-inspired variant of our model that learns document topics that are ordered by sentiment.","tags":[],"title":"Sentiment as an Ordinal Latent Variable","type":"publication"},{"authors":["Mitja Nikolaus"],"categories":null,"content":" Bio Mitja Nikolaus is currently finishing his PhD with Abdellah Fourtassi at Aix-Marseille University. Mitja is a cognitive scientist who uses machine learning to study how children acquire language. He\u0026rsquo;s especially interested in communicative feedback and multimodal learning.\n","date":1680597000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1680597000,"objectID":"748b7ee1349cf879a0b31137e2063e67","permalink":"https://rycolab.io/talk/nikolaus-apr-02-23/","publishdate":"2023-04-03T00:40:00+02:00","relpermalink":"/talk/nikolaus-apr-02-23/","section":"talk","summary":"Children start to communicate and use language in social interactions from a very young age. This allows them to experiment with their developing linguistic knowledge and receive valuable feedback from their interlocutors. While research in language acquisition has focused a great deal on children's ability to learn from the linguistic input or social cues, little work, in comparison, has investigated the nature and role of Communicative Feedback, a process that results from children and caregivers trying to coordinate mutual understanding. In this presentation, I will draw on insights from theories of communicative coordination to formalize a mechanism for language acquisition: We argue that children can improve their linguistic knowledge in conversation by leveraging explicit or implicit signals of communication success or failure. This new formalization provides a common framework for several lines of research in child development and will enable us to obtain a more complete understanding of language acquisition within and through social interaction. Finally, I will present a recent corpus study that highlights the role of Communicative Feedback as a mechanism supporting the production of intelligible speech in early childhood as well as preliminary results regarding the role of such feedback mechanisms for learning the grammar of one's native language.","tags":[],"title":"Communicative Feedback in Language Acquisition","type":"talk"},{"authors":["Niklas Stoehr","Benjamin J. Radford","Ryan Cotterell","Aaron Schein"],"categories":[],"content":"","date":1680307200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913022,"objectID":"fd9da1986cf64cf366610439579e6957","permalink":"https://rycolab.io/publication/stoehral-aistats-23/","publishdate":"2023-12-20T23:16:45.450658Z","relpermalink":"/publication/stoehral-aistats-23/","section":"publication","summary":"Many dynamical systems in the real world are naturally described by latent states with intrinsic orderings, such as \\\"ally\\\", \\\"neutral\\\", and \\\"enemy\\\" relationships in international relations. These latent states manifest through countries' cooperative versus conflictual interactions over time. State-space models (SSMs) explicitly relate the dynamics of observed measurements to transitions in latent states. For discrete data, SSMs commonly do so through a state-to-action emission matrix and a state-to-state transition matrix. This paper introduces the Ordered Matrix Dirichlet (OMD) as a prior distribution over ordered stochastic matrices wherein the discrete distribution in the kth row stochastically dominates the (k+1)th, such that probability mass is shifted to the right when moving down rows. We illustrate the OMD prior within two SSMs: a hidden Markov model, and a novel dynamic Poisson Tucker decomposition model tailored to international relations data. We find that models built on the OMD recover interpretable ordered latent structure without forfeiting predictive performance. We suggest future applications to other domains where models with stochastic matrices are popular (e.g., topic modeling), and publish user-friendly code.","tags":[],"title":"The Ordered Matrix Dirichlet for State-Space Models","type":"publication"},{"authors":["Shayne Sloggett"],"categories":null,"content":" Bio Shayne Sloggett has been the Experimental Officer in Psycholinguistics at the University of York\u0026rsquo;s Department of Language and Linguistic Science since September 2019. His research interests are sentence processing, syntax, and the interaction of grammatical knowledge and sentence comprehension routines. This research draws on insights from linguistic theory to better inform psycholinguistic models (and vice versa), using a range of methods from formal acceptability rating tasks, to eye-tracking while reading and corpus linguistics.\n","date":1677756600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677756600,"objectID":"54c7bf98d834cea94113029a918648b5","permalink":"https://rycolab.io/talk/sloggett-mar-02-23/","publishdate":"2023-02-27T13:00:00+02:00","relpermalink":"/talk/sloggett-mar-02-23/","section":"talk","summary":"One of the more surprising (or at least, counter-intuitive) findings in current sentence processing is the fact that fully ambiguous structures like (1) seem to be processed more easily than their disambiguated counterparts like (2): The brother of the colonel who shot himself on the balcony had been very depressed. The daughter of the colonel who shot himself on the balcony had been very depressed. This so-called \"ambiguity advantage\" in sentence processing (Traxler, Pickering, \u0026 Clifton 1998) poses a serious challenge to attempts at unnifying syntactic and lexical ambiguity resolution strategies (MacDonald, Pearlmutter, \u0026 Seidenberg 1994, i.m.a.), and there remains widespread disagreement over the preferred explanation. In this talk, we focus on addressing one prominent account of the ambiguity advantage: strategic underspecification. This hypothesis suggests that syntactic ambiguity is easier to process because comprehenders may not fully parse material when the task context doesn't demand it (Swets, Desmet, Clifton, \u0026 Ferreira, 2008). We address this hypothesis by attempting to replicate the initial findings of Swets et al. in three different task contexts: self-paced reading, eye-tracking while reading, and the maze task. Our findings consistently show that the task context does not (straightforwardly) impact the ambiguity advantage, favouring alternative explanations of the phenomenon (Van Gompel, Pickering, \u0026 Traxler, 2001; Levy, 2008; Logačev \u0026 Vasishth, 2016; Dillon, Andrews, Rotello, \u0026 Wagers, 2019). We conclude by suggesting (i) our findings reinforce the difference between syntactic and lexical processing (ii) we need to be cautious when applying \"good enough\" explanations to effects of interest.","tags":[],"title":"\"Ambiguous\" isn't \"Underspecified\": Evidence from three tasks","type":"talk"},{"authors":["Leonie Weissweiler"],"categories":null,"content":" Bio Leonie Weissweiler is a PhD student in Computational Linguistics at LMU Munich, working with Hinrich Schütze, as well as with Lori Levin and David Mortensen at CMU. Her research interests are applying cognitively plausible theories of linguistics to NLP, as well as unsupervised crosslingual computational morphosyntax.\n","date":1677670200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1677670200,"objectID":"869f926d19f2f18af9ed291c777314e9","permalink":"https://rycolab.io/talk/weissweiler-mar-01-23/","publishdate":"2023-02-22T13:00:00+02:00","relpermalink":"/talk/weissweiler-mar-01-23/","section":"talk","summary":"Construction Grammar (CxG) is a paradigm from cognitive linguistics that emphasises the connection between syntax and semantics. Rather than rules that operate on lexical items, it posits constructions as the central building blocks of language, i.e., linguistic units of different granularity that combine syntax and semantics. When combined with novel probing methodology, this theory enables us to ask interesting questions about the performance, and holes therein, of large language models. In this talk, we will briefly introduce Construction Grammar for an NLP audience. We then report on the methodology and results of several empirical studies focusing on language models' behaviour on specific constructions, exemplifying how Construction Grammar can be used to shine a light on properties and current challenges of language models. We also outline how larger, more generalisable conclusions could be drawn, by utilising existing resources from linguistics, combined with further support from the NLP community.","tags":[],"title":"Everything is a Construction: New Goals for Syntactic and Semantic Probing","type":"talk"},{"authors":["Jordan Boyd-Graber"],"categories":null,"content":" Bio Jordan Boyd-Graber is an associate professor in the University of Maryland\u0026rsquo;s Computer Science Department, iSchool, UMIACS, and Language Science Center. He generally works on how humans can interact with AI tools, starting first with topic models, then translation, then negotiation, and most recently question answering. He and his students have won \u0026ldquo;best of\u0026rdquo; awards at NIPS (2009, 2015), NAACL (2016), and CoNLL (2015). Jordan also won the British Computing Society\u0026rsquo;s 2015 Karen Spärk Jones Award and a 2017 NSF CAREER award.\n","date":1673438400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673438400,"objectID":"66d8850374476eb7194c6d15a248c55c","permalink":"https://rycolab.io/talk/boyd-graber-jan-11-23/","publishdate":"2023-01-11T16:05:00+02:00","relpermalink":"/talk/boyd-graber-jan-11-23/","section":"talk","summary":"AI tools are ubiquitous, but most users treat it as a black box: a handy tool that suggests purchases, flags spam, or autocompletes text. While researchers have presented explanations for making AI less of a black box, a lack of metrics make it hard to optimize explicitly for interpretability. Thus, I propose two metrics for interpretability suitable for unsupervised and supervised AI methods. For unsupervised topic models, I discuss our proposed \"intruder\" interpretability metric, how it contradicts the previous evaluation metric for topic models (perplexity), and discuss its uptake in the community over the last decade. For supervised question answering approaches, I show how human-computer cooperation can be measured and directly optimized by a multi-armed bandit approach to learn what kinds of explanations help specific users. I will then briefly discuss how similar setups can help users navigate information-rich domains like fact checking, translation, and web search.","tags":[],"title":"If We Want AI to be Interpretable, We Need to Measure Interpretability","type":"talk"},{"authors":["Thomas Clark","Clara Meister","Tiago Pimentel","Michael Hahn","Richard Futrell","Ryan Cotterell","Roger Levy"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913024,"objectID":"f2c99c39498a0792b1bd341a9604fc9f","permalink":"https://rycolab.io/publication/clarkal-tacl-23/","publishdate":"2023-12-20T23:16:46.771146Z","relpermalink":"/publication/clarkal-tacl-23/","section":"publication","summary":"While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: the uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.","tags":[],"title":"A Cross-Linguistic Pressure for Uniform Information Density in Word Order","type":"publication"},{"authors":["Karolina Stańczak","Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell","Isabelle Augenstein"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688738400,"objectID":"b45962446dd5c1a217ca7090f0062035","permalink":"https://rycolab.io/publication/stanczakal-aaai-23/","publishdate":"2023-12-20T23:16:45.186067Z","relpermalink":"/publication/stanczakal-aaai-23/","section":"publication","summary":"The success of pre-trained contextualized representations has prompted researchers to analyze them for the presence of linguistic information. Indeed, it is natural to assume that these pre-trained representations do encode some level of linguistic knowledge as they have brought about large empirical improvements on a wide variety of NLP tasks, which suggests they are learning true linguistic generalization. In this work, we focus on intrinsic probing, an analysis technique where the goal is not only to identify whether a representation encodes a linguistic attribute but also to pinpoint where this attribute is encoded. We propose a novel latent-variable formulation for constructing intrinsic probes and derive a tractable variational approximation to the log-likelihood. Our results show that our model is versatile and yields tighter mutual information estimates than two intrinsic probes previously proposed in the literature. Finally, we find empirical evidence that pre-trained representations develop a cross-lingually entangled notion of morphosyntax.","tags":[],"title":"A Latent-Variable Model for Intrinsic Probing","type":"publication"},{"authors":["Wangchunshu Zhou","Yuchen Jiang","Ethan Wilcox","Ryan Cotterell","Mrinmaya Sachan"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688913028,"objectID":"2a6ceeb5009ed920ca567da1dd2c0343","permalink":"https://rycolab.io/publication/zhoual-icml-23/","publishdate":"2023-12-20T23:16:49.949123Z","relpermalink":"/publication/zhoual-icml-23/","section":"publication","summary":"Large language models generate fluent texts and can follow natural language instructions to solve a wide range of tasks without task-specific training. Nevertheless, it is notoriously difficult to control their generation to satisfy the various constraints required by different applications. In this work, we present InstructCTG, a controlled text generation framework that incorporates different constraints by conditioning on natural language descriptions and demonstrations of the constraints. In particular, we first extract the underlying constraints of natural texts through a combination of off-the-shelf NLP tools and simple heuristics. We then verbalize the constraints into natural language instructions to form weakly supervised training data. By prepending natural language descriptions of the constraints and a few demonstrations, we fine-tune a pre-trained language model to incorporate various types of constraints. Compared to existing search-based or score-based methods, InstructCTG is more flexible to different constraint types and has a much smaller impact on the generation quality and speed because it does not modify the decoding procedure. Additionally, InstructCTG allows the model to adapt to new constraints without re-training through the use of few-shot task generalization and in-context learning abilities of instruction-tuned language models.","tags":[],"title":"Controlled Text Generation with Natural Language Instructions","type":"publication"},{"authors":["Josef Valvoda","Simone Teufel","Ryan Cotterell"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986972,"objectID":"88d05f16a709fbeb3f013392a3430628","permalink":"https://rycolab.io/publication/valvodaal-tacl-22/","publishdate":"2023-12-20T23:16:51.508343Z","relpermalink":"/publication/valvodaal-tacl-22/","section":"publication","summary":"Every legal case sets a precedent by devel- oping the law in one of the following two ways. It either expands its scope, in which case it sets positive precedent, or it narrows it down, in which case it sets negative precedent. While legal outcome prediction, which is nothing other than the prediction of positive precedents, is an increasingly pop- ular task in AI, we are the first to investigate negative precedent prediction by focusing on negative outcomes. We discover an asymmetry in existing models’ ability to predict positive and negative outcomes. Where state-of-the-art outcome prediction models predicts positive outcomes at 75.06 F1, they predicts negative outcomes at only 10.09 F1, worse than a random baseline. To address this performance gap, we develop two new models inspired by the dynamics of a court process. Our first model significantly improves positive outcome prediction score to 77.15 F1 and our second model more than doubles the negative outcome prediction performance to 24.01 F1. Despite this improvement, shifting focus to negative outcomes reveals that there is still plenty of room to grow when it comes to modelling law.","tags":[],"title":"On the Role of Negative Precedent in Legal Outcome Prediction","type":"publication"},{"authors":["Karolina Stańczak","Sagnik Ray Choudhury","Tiago Pimentel","Ryan Cotterell","Isabelle Augenstein"],"categories":[],"content":"","date":1672531200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1703112811,"objectID":"b148362dbacbe3543d21ab269c0a1ae7","permalink":"https://rycolab.io/publication/stanczakal-plosone-23/","publishdate":"2023-12-20T23:16:50.208041Z","relpermalink":"/publication/stanczakal-plosone-23/","section":"publication","summary":"While the prevalence of large pre-trained language models has led to significant improvements in the performance of NLP systems, recent research has demonstrated that these models inherit societal biases extant in natural language. In this paper, we explore a simple method to probe pre-trained language models for gender bias, which we use to effect a multi-lingual study of gender bias towards politicians. We construct a dataset of 250k politicians from most countries in the world and quantify adjective and verb usage around those politicians' names as a function of their gender. We conduct our study in 7 languages across 6 different language modeling architectures. Our results demonstrate that stance towards politicians in pre-trained language models is highly dependent on the language used. Finally, contrary to previous findings, our study suggests that larger language models do not tend to be significantly more gender-biased than smaller ones.","tags":[],"title":"Quantifying Gender Bias Towards Politicians in Cross-Lingual Language Models","type":"publication"},{"authors":["Anej Svete","Benjamin Dayan","Tim Vieira","Ryan Cotterell","Jason Eisner"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975466,"objectID":"c2109e6fff15e4a8490d9485bc454bbc","permalink":"https://rycolab.io/publication/sveteal-emnlp-22/","publishdate":"2023-07-09T14:51:12.346875Z","relpermalink":"/publication/sveteal-emnlp-22/","section":"publication","summary":"Weighted finite-state automata (WSFAs) are commonly used in NLP. Failure transitions are a useful extension for compactly representing backoffs or interpolation in n-gram models and CRFs, which are special cases of WFSAs. The pathsum in ordinary acyclic WFSAs is efficiently computed by the backward algorithm in time O(∣E∣), where E is the set of transitions. However, this does not allow failure transitions, and preprocessing the WFSA to eliminate failure transitions could greatly increase ∣E∣. We extend the backward algorithm to handle failure transitions directly. Our approach is efficient when the average state has outgoing arcs for only a small fraction s ≪ 1 of the alphabet Σ. We propose an algorithm for general acyclic WFSAs which runs in O(∣E∣ + s∣Σ∣∣Q∣∣Tmax∣ log ∣Σ∣), where Q is the set of states and ∣Tmax∣ is the size of the largest connected component of failure transitions. When the failure transition topology satisfies a condition exemplified by CRFs, the ∣Tmax∣ factor can be dropped, and when the weight semiring is a ring, the log ∣Σ∣ factor can be dropped. In the latter case (ring-weighted acyclic WFSAs), we also give an alternative algorithm with complexity O(∣E∣ + ∣Σ∣∣Q∣ min(1, s∣πmax∣)), where ∣πmax∣ is the size of the longest failure path.","tags":[],"title":"Algorithms for Weighted Finite-State Automata with Failure Arcs","type":"publication"},{"authors":["Alexandra Butoi","Brian DuSell","Tim Vieira","Ryan Cotterell","David Chiang"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986976,"objectID":"999b845f494d01dc28794b4dabed572a","permalink":"https://rycolab.io/publication/butoial-emnlp-22/","publishdate":"2023-07-09T14:51:12.079389Z","relpermalink":"/publication/butoial-emnlp-22/","section":"publication","summary":"Weighted pushdown automata (WPDAs) are at the core of many natural language processing tasks, like syntax-based statistical machine translation and transition-based dependency parsing. As most existing dynamic programming algorithms are designed for context-free grammars (CFGs), algorithms for PDAs often resort to a PDA-to-CFG conversion. In this paper, we develop novel algorithms that operate directly on WPDAs. Our algorithms are inspired by Lang's algorithm, but use a more general definition of pushdown automaton and either reduce the space requirements by a factor of |Γ|(the size of the stack alphabet) or reduce the runtime by a factor of more than |Q| (the number of states). When run on the same class of PDAs as Lang's algorithm, our algorithm is both more space-efficient by a factor of |Γ| and more time-efficient by a factor of |Q|⋅|Γ|.","tags":[],"title":"Algorithms for Weighted Pushdown Automata","type":"publication"},{"authors":["Tianyu Liu","Yuchen Jiang","Nicholas Monath","Ryan Cotterell","Mrinmaya Sachan"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975466,"objectID":"235a394c42d1bbe8c2569f3fa67e7252","permalink":"https://rycolab.io/publication/liual-emnlp-22/","publishdate":"2023-07-09T14:51:12.8922Z","relpermalink":"/publication/liual-emnlp-22/","section":"publication","summary":"In recent years, NLP has moved towards the application of language models to a more diverse set of tasks. However, applying language models to structured prediction, e.g., predicting parse trees, taggings, and coreference chains, is not straightforward. Prior work on language model-based structured prediction typically flattens the target structure into a string to easily fit it into the language modeling framework. Such flattening limits the accessibility of structural information and can lead to inferior performance compared to approaches that overtly model the structure. In this work, we propose to construct a conditional language model over sequences of structure-building actions, rather than over strings in a way that makes it easier for the model to pick up on intra-structure dependencies. Our method sets the new state of the art on named entity recognition, end-to-end relation extraction, and coreference resolution.","tags":[],"title":"Autoregressive Structure Prediction with Language Models","type":"publication"},{"authors":["Shauli Ravfogel","Francisco Vargas","Yoav Goldberg","Ryan Cotterell"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986974,"objectID":"bc85e5652a657d6a3261e7094d6e93fc","permalink":"https://rycolab.io/publication/ravfogelal-emnlp-22/","publishdate":"2023-07-09T14:51:11.2713Z","relpermalink":"/publication/ravfogelal-emnlp-22/","section":"publication","summary":"One prominent approach for the identification of concepts in neural representations is searching for a linear subspace whose erasure prevents the prediction of the concept from the representations. However, while many linear erasure algorithms are tractable and interpretable, neural networks do not necessarily represent concepts in a linear manner. To identify non-linearly encoded concepts, we propose a kernelization of a linear minimax game for concept erasure. We demonstrate that it is possible to prevent specific non-linear adversaries from predicting the concept. However, the protection does not transfer to different nonlinear adversaries. Therefore, exhaustively erasing a non-linearly encoded concept remains an open problem.","tags":[],"title":"Kernelized Concept Erasure","type":"publication"},{"authors":["Liam van der Poel","Ryan Cotterell","Clara Meister"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975466,"objectID":"d5b4dbf169cd4b3e4b3998bae0bcb3a2","permalink":"https://rycolab.io/publication/vanderpoelal-emnlp-22/","publishdate":"2023-07-09T14:51:12.62802Z","relpermalink":"/publication/vanderpoelal-emnlp-22/","section":"publication","summary":"Despite significant progress in the quality of language generated from abstractive summarization models, these models still exhibit the tendency to hallucinate, i.e., output content not supported by the source document. A number of works have tried to fix—or at least uncover the source of—the problem with limited success. In this paper, we identify a simple criterion under which models are significantly more likely to assign more probability to hallucinated content during generation: high model uncertainty. This finding offers a potential explanation for hallucinations: models default to favoring text with high marginal probability, i.e., high-frequency occurrences in the training set, when uncertain about a continuation. It also motivates possible routes for real-time intervention during decoding to prevent such hallucinations. We propose a decoding strategy that switches to optimizing for pointwise mutual information of the source and target token—rather than purely the probability of the target token—when the model exhibits uncertainty. Experiments on the XSUM dataset show that our method decreases the probability of hallucinated tokens while maintaining the ROUGE and BERTS scores of top-performing decoding strategies.","tags":[],"title":"Mutual Information and Hallucinations in Abstractive Summarization","type":"publication"},{"authors":["Afra Amini","Ryan Cotterell"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986975,"objectID":"52724402e87197cda178d5c1964ec513","permalink":"https://rycolab.io/publication/aminicotterell-emnlp-22/","publishdate":"2023-07-09T14:51:11.537319Z","relpermalink":"/publication/aminicotterell-emnlp-22/","section":"publication","summary":"There have been many proposals to reduce constituency parsing to tagging in the literature. To better understand what these approaches have in common, we cast several existing proposals into a unifying pipeline consisting of three steps: linearization, learning, and decoding. In particular, we show how to reduce tetratagging, a state-of-the-art constituency tagger, to shift--reduce parsing by performing a right-corner transformation on the grammar and making a specific independence assumption. Furthermore, we empirically evaluate our taxonomy of tagging pipelines with different choices of linearizers, learners, and decoders. Based on the results in English and a set of 8 typologically diverse languages, we conclude that the linearization of the derivation tree and its alignment with the input sequence is the most critical factor in achieving accurate taggers.","tags":[],"title":"On Parsing as Tagging","type":"publication"},{"authors":["Tiago Pimentel*","Josef Valvoda*","Niklas Stoehr","Ryan Cotterell"],"categories":[],"content":"","date":1669852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986975,"objectID":"8e970e5a60df84c838463f5987d067a5","permalink":"https://rycolab.io/publication/pimentelal-emnlp-22/","publishdate":"2023-07-09T14:51:11.807329Z","relpermalink":"/publication/pimentelal-emnlp-22/","section":"publication","summary":"In this paper, we seek to measure how much information a component in a neural network could extract from the representations fed into it. Our work stands in contrast to prior probing work, most of which investigates how much information a model's representations contain. This shift in perspective leads us to propose a new principle for probing, the architectural bottleneck principle: In order to estimate how much information a given component could extract, a probe should look exactly like the component. Relying on this principle, we estimate how much syntactic information is available to transformers through our attentional probe, a probe that exactly resembles a transformer's self-attention head. Experimentally, we find that, in three models (BERT, ALBERT, and RoBERTa), a sentence's syntax tree is mostly extractable by our probe, suggesting these models have access to syntactic information while composing their contextual representations. Whether this information is actually used by these models, however, remains an open question.","tags":[],"title":"The Architectural Bottleneck Principle","type":"publication"},{"authors":["Sophie Hao"],"categories":null,"content":" Bio Sophie Hao is a Faculty Fellow (i.e., a postdoc without a supervisor) in Data Science at New York University. Her research is on interpretability and explanability for natural language processing, with the aim of understanding what it means for a deep neural network to be \u0026lsquo;interpreted by\u0026rsquo; or \u0026lsquo;explained to\u0026rsquo; a human audience. She recently completed her PhD in Linguistics and Computer Science with Prof. Robert Frank and Prof. Dana Angluin at Yale University.\n","date":1669032000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669032000,"objectID":"033b26a0bb8e9728009422ba14da7152","permalink":"https://rycolab.io/talk/hao-nov-21-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/hao-nov-21-22/","section":"talk","summary":"I present three case studies in which formal languages offer natural notions of generalization in neural networks. ","tags":[],"title":"Understanding RNNs and Transformers using Formal Languages","type":"talk"},{"authors":["Guy Aglionby"],"categories":null,"content":" Bio Guy Aglionby is a final year PhD student at the University of Cambridge Computer Lab, supervised by Prof Simone Teufel and a member of Homerton College. His PhD research is on developing interpretable models for common sense multi-hop reasoning.\n","date":1668772800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668772800,"objectID":"e65a34bd868aa29c99cccfe651f917c1","permalink":"https://rycolab.io/talk/aglionby-nov-18-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/aglionby-nov-18-22/","section":"talk","summary":"Explainability in question answering allows researchers to check that the model is making the right decision for the right reason. Datasets of explanations are useful for both training and evaluation of models along these lines. However, in common sense question answering, annotators have different opinions about how much detail an explanation should contain. This leads to low inter-annotator agreement. In this talk I will discuss a new annotation procedure that aims to identify only the key facts used when choosing between answers. Rather than directly asking annotators to choose these, we ask them to create explanations for a counterfactual situation and analyse how they differ from the real situation.","tags":[],"title":"Collecting Knowledge Graph Explanations for Commonsense QA via Counterfactual Annotation","type":"talk"},{"authors":["Róbert Csordás"],"categories":null,"content":" Bio Róbert Csordás is a PhD candidate at the Swiss AI lab IDSIA and currently doing an internship at DeepMind. His research interests are systematic generalizationi n the context of algorithmic reasoning and network architectures with nductive biases like information routing (attention, memory) and learning modular structures.\n","date":1668600000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668600000,"objectID":"9a092139e75fcaae290985d8b4cb96a4","permalink":"https://rycolab.io/talk/csordas-nov-16-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/csordas-nov-16-22/","section":"talk","summary":"Systematic generalization is one of the most important open problems of neural networks: given a model trained to solve a certain problem, it will often fail on a test problem with different statistics than the training one, even if the problem should be solvable by the same algorithm.","tags":[],"title":"Principles of Compositionality Improve Systematic Generalization of Neural Networks","type":"talk"},{"authors":["Eric Malmi"],"categories":null,"content":" Bio Eric Malmi is a Senior Research Scientist at Google, Zürich. His research focuses on developing Natural Language Generation (NLG) methods for Google Assistant. He received his PhD (2018) in Computer Science from Aalto University, Finland. During his studies, Eric did internships at Google, Qatar Computing Research Institute, Idiap Research Institute, and CERN.\n","date":1666940400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666940400,"objectID":"d9299d4748ff1bc39ab21edcd95490ae","permalink":"https://rycolab.io/talk/malmi-oct-28-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/malmi-oct-28-22/","section":"talk","summary":"This talk provides an introduction to text-editing models and a closer look at two models: LaserTagger and EdiT5.","tags":[],"title":"Text Generation with Text-Editing Models","type":"talk"},{"authors":["Serge Belongie"],"categories":null,"content":" Bio Serge Belongie is a professor of Computer Science at the University of Copenhagen, where he also serves as the head of the Pioneer Centre for Artificial Intelligence. His research interests include Computer Vision, Machine Learning, Augmented Reality, and Human-in-the-Loop Computing.\n","date":1666873800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666873800,"objectID":"9956d930077889dbce5972026e0d2f62","permalink":"https://rycolab.io/talk/belongie-oct-27-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/belongie-oct-27-22/","section":"talk","summary":"While advances in automated fact-checking are critical in the fight against the spread of misinformation in social media, we argue that more attention is needed in the domain of unfalsifiable claims. In this talk, we outline some promising directions for identifying the prevailing narratives in shared content (image \u0026 text) and explore how the associated learned representations can be used to identify misinformation campaigns and sources of polarization.","tags":[],"title":"Searching for Structure in Unfalsifiable Claims","type":"talk"},{"authors":["Miloš Stanojević"],"categories":null,"content":" Bio Miloš Stanojević is a Senior Research Scientist in DeepMind. Prior to that he did a PostDoc at University of Edinburgh with Mark Steedman where he worked on Combinatory Categorial Grammars (CCG), and collaborated with Ed Stabler on Minimalist Grammars. He has received a PhD degree from University of Amsterdam for the work on machine translation. His main research interests lie in incremental sentence processing, structured inference, formal grammars, and generative linguistics.\n","date":1666180800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666180800,"objectID":"2e8ece3d1d28577f6a895587808c8f4c","permalink":"https://rycolab.io/talk/stanojevic-oct-19-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/stanojevic-oct-19-22/","section":"talk","summary":"In this talk we will look at Combinatory Categorial Grammar (CCG) on three distinct, complementary levels of analysis.","tags":[],"title":"CCG on all levels of analysis: Linguistic Universals, Incremental Processing and Brain Activity","type":"talk"},{"authors":["Jonas Pfeiffer"],"categories":null,"content":" Bio Jonas Pfeiffer is a Research Scientist at Google Research. He is interested in modular representation learning in multi-task, multilingual, and multi-modal contexts, and in low-resource scenarios. He worked on his PhD at the Technical University of Darmstadt, was a visiting researcher at the New York University and a Research Scientist Intern at Meta Research. Jonas has received the IBM PhD Research Fellowship award for 2021/2022. He has given numerous invited talks at academia, industry and ML summer schools, and has co-organized multiple workshops on multilinguality and multimodality.\n","date":1666004400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1666004400,"objectID":"8e618d8eba8485c652dc84c42ffc2f7c","permalink":"https://rycolab.io/talk/pfeiffer-oct-17-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/pfeiffer-oct-17-22/","section":"talk","summary":"With pre-trained transformer-based models continuously increasing in size, there is a dire need for parameter-efficient and modular transfer learning strategies. In this talk, we will touch base on adapter-based fine-tuning, where instead of fine-tuning all weights of a model, small neural network components are introduced at every layer.","tags":[],"title":"Modular and Composable Transfer Learning","type":"talk"},{"authors":["André Martins"],"categories":null,"content":" Bio André Martins (PhD 2012, Carnegie Mellon University and University of Lisbon) is an Associate Professor at Instituto Superior Técnico, University of Lisbon, researcher at Instituto de Telecomunicações, and the VP of AI Research at Unbabel. His research, funded by a ERC Starting Grant (DeepSPIN) and other grants (P2020 project Unbabel4EU and CMU-Portugal project MAIA) include machine translation, quality estimation, structure and interpretability in deep learning systems for NLP. His work has received best paper awards at ACL 2009 (long paper) and ACL 2019 (system demonstration paper). He co-founded and co-organizes the Lisbon Machine Learning School (LxMLS), and he is a Fellow of the ELLIS society.\n","date":1664884800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664884800,"objectID":"b89b3e25bda58a71084f7d207bc22b71","permalink":"https://rycolab.io/talk/martins-oct-4-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/martins-oct-4-22/","section":"talk","summary":"Neural networks and other machine learning models compute continuous representations, while humans communicate mostly through discrete symbols. Reconciling these two forms of communication is desirable for generating human-readable interpretations or learning discrete latent variable models, while maintaining end-to-end differentiability. Some existing approaches (such as the Gumbel-Softmax transformation) build continuous relaxations that are discrete approximations in the zero-temperature limit, while others (such as sparsemax transformations and the Hard Concrete distribution) produce discrete/continuous hybrids. In this talk, I will describe theoretical foundations for these hybrids, which we call \"mixed random variables.\" The starting point is a new \"direct sum\" base measure defined on the face lattice of the probability simplex. From this measure, I introduce a new entropy function that includes the discrete and differential entropies as particular cases, and has an interpretation in terms of code optimality, as well as two other information-theoretic counterparts that generalize the mutual information and Kullback-Leibler divergences. Our framework suggests two strategies for representing and sampling mixed random variables, an extrinsic (\"sample-and-project\") and an intrinsic one (based on face stratification). We experiment with both approaches on an emergent communication benchmark and on modeling MNIST and Fashion-MNIST data with variational auto-encoders with mixed latent variables. Finally, I introduce \"mixed languages\" as strings of hybrid symbols and a new mixed weighted finite state automaton that recognizes a class of regular mixed languages, generalizing closure properties of regular languages.","tags":[],"title":"Reconciling the Discrete-Continuous Divide: Towards a Mathematical Theory of Sparse Communication","type":"talk"},{"authors":["Josef Valvoda","Naomi Saphra","Jon Rawski","Adina Williams","Ryan Cotterell"],"categories":[],"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986974,"objectID":"137904b24fe9379dd16cdf333ff3df17","permalink":"https://rycolab.io/publication/valvodaal-coling-22/","publishdate":"2023-07-09T14:51:10.663851Z","relpermalink":"/publication/valvodaal-coling-22/","section":"publication","summary":"Recombining known primitive concepts into larger novel combinations is a quintessentially human cognitive capability. Whether large neural models in NLP acquire this ability while learning from data is an open question. In this paper, we look at this problem from the perspective of formal languages. We use deterministic finite-state transducers to make an unbounded number of datasets with controllable properties governing compositionality. By randomly sampling over many transducers, we explore which of their properties (number of states, alphabet size, number of transitions etc.) contribute to learnability of a compositional relation by a neural network. In general, we find that the models either learn the relations completely or not at all. The key is transition coverage, setting a soft learnability limit at 400 examples per transition.","tags":[],"title":"Benchmarking Compositionality with Formal Languages","type":"publication"},{"authors":["Jennifer White","Ryan Cotterell"],"categories":[],"content":"","date":1664582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975464,"objectID":"0c76435c2bc53748d768c5d84ae35a06","permalink":"https://rycolab.io/publication/whitecotterell-coling-22/","publishdate":"2023-07-09T14:51:10.945598Z","relpermalink":"/publication/whitecotterell-coling-22/","section":"publication","summary":"The ability to generalize compositionally is key to understanding the potentially infinite number of sentences that can be constructed in a human language from only a finite number of words. Investigating whether NLP models possess this ability has been a topic of interest: SCAN (Lake and Baroni, 2018) is one task specifically proposed to test for this property. Previous work has achieved impressive empirical results using a group-equivariant neural network that naturally encodes a useful inductive bias for SCAN (Gordon et al., 2020). Inspired by this, we introduce a novel group-equivariant architecture that incorporates a group-invariant hard alignment mechanism. We find that our network's structure allows it to develop stronger equivariance properties than existing group-equivariant approaches. We additionally find that it outperforms previous group-equivariant networks empirically on the SCAN task. Our results suggest that integrating group-equivariance into a variety of neural architectures is a potentially fruitful avenue of research, and demonstrate the value of careful analysis of the theoretical properties of such architectures.","tags":[],"title":"Equivariant Transduction through Invariant Alignment","type":"publication"},{"authors":["Shay Cohen"],"categories":null,"content":" Bio Shay Cohen is a Reader at the University of Edinburgh (School of Informatics). Before this, he was a postdoctoral research scientist in the Department of Computer Science at Columbia University and held an NSF/CRA Computing Innovation Fellowship. He received his B.Sc. and M.Sc. from Tel Aviv University in 2000 and 2004, and his Ph.D. from Carnegie Mellon University in 2011. His research interests span a range of topics in natural language processing and machine learning, focusing on structured prediction (for example, parsing) and text generation.\n","date":1664539200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664539200,"objectID":"0178ee0c3fba2787c44501152b172b1d","permalink":"https://rycolab.io/talk/cohen-sep-30-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/cohen-sep-30-22/","section":"talk","summary":"I will present two advances in significantly improving the speed of dependency parsing (EMNLP, 2021) and considerably improving the accuracy of unsupervised constituency parsing (Findings of ACL, 2022).","tags":[],"title":"Faster Dependency Parsing, More Accurate Unsupervised Parsing","type":"talk"},{"authors":["Mark Fišel"],"categories":null,"content":" Bio Mark Fišel is head of NLP at University of Tartu (Estonia) with focus on machine translation, speech synthesis, large language model usage, tuning and analysis, etc.\n","date":1664525400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664525400,"objectID":"f9e5324445b6497343445c264bfb1402","permalink":"https://rycolab.io/talk/fisel-sep-30-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/fisel-sep-30-22/","section":"talk","summary":"Overview of a couple of projects that involved transformer encoder and decoder recombination, analysis and exploitation. This includes modular and partially-shared encoders for machine translation, multilinguality analysis of BERT and GPT-like models as well as experiments on porting success of English large LMs to Estonian.","tags":[],"title":"Encoder-decoder manipulations at TartuNLP","type":"talk"},{"authors":["Tuhin Chakrabarty"],"categories":null,"content":" Bio Tuhin Chakrabarty is a PhD student in Computer Science at Columbia University. Within the department he is a part of the Natural Language Processing group, where he is advised by Smaranda Muresan. His research is supported by the Columbia Center of Artificial Intelligence \u0026amp; Technology (CAIT) \u0026amp; Amazon Science Ph.D. Fellowship. His research interests are broadly in Natural Language Processing and Machine Learning, with special focus in Language Generation. His overarching research question centers around how we can control large language models to understand, interpret or generate creative text.Recently he is also working in Continual Learning of Large language models. https://tuhinjubcse.github.io/\n","date":1659526200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659526200,"objectID":"85e3e1bf631ea0567a6e7f076dba2a8e","permalink":"https://rycolab.io/talk/chakrabarty-aug-8-22/","publishdate":"2022-07-29T00:16:50+02:00","relpermalink":"/talk/chakrabarty-aug-8-22/","section":"talk","summary":"Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models, even though impressive, still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.","tags":[],"title":"Continual-T0: Fine-tuned Language Models are Continual Learners","type":"talk"},{"authors":["David Mortensen"],"categories":null,"content":" Bio David Mortensen is a computational linguist interested in phonology, morphology, language change, linguistic typology, and human-in-the-loop computation. He is currently a Systems Scientist (a non-tenure track research faculty member at the Assistant Professor level) in the Language Technologies Institute, which is part of Carnegie Mellon University’s School of Computer Science. Before coming to CMU, he was an Assistant Professor in the Department of Linguistics at the University of Pittsburgh.\n","date":1658230200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658230200,"objectID":"e2b495f7c1f6cff27e0cabf44aa8bde2","permalink":"https://rycolab.io/talk/mortensen-jul-19-22/","publishdate":"2022-07-19T23:46:48+02:00","relpermalink":"/talk/mortensen-jul-19-22/","section":"talk","summary":"There is wide debate about the degree to which the properties of human cognition affect how languages are structured and how they change over time. This controversy extends to the lexicon. We show that the decline of lexical items can be partially accounted for by biases that have been demonstrated in the cognitive science literature. ","tags":[],"title":"How Cognitive Biases Do (and Do Not) Shape Lexicons: Two Computational Studies","type":"talk"},{"authors":["Noga Zaslavsky"],"categories":null,"content":" Bio Noga Zaslavsky is a postdoc at MIT. Her research aims to understand language, learning, and reasoning from first principles, building on ideas and methods from machine learning and information theory.\n","date":1656936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656936000,"objectID":"b5c0197ec506185743d9a146420704f4","permalink":"https://rycolab.io/talk/zaslavsky-jul-4-22/","publishdate":"2022-07-19T23:46:48+02:00","relpermalink":"/talk/zaslavsky-jul-4-22/","section":"talk","summary":"Our world is extremely complex, and yet we are able to exchange our thoughts and beliefs about it using a relatively small number of words. What computational principles can explain this extraordinary ability?","tags":[],"title":"Losing bits and finding meaning: Efficient compression shapes meaning in language","type":"talk"},{"authors":["Tianyu Liu","Yuchen Eleanor Jiang","Ryan Cotterell","Mrinmaya Sachan"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986969,"objectID":"eb842e2e80b4b6fad1ce09bac8158318","permalink":"https://rycolab.io/publication/liual-naacl-22/","publishdate":"2023-07-09T14:51:06.091982Z","relpermalink":"/publication/liual-naacl-22/","section":"publication","summary":"Many natural language processing tasks, e.g., coreference resolution and semantic role labeling, require selecting text spans and making decisions about them. A typical approach to such tasks is to score all possible spans and greedily select spans for task-specific downstream processing. This approach, however, does not incorporate any inductive bias about what sort of spans ought to be selected, e.g., that selected spans tend to be syntactic constituents. In this paper, we propose a novel grammar-based structured span selection model which learns to make use of the partial span-level annotation provided for such problems. Compared to previous approaches, our approach gets rid of the heuristic greedy span selection scheme, allowing us to model the downstream task on an optimal set of spans. We evaluate our model on two popular span prediction tasks: coreference resolution and semantic role labeling; and show improvements on both.","tags":[],"title":"A Structured Span Selector","type":"publication"},{"authors":["Yuchen Eleanor Jiang","Tianyu Liu","Shuming Ma","Dongdong Zhang","Jian Yang","Haoyang Huang","Rico Sennrich","Ryan Cotterell","Mrinmaya Sachan","Ming Zhou"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986970,"objectID":"e2287db463ef53d795fdd6d6bfefb874","permalink":"https://rycolab.io/publication/jiangal-naacl-22/","publishdate":"2023-07-09T14:51:06.641888Z","relpermalink":"/publication/jiangal-naacl-22/","section":"publication","summary":"Standard automatic metrics, e.g. BLEU, are not reliable for document-level MT evaluation. They can neither distinguish document-level improvements in translation quality from sentence-level ones, nor identify the discourse phenomena that cause context-agnostic translations. This paper introduces a novel automatic metric BlonDe to widen the scope of automatic MT evaluation from sentence to document level. BlonDe takes discourse coherence into consideration by categorizing discourse-related spans and calculating the similarity-based F1 measure of categorized spans. We conduct extensive comparisons on a newly constructed dataset BWB. The experimental results show that BlonDe possesses better selectivity and interpretability at the document-level, and is more sensitive to document-level nuances. In a large-scale human study, BlonDe also achieves significantly higher Pearson’s r correlation with human judgments compared to previous metrics.","tags":[],"title":"BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986970,"objectID":"8610a8d2dabcdc73921686fce54b72f6","permalink":"https://rycolab.io/publication/zmigrodal-naacl-22/","publishdate":"2023-07-09T14:51:06.914047Z","relpermalink":"/publication/zmigrodal-naacl-22/","section":"publication","summary":"Significance testing—especially the paired-permutation test—has played a vital role in developing NLP systems to provide confidence that the difference in performance between two systems (i.e., the test statistic) is not due to luck. However, practitioners rely on Monte Carlo approximation to perform this test due to a lack of a suitable exact algorithm. In this paper, we provide an efficient exact algorithm for the paired-permutation test for a family of structured test statistics. Our algorithm runs in 𝒪(G N (log GN )(log N)) time where N is the dataset size and G is the range of the test statistic. We found that our exact algorithm was 10x faster than the Monte Carlo approximation with 20000 samples on a common dataset","tags":[],"title":"Exact Paired-Permutation Testing for Structured Test Statistics","type":"publication"},{"authors":["Shauli Ravfogel","Michael Twiton","Yoav Goldberg","Ryan Cotterell"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986971,"objectID":"1aa500ff5a6f4b378b7677d0a5f41eb0","permalink":"https://rycolab.io/publication/ravfogelal-icml-22/","publishdate":"2023-07-09T14:51:07.907989Z","relpermalink":"/publication/ravfogelal-icml-22/","section":"publication","summary":"Modern neural models trained on textual data rely on pre-trained representations that emerge without direct supervision. As these representations are increasingly being used in real-world applications, the inability to emphcontrol their content becomes an increasingly important problem. We formulate the problem of identifying and erasing a linear subspace that corresponds to a given concept, in order to prevent linear predictors from recovering the concept. We model this problem as a constrained, linear minimax game, and show that existing solutions are generally not optimal for this task. We derive a closed-form solution for certain objectives, and propose a convex relaxation, R-LACE, that works well for others. When evaluated in the context of binary gender removal, the method recovers a low-dimensional subspace whose removal mitigates bias by intrinsic and extrinsic evaluation. We show that the method -- despite being linear -- is highly expressive, effectively mitigating bias in deep nonlinear classifiers while maintaining tractability and interpretability.","tags":[],"title":"Linear Adversarial Concept Erasure","type":"publication"},{"authors":["Zeerak Talat","Hagen Blix","Josef Valvoda","Maya Indira Ganesh","Ryan Cotterell","Adina Williams"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"b8155b530bd7681a2c466b8463a4440f","permalink":"https://rycolab.io/publication/talatal-naacl-22/","publishdate":"2023-07-09T14:51:07.484796Z","relpermalink":"/publication/talatal-naacl-22/","section":"publication","summary":"Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to address issues of harmful outcomes in machine learning systems that are made to interface with humans. One recent approach in this vein is the construction of NLP morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we offer a critique of such NLP methods for automating ethical decision-making. Through an audit of recent work on computational approaches for predicting morality, we examine the broader issues that arise from such efforts. We conclude with a discussion of how machine ethics could usefully proceed in NLP, by focusing on current and near-future uses of technology, in a way that centers around transparency, democratic values, and allows for straightforward accountability.","tags":null,"title":"On the Machine Learning of Ethical Judgments from Natural Language","type":"publication"},{"authors":["Jiaoda Li","Ryan Cotterell","Mrinmaya Sachan"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986970,"objectID":"f2332cd84dad7d54ff6145b247fcf423","permalink":"https://rycolab.io/publication/lial-naacl-22/","publishdate":"2023-07-09T14:51:07.212645Z","relpermalink":"/publication/lial-naacl-22/","section":"publication","summary":"Probing is a popular approach to understand what linguistic information is contained in the representations of pre-trained language models. However, the mechanism of selecting the probe model has recently been subject to intense debate, as it is not clear if the probes are merely extracting information or modelling the linguistic property themselves. To address this challenge, this paper introduces a novel model-free approach to probing via prompting, which formulates probing as a prompting task. We conduct experiments on five probing tasks and show that PP is comparable or better at extracting information than diagnostic probes while learning much less on its own. We further combine the probing via prompting approach with pruning to analyze where the model stores the linguistic information in its architecture. Finally, we apply the probing via prompting approach to examine the usefulness of a linguistic property for pre-training by removing the heads that are essential to it and evaluating the resulting model’s performance on language modeling.","tags":[],"title":"Probing via Prompting","type":"publication"},{"authors":["Karolina Stańczak","Edoardo Ponti","Lucas Torroba Hennigen","Ryan Cotterell","Isabelle Augenstein"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986969,"objectID":"ecaee66061825591c4310b2e456508d2","permalink":"https://rycolab.io/publication/stanczakal-naacl-22/","publishdate":"2023-07-09T14:51:06.360792Z","relpermalink":"/publication/stanczakal-naacl-22/","section":"publication","summary":"The success of multilingual pre-trained models is underpinned by their ability to learn representations shared by multiple languages even in absence of any explicit supervision. However, it remains unclear how these models learn to generalise across languages. In this work, we conjecture that multilingual pre-trained models can derive language-universal abstractions about grammar. In particular, we investigate whether morphosyntactic information is encoded in the same subset of neurons in different languages.We conduct the first large-scale empirical study over 43 languages and 14 morphosyntactic categories with a state-of-the-art neuron-level probe. Our findings show that the cross-lingual overlap between neurons is significant, but its extent may vary across categories and depends on language proximity and pre-training data size.","tags":[],"title":"Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models","type":"publication"},{"authors":["Jordan Kodner","Salam Khalifa","Khuyagbaatar Batsuren","Hossep Dolatian","Ryan Cotterell","Faruk Akkus","Antonios Anastasopoulos","Taras Andrushko","Aryaman Arora","Nona Atanalov","Gábor Bella","Elena Budianskaya","Yustinus Ghanggo Ate","Omer Goldman","David Guriel","Simon Guriel","Silvia Guriel-Agiashvili","Witold Kieraś","Andrew Krizhanovsky","Natalia Krizhanovsky","Igor Marchenko","Magdalena Markowska","Polina Mashkovtseva","Maria Nepomniashchaya","Daria Rodionova","Karina Scheifer","Alexandra Sorova","Anastasia Yemelina","Jeremiah Young","Ekaterina Vylomova"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975463,"objectID":"2613c0b182f5f242d9b54a3ea22cf6f8","permalink":"https://rycolab.io/publication/kodneral-sigmorphon-22/","publishdate":"2023-07-09T14:51:09.786927Z","relpermalink":"/publication/kodneral-sigmorphon-22/","section":"publication","summary":"The 2022 SIGMORPHON–UniMorph shared task on large scale morphological inflection generation included a wide range of typologically diverse languages: 33 languages from 11 top-level language families: Arabic (Modern Standard), Assamese, Braj, Chukchi, Eastern Armenian, Evenki, Georgian, Gothic, Gujarati, Hebrew, Hungarian, Itelmen, Karelian, Kazakh, Ket, Khalkha Mongolian, Kholosi, Korean, Lamahalot, Low German, Ludic, Magahi, Middle Low German, Old English, Old High German, Old Norse, Polish, Pomak, Slovak, Turkish, Upper Sorbian, Veps, and Xibe. We emphasize generalization along different dimensions this year by evaluating test items with unseen lemmas and unseen features separately under small and large training conditions. Across the five submitted systems and two baselines, the prediction of inflections with unseen features proved challenging, with average performance decreased substantially from last year. This was true even for languages for which the forms were in principle predictable, which suggests that further work is needed in designing systems that capture the various types of generalization required for the world’s languages.","tags":[],"title":"SIGMORPHON--UniMorph 2022 Shared Task 0: Generalization and Typologically Diverse Morphological Inflection","type":"publication"},{"authors":["Khuyagbaatar Batsuren","Gábor Bella","Aryaman Arora","Viktor Martinovic","Kyle Gorman","Zdeněk Žabokrtský","Amarsanaa Ganbold","Šárka Dohnalová","Magda Ševčíková","Kateřina Pelegrinová","Fausto Giunchiglia","Ryan Cotterell","Ekaterina Vylomova"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975464,"objectID":"38c063d20016380be2d175f06cfc702c","permalink":"https://rycolab.io/publication/batsurenal-sigmorphon-22/","publishdate":"2023-07-09T14:51:10.065372Z","relpermalink":"/publication/batsurenal-sigmorphon-22/","section":"publication","summary":"The SIGMORPHON 2022 shared task on morpheme segmentation challenged systems to decompose a word into a sequence of morphemes and covered most types of morphology: compounds, derivations, and inflections. Subtask 1, word-level morpheme segmentation, covered 5 million words in 9 languages (Czech, English, Spanish, Hungarian, French, Italian, Russian, Latin, Mongolian) and received 13 system submissions from 7 teams and the best system averaged 97.29% F1 score across all languages, ranging English (93.84%) to Latin (99.38%). Subtask 2, sentence-level morpheme segmentation, covered 18,735 sentences in 3 languages (Czech, English, Mongolian), received 10 system submissions from 3 teams, and the best systems outperformed all three state-of-the-art subword tokenization methods (BPE, ULM, Morfessor2) by 30.71% absolute. To facilitate error analysis and support any type of future studies, we released all system predictions, the evaluation script, and all gold standard datasets.","tags":[],"title":"The SIGMORPHON 2022 Shared Task on Morpheme Segmentation","type":"publication"},{"authors":["Johann-Mattis List","Ekaterina Vylomova","Robert Forkel","Nathan Hill","Ryan Cotterell"],"categories":[],"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975464,"objectID":"483340c5c2d0c692dac1c2b2fd825ed4","permalink":"https://rycolab.io/publication/listal-sigtyp-22/","publishdate":"2023-07-09T14:51:10.353821Z","relpermalink":"/publication/listal-sigtyp-22/","section":"publication","summary":"This study describes the structure and the results of the SIGTYP 2022 shared task on the prediction of cognate reflexes from multilingual wordlists. We asked participants to submit systems that would predict words in individual languages with the help of cognate words from related languages. Training and surprise data were based on standardized multilingual wordlists from several language families. Four teams submitted a total of eight systems, including both neural and non-neural systems, as well as systems adjusted to the task and systems using more general settings. While all systems showed a rather promising performance, reflecting the overwhelming regularity of sound change, the best performance throughout was achieved by a system based on convolutional networks originally designed for image restoration.","tags":[],"title":"The SIGTYP 2022 Shared Task on the Prediction of Cognate Reflexes","type":"publication"},{"authors":["Aaron Schein"],"categories":null,"content":" Bio Aaron Schein is an Assistant Professor in the Department of Statistics and the Data Science Institute at the University of Chicago. His research develops statistical models and computational methods to analyze modern large-scale data in political science, economics, and genetics, among other fields in the social and natural sciences.\n","date":1654869600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654869600,"objectID":"777f579ae83f140242f77c68c2a622fb","permalink":"https://rycolab.io/talk/schein-jun-10-22/","publishdate":"2022-07-19T23:46:48+02:00","relpermalink":"/talk/schein-jun-10-22/","section":"talk","summary":"Recent mobile app technology lets people systematize the process of messaging their friends to urge them to vote. Prior to the most recent US midterm elections in 2018, the mobile app Outvote randomized an aspect of their system, hoping to unobtrusively assess the causal effect of their users’ messages on voter turnout. However, properly assessing this causal effect is hindered by multiple statistical challenges, including attenuation bias due to mismeasurement of subjects’ outcomes and low precision due to two-sided non-compliance with subjects’ assignments. We address these challenges, which are likely to impinge upon any study that seeks to randomize authentic friend-to-friend interactions, by tailoring the statistical analysis to make use of additional data about both users and subjects. Using meta-data of users’ in-app behavior, we reconstruct subjects’ positions in users’ queues. We use this information to refine the study population to more compliant subjects who were higher in the queues, and we do so in a systematic way which optimizes a proxy for the study’s power. To mitigate attenuation bias, we then use ancillary data of subjects’ matches to the voter rolls that lets us refine the study population to one with low rates of outcome mismeasurement.","tags":[],"title":"Assessing the Effects of Friend-to-Friend Texting on Turnout in the 2018 US Midterm Elections","type":"talk"},{"authors":["Sien Moens"],"categories":null,"content":" Bio Marie-Francine (Sien) Moens is full professor at the Department of Computer Science at KU Leuven, Belgium. She is the director of the Language Intelligence and Information Retrieval (LIIR) research lab and a member of the Human Computer Interaction group. Her main direction of research is the development of novel methods for automated content recognition in text and multimedia using machine learning and exploiting insights from linguistic and cognitive theories.\n","date":1654848900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654848900,"objectID":"1aad8df1602439ae6c450385da7e1511","permalink":"https://rycolab.io/talk/moens-jun-16-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/moens-jun-16-22/","section":"talk","summary":"Translating text to events happening in a 3D virtual world is one way of evaluating machine understanding of human language. This entails several challenges including how to represent sentences and full discourses that are grounded in the physical and social world.","tags":[],"title":"Human Language Understanding: Research of the CALCULUS project","type":"talk"},{"authors":["Clara Meister","Tiago Pimentel","Thomas Hikaru Clark","Ryan Cotterell","Roger P. Levy"],"categories":[],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986968,"objectID":"d4a47aa546801b1900049bdbba27569c","permalink":"https://rycolab.io/publication/meisteral-acl-22-b/","publishdate":"2023-07-09T14:51:04.95019Z","relpermalink":"/publication/meisteral-acl-22-b/","section":"publication","summary":"Numerous analyses of reading time (RT) data have been undertaken in the effort to learn more about the internal processes that occur during reading comprehension. However, data measured on words at the end of a sentence–or even clause–is often omitted due to the confounding factors introduced by so-called “wrap-up effects,” which manifests as a skewed distribution of RTs for these words. Consequently, the understanding of the cognitive processes that might be involved in these effects is limited. In this work, we attempt to learn more about these processes by looking for the existence–or absence–of a link between wrap-up effects and information theoretic quantities, such as word and context information content. We find that the information distribution of prior context is often predictive of sentence- and clause-final RTs (while not of sentence-medial RTs), which lends support to several prior hypotheses about the processes involved in wrap-up effects.","tags":[],"title":"Analyzing Wrap-Up Effects through an Information-Theoretic Lens","type":"publication"},{"authors":["Aryaman Arora","Clara Meister","Ryan Cotterell"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"acee213decb79795ab9d2e135dbc8655","permalink":"https://rycolab.io/publication/aroraal-acl-22/","publishdate":"2023-07-09T14:51:05.543137Z","relpermalink":"/publication/aroraal-acl-22/","section":"publication","summary":"Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language. However, entropymust typically be estimated from observed data because researchers do not have access to the underlying probability distribution. While entropy estimation is a well-studied problem in other fields, there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data. In this work, we fill this void, studying the empirical effectiveness of different entropy estimators for linguistic distributions. In a replication of two recent information-theoretic linguistic studies, we find evidence that the reported effect size is over-estimated due to over-reliance on poor entropy estimators. We end this paper with a concrete recommendation for the entropy estimators that should be used in future linguistic studies.","tags":null,"title":"Estimating the Entropy of Linguistic Distributions","type":"publication"},{"authors":["On the probability-quality paradox in language generation"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"ba3944c5206a4072eb9370c945ec44ee","permalink":"https://rycolab.io/publication/meisteral-acl-22-a/","publishdate":"2023-07-09T14:51:04.689767Z","relpermalink":"/publication/meisteral-acl-22-a/","section":"publication","summary":"When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text—covering multiple tasks and common decoding strategies—suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.","tags":null,"title":"On the probability-quality paradox in language generation","type":"publication"},{"authors":["Alexander Immer","Lucas Torroba Hennigen","Vincent Fortuin","Ryan Cotterell"],"categories":[],"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986968,"objectID":"cc1283c731525434a2bf5d5dfebe761b","permalink":"https://rycolab.io/publication/immeral-acl-22/","publishdate":"2023-07-09T14:51:05.269787Z","relpermalink":"/publication/immeral-acl-22/","section":"publication","summary":"Pre-trained contextual representations have led to dramatic performance improvements on a range of downstream tasks. Such performance improvements have motivated researchers to quantify and understand the linguistic information encoded in these representations. In general, researchers quantify the amount of linguistic information through probing, an endeavor which consists of training a supervised model to predict a linguistic property directly from the contextual representations. Unfortunately, this definition of probing has been subject to extensive criticism in the literature, and has been observed to lead to paradoxical and counter-intuitive results. In the theoretical portion of this paper, we take the position that the goal of probing ought to be measuring the amount of inductive bias that the representations encode on a specific task. We further describe a Bayesian framework that operationalizes this goal and allows us to quantify the representations’ inductive bias. In the empirical portion of the paper, we apply our framework to a variety of NLP tasks. Our results suggest that our proposed framework alleviates many previous problems found in probing. Moreover, we are able to offer concrete evidence that—for some tasks—fastText can offer a better inductive bias than BERT.","tags":[],"title":"Probing as Quantifying the Inductive Bias of Pre-trained Representations","type":"publication"},{"authors":["Karim Lasri","Tiago Pimentel","Alessandro Lenci","Thierry Poibeau","Ryan Cotterell"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"e117f7367d44d174ecb858dd249d98cb","permalink":"https://rycolab.io/publication/lasrial-acl-22/","publishdate":"2023-07-09T14:51:05.810509Z","relpermalink":"/publication/lasrial-acl-22/","section":"publication","summary":"A central quest of probing is to uncover how pre-trained models encode a linguistic property within their representations. An encoding, however, might be spurious—i.e., the model might not rely on it when making predictions. In this paper, we try to find an encoding that the model actually uses, introducing a usage-based probing setup. We first choose a behavioral task which cannot be solved without using the linguistic property. Then, we attempt to remove the property by intervening on the model's representations. We contend that, if an encoding is used by the model, its removal should harm the performance on the chosen behavioral task. As a case study, we focus on how BERT encodes grammatical number, and on how it uses this encoding to solve the number agreement task. Experimentally, we find that BERT relies on a linear encoding of grammatical number to produce the correct behavioral output. We also find that BERT uses a separate encoding of grammatical number for nouns and verbs. Finally, we identify in which layers information about grammatical number is transferred from a noun to its head verb.","tags":null,"title":"Probing for the Usage of Grammatical Number","type":"publication"},{"authors":["Chris Dyer"],"categories":null,"content":" Bio Chris Dyer is a principal scientist with DeepMind in London, UK, and holds an Adjunct Professor appointment in the School of Computer Science at Carnegie Mellon University. His former PhD students have gone on to research positions in major industrial labs, tenure track academic positions (CMU, UC Irvine, USC, and the University of Washington). He is a 2016 recipient of the Presidential Early Career Award for Scientists and Engineers (PECASE). He lives in London and plays the cello.\n","date":1649081700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1649081700,"objectID":"9aaf6c748c875ca5cc9267bcfbce57e2","permalink":"https://rycolab.io/talk/dryer-apr-4-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/dryer-apr-4-22/","section":"talk","summary":"Although the success of transformers is widely acknowledged in the problem of modelling word sequences, they are in fact general-purpose learners, capable of modelling virtually any kind of sequential data. In this talk, I dispute whether a general-purpose learner like this is well-suited to the task of language learning","tags":[],"title":"Syntactic Structure and Transformer Models of Natural Language","type":"talk"},{"authors":["Gail Weiss"],"categories":null,"content":" Bio Gail is a PhD student at Technion working with Eran Yahav and Yoav Goldberg. Her research interest is in understanding sequential neural networks (such as RNNs and transformers) through the lens of formal language theory.\n","date":1646146800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1646146800,"objectID":"7c0fa8e2542f6c2188dfcc513c213fe1","permalink":"https://rycolab.io/talk/weiss-mar-1-22/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/weiss-mar-1-22/","section":"talk","summary":"Transformers - the purely attention based NN architecture - have emerged as a powerful tool in sequence processing. But how does a transformer think?","tags":[],"title":"Thinking like Transformers","type":"talk"},{"authors":["Niklas Stoehr","Lucas Torroba Hennigen","Josef Valvoda","Robert West","Ryan Cotterell","Aaron Schein"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975467,"objectID":"652ef9ff91a5855908d85ff59542210a","permalink":"https://rycolab.io/publication/stoehral-arxiv-22/","publishdate":"2022-11-20T23:29:37.24688Z","relpermalink":"/publication/stoehral-arxiv-22/","section":"publication","summary":"For the quantitative monitoring of international relations, political events are extracted from the news and parsed into \\\"who-did-what-to-whom\\\" patterns. This has resulted in large data collections which require aggregate statistics for analysis. The Goldstein Scale is an expert-based measure that ranks individual events on a one-dimensional scale from conflictual to cooperative. However, the scale disregards fatality counts as well as perpetrator and victim types involved in an event. This information is typically considered in qualitative conflict assessment. To address this limitation, we propose a probabilistic generative model over the full subject-predicate-quantifier-object tuples associated with an event. We treat conflict intensity as an interpretable, ordinal latent variable that correlates conflictual event types with high fatality counts. Taking a Bayesian approach, we learn a conflict intensity scale from data and find the optimal number of intensity classes. We evaluate the model by imputing missing data. Our scale proves to be more informative than the original Goldstein Scale in autoregressive forecasting and when compared with global online attention towards armed conflicts.","tags":[],"title":"An Ordinal Latent Variable Model of Conflict Intensity","type":"publication"},{"authors":["Tiago Pimentel","Clara Meister","Ryan Cotterell"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668986973,"objectID":"53e500c2c3776a5f90ff8a75471d9bbf","permalink":"https://rycolab.io/publication/pimentelal-arxiv-22/","publishdate":"2022-11-20T23:29:33.133493Z","relpermalink":"/publication/pimentelal-arxiv-22/","section":"publication","summary":"While probabilistic language generators have improved dramatically over the last few years, the automatic evaluation metrics used to assess them have not kept pace with this progress. In the domain of language generation, a good metric must correlate highly with human judgements. Yet, with few exceptions, there is a lack of such metrics in the literature. In this work, we analyse the general paradigm of language generator evaluation. We first discuss the computational and qualitative issues with using automatic evaluation metrics that operate on probability distributions over strings, the backbone of most language generators. We then propose the use of distributions over clusters instead, where we cluster strings based on their text embeddings (obtained from a pretrained language model). While we find the biases introduced by this substitution to be quite strong, we observe that, empirically, this methodology leads to metric estimators with higher correlation with human judgements, while simultaneously reducing estimator variance. We finish the paper with a probing analysis, which leads us to conclude that -- by encoding syntactic- and coherence-level features of text, while ignoring surface-level features -- these clusters may simply be better equipped to evaluate state-of-the-art language models.","tags":[],"title":"Cluster-based Evaluation of Automatically Generated Text","type":"publication"},{"authors":["Gian Wiher","Clara Meister","Ryan Cotterell"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975462,"objectID":"12482c2a89690d5ab25b1d3e14b7e83d","permalink":"https://rycolab.io/publication/wiheral-tacl-22/","publishdate":"2023-07-09T14:51:08.296717Z","relpermalink":"/publication/wiheral-tacl-22/","section":"publication","summary":"When generating text from probabilistic models, the chosen decoding strategy has a profound effect on the resulting text. Yet theproperties elicited by various decoding strategies do not always transfer across natural language generation tasks. For example, while mode-seeking methods like beam search perform remarkably well for machine translation, they have been observed to lead to incoherent and repetitive text in story generation. Despite such observations, the effectiveness of decod- ing strategies is often assessed on only a single task. This work—in contrast—provides a comprehensive analysis of the interaction between language generation tasks and decoding strategies. Specifically, we measure changes in attributes of generated text as a function of both decoding strategy and task using human and automatic evaluation. Our results reveal both previously observed and novel findings. For example, the nature of the diversity-quality trade-off in language generation is very task-specific; the length bias often attributed to beam search is not constant across tasks.","tags":[],"title":"On Decoding Strategies for Neural Text Generators","type":"publication"},{"authors":["Clemente Pasti","Andreas Opedal","Tiago Pimentel","Tim Vieira","Jason Eisner","Ryan Cotterell"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975467,"objectID":"005c329d0435210682e569f7a9fd1a7f","permalink":"https://rycolab.io/publication/pastial-arxiv-22/","publishdate":"2022-11-20T23:29:37.605676Z","relpermalink":"/publication/pastial-arxiv-22/","section":"publication","summary":"The Bar-Hillel construction is a classic result in formal language theory. It shows, by construction, that the intersection between a context-free language and a regular language is itself context-free. However, neither its original formulation (Bar-Hillel et al., 1961) nor its weighted extension (Nederhof and Satta, 2003) can handle automata with ϵ-arcs. In this short note, we generalize the Bar-Hillel construction to correctly compute the intersection even when the automaton contains ϵ-arcs. We further prove that our generalized construction leads to a grammar that encodes the structure of both the input automaton and grammar while retaining the asymptotic size of the original construction.","tags":[],"title":"On the Intersection of Context-Free and Regular Languages","type":"publication"},{"authors":["Dieuwke Hupkes","Mario Giulianelli","Verna Dankers","Mikel Artetxe","Yanai Elazar","Tiago Pimentel","Christos Christodoulopoulos","Karim Lasri","Naomi Saphra","Arabella Sinclair","Dennis Ulmer","Florian Schottmann","Khuyagbaatar Batsuren","Kaiser Sun","Koustuv Sinha","Leila Khalatbari","Maria Ryskina","Rita Frieske","Ryan Cotterell","Zhijing Jin"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975466,"objectID":"53a2ff917d3a75431fa453d772e97cca","permalink":"https://rycolab.io/publication/hupkesal-arxiv-22/","publishdate":"2023-07-09T14:51:13.204558Z","relpermalink":"/publication/hupkesal-arxiv-22/","section":"publication","summary":"","tags":[],"title":"State-of-the-art generalisation research in NLP: a taxonomy and review","type":"publication"},{"authors":["Rita Sevastjanova","Eren Cakmak","Shauli Ravfogel","Ryan Cotterell","Mennatallah El-Assady"],"categories":[],"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1668975463,"objectID":"afc332f5f200f93e6d131fdcd68ddcbe","permalink":"https://rycolab.io/publication/sevastjanovaal-vis-22/","publishdate":"2023-07-09T14:51:09.455224Z","relpermalink":"/publication/sevastjanovaal-vis-22/","section":"publication","summary":"Neural language models are widely used; however, their model parameters often need to be adapted to the specific domains and tasks of an application, which is time- and resource-consuming. Thus, adapters have recently been introduced as a lightweight alternative for model adaptation. They consist of a small set of task-specific parameters with a reduced training time and simple parameter composition. The simplicity of adapter training and composition comes along with new challenges, such as maintaining an overview of adapter properties and effectively comparing their produced embedding spaces. To help developers overcome these challenges, we provide a twofold contribution. First, in close collaboration with NLP researchers, we conducted a requirement analysis for an approach supporting adapter evaluation and detected, among others, the need for both intrinsic (i.e., embedding similarity-based) and extrinsic (i.e., prediction-based) explanation methods. Second, motivated by the gathered requirements, we designed a flexible visual analytics workspace that enables the comparison of adapter properties. In this paper, we discuss several design iterations and alternatives for interactive, comparative visual explanation methods. Our comparative visualizations show the differences in the adapted embedding vectors and prediction outcomes for diverse human-interpretable concepts (e.g., person names, human qualities).  We evaluate our workspace through case studies and show that, for instance, an adapter trained on the language debiasing task according to context-0 (decontextualized) embeddings introduces a new type of bias where words (even gender-independent words such as countries) become more similar to female- than male pronouns. We demonstrate that these are artifacts of context-0 embeddings, and the adapter effectively eliminates the gender information from the contextualized word representations.","tags":[],"title":"Visual Comparison of Language Model Adaptation","type":"publication"},{"authors":["Antoine Bosselut"],"categories":null,"content":" Bio Antoine Bosselut is an assistant professor in the School of Computer and Communication Sciences at EPFL. He leads the EPFL NLP group where they conduct research on natural language processing (NLP) systems that can model, represent, and reason about human and world knowledge.\n","date":1637665200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1637665200,"objectID":"c18d8e00d3ef849ad2771d375e03b97a","permalink":"https://rycolab.io/talk/bosselut-nov-23-21/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/bosselut-nov-23-21/","section":"talk","summary":"This talk presents work in designing systems that use knowledge graphs as structural scaffolds for commonsense reasoning in QA systems.","tags":[],"title":"Neuro-symbolic Scaffolds for Commonsense Reasoning","type":"talk"},{"authors":["Zeerak Talat$^*$","Hagen Blix$^*$","Josef Valvoda","Maya Indira Ganesh","Ryan Cotterell","Adina Williams"],"categories":null,"content":"","date":1636156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636156800,"objectID":"8f95a5a044a8db45399f3fc462e193f8","permalink":"https://rycolab.io/publication/talatal-local-21/","publishdate":"2021-11-06T14:17:51.046327Z","relpermalink":"/publication/talatal-local-21/","section":"publication","summary":"Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to wrangle with how learning systems that interact with humans should be constrained to behave ethically. One proposal in this vein is the construction of morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we focus on a single case study of the recently proposed Delphi model and offer a critique of the project’s proposed method of automating morality judgments. Through an analysis of Delphi, we examine broader issues that would be applicable to any similar attempt. We conclude with a discussion of how machine ethics could usefully proceed, by focusing on current and near-future uses of technology, in a way that centers around transparency, democratic values, and allows for straightforward accountability.","tags":null,"title":"A Word on Machine Ethics: A Response to Jiang et al. (2021)","type":"publication"},{"authors":["Tiago Pimentel","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"479d27b86e873c02ae67666bc5bb6ad4","permalink":"https://rycolab.io/emnlp21/pimentelcotterell-emnlp-21/","publishdate":"2021-10-08T07:56:32.687045Z","relpermalink":"/emnlp21/pimentelcotterell-emnlp-21/","section":"emnlp21","summary":"Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents -- allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.","tags":null,"title":"A Bayesian Framework for Information-Theoretic Probing","type":"emnlp21"},{"authors":["Tiago Pimentel","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"f6a55062f00899750cd965563afa6509","permalink":"https://rycolab.io/publication/pimentelcotterell-emnlp-21/","publishdate":"2023-07-09T14:56:57.11172Z","relpermalink":"/publication/pimentelcotterell-emnlp-21/","section":"publication","summary":"Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents -- allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.","tags":null,"title":"A Bayesian Framework for Information-Theoretic Probing","type":"publication"},{"authors":["Damian Pascual","Beni Egressy","Clara Meister","Ryan Cotterell","Roger Wattenhofer"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"ce746d39e90b836a5d22cf20db08abd5","permalink":"https://rycolab.io/emnlp21/pascualal-emnlp-21/","publishdate":"2021-10-08T07:56:33.802722Z","relpermalink":"/emnlp21/pascualal-emnlp-21/","section":"emnlp21","summary":"Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.","tags":null,"title":"A Plug-and-Play Method for Controlled Text Generation","type":"emnlp21"},{"authors":["Damian Pascual","Beni Egressy","Clara Meister","Ryan Cotterell","Roger Wattenhofer"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fd0ba2dbbd1cb1c255b6ca5407598f13","permalink":"https://rycolab.io/publication/pascualal-emnlp-21/","publishdate":"2023-07-09T14:56:54.837663Z","relpermalink":"/publication/pascualal-emnlp-21/","section":"publication","summary":"Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.","tags":null,"title":"A Plug-and-Play Method for Controlled Text Generation","type":"publication"},{"authors":["Tiago Pimentel","Clara Meister","Elizabeth Salesky","Simone Teufel","Damián Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"afa54473bcc2caa6c10c931c60a30bd2","permalink":"https://rycolab.io/emnlp21/pimentelal-emnlp-21-a/","publishdate":"2021-10-08T07:56:33.303646Z","relpermalink":"/emnlp21/pimentelal-emnlp-21-a/","section":"emnlp21","summary":"While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal--duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal--duration trade-off in operation, both across and within the world's languages.","tags":null,"title":"A surprisal--duration trade-off across and within the world's languages","type":"emnlp21"},{"authors":["Tiago Pimentel","Clara Meister","Elizabeth Salesky","Simone Teufel","Damián Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"0cbed81dcb7da785c5fb9d75fa491e3b","permalink":"https://rycolab.io/publication/pimentelal-emnlp-21-a/","publishdate":"2023-07-09T14:56:59.415442Z","relpermalink":"/publication/pimentelal-emnlp-21-a/","section":"publication","summary":"While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal--duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal--duration trade-off in operation, both across and within the world's languages.","tags":null,"title":"A surprisal--duration trade-off across and within the world’s languages","type":"publication"},{"authors":["Niklas Stoehr","Lucas Torroba Hennigen","Samin Ahbab","Robert West","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"af8cb472fb23d34debca785461ad7618","permalink":"https://rycolab.io/emnlp21/stoehral-emnlp-21-a/","publishdate":"2021-10-08T07:56:32.537611Z","relpermalink":"/emnlp21/stoehral-emnlp-21-a/","section":"emnlp21","summary":"Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies.","tags":null,"title":"Classifying Dyads for Militarized Conflict Analysis","type":"emnlp21"},{"authors":["Niklas Stoehr","Lucas Torroba Hennigen","Samin Ahbab","Robert West","Ryan Cotterell"],"categories":[],"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688914350,"objectID":"f9a9e4bff9ed5454b689e2dea6b0c604","permalink":"https://rycolab.io/publication/stoehral-emnlp-21/","publishdate":"2023-07-09T14:56:54.507869Z","relpermalink":"/publication/stoehral-emnlp-21/","section":"publication","summary":"Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies.","tags":[],"title":"Classifying Dyads for Militarized Conflict Analysis","type":"publication"},{"authors":["Clara Meister","Afra Amini","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fed5f9462fb373890e86362d23dde319","permalink":"https://rycolab.io/emnlp21/meisteral-emnlp-21-b/","publishdate":"2021-10-08T07:56:32.842369Z","relpermalink":"/emnlp21/meisteral-emnlp-21-b/","section":"emnlp21","summary":"Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et. al. 2019's stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.","tags":null,"title":"Conditional Poisson Stochastic Beam Search","type":"emnlp21"},{"authors":["Clara Meister","Afra Amini","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"7820403512dddfef6a03747644239b2a","permalink":"https://rycolab.io/publication/meisteral-emnlp-21-b/","publishdate":"2023-07-09T14:56:57.466251Z","relpermalink":"/publication/meisteral-emnlp-21-b/","section":"publication","summary":"Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et. al. 2019's stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.","tags":null,"title":"Conditional Poisson Stochastic Beam Search","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fb6b8389daf62efc56af42fce7d915f5","permalink":"https://rycolab.io/emnlp21/zmigrodal-emnlp-21/","publishdate":"2021-10-08T07:56:33.451426Z","relpermalink":"/emnlp21/zmigrodal-emnlp-21/","section":"emnlp21","summary":"Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no sampling algorithm has been presented in the literature to account for this additional constraint. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a graph subject to the root constraint. Wilson (1996)'s sampling algorithm has a running time of O(H) where H is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm has a running time of O(N^3), which is often greater than the mean hitting time of a directed graph. Additionally, we build upon Colbourn's algorithm and present a novel extension that can sample K trees without replacement in O(K N^3 + K^2 N) time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph.","tags":null,"title":"Efficient Sampling of Dependency Structure","type":"emnlp21"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"f702d57dd5fd3519dcb9aeca11d1b269","permalink":"https://rycolab.io/publication/zmigrodal-emnlp-21/","publishdate":"2023-07-09T14:57:00.377799Z","relpermalink":"/publication/zmigrodal-emnlp-21/","section":"publication","summary":"Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no sampling algorithm has been presented in the literature to account for this additional constraint. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a graph subject to the root constraint. Wilson (1996)'s sampling algorithm has a running time of O(H) where H is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm has a running time of O(N^3), which is often greater than the mean hitting time of a directed graph. Additionally, we build upon Colbourn's algorithm and present a novel extension that can sample K trees without replacement in O(K N^3 + K^2 N) time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph.","tags":null,"title":"Efficient Sampling of Dependency Structure","type":"publication"},{"authors":["Tiago Pimentel","Clara Meister","Simone Teufel","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"9afa98fb3de5a92d0ac02c3d3aaa4621","permalink":"https://rycolab.io/emnlp21/pimentelal-emnlp-21-b/","publishdate":"2021-10-08T07:56:32.997021Z","relpermalink":"/emnlp21/pimentelal-emnlp-21-b/","section":"emnlp21","summary":"Homophony's widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time; e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen (2020), who posit that good wordforms are more often homophonous simply because they are more phonotactically probable. In this paper, we join in on the debate. We first propose a new information-theoretic quantification of a language's homophony: the sample Rényi entropy. Then, we use this quantification to revisit Trott and Bergen's claims. While their point is theoretically sound, a specific methodological issue in their experiments raises doubts about their results. After addressing this issue, we find no clear pressure either towards or against homophony -- a much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's findings.","tags":null,"title":"On Homophony and Rényi Entropy","type":"emnlp21"},{"authors":["Tiago Pimentel","Clara Meister","Simone Teufel","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fe0d7f9fdc7d8fc35a396de8a6fc232c","permalink":"https://rycolab.io/publication/pimentelal-emnlp-21-b/","publishdate":"2023-07-09T14:56:58.409446Z","relpermalink":"/publication/pimentelal-emnlp-21-b/","section":"publication","summary":"Homophony's widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time; e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen (2020), who posit that good wordforms are more often homophonous simply because they are more phonotactically probable. In this paper, we join in on the debate. We first propose a new information-theoretic quantification of a language's homophony: the sample Rényi entropy. Then, we use this quantification to revisit Trott and Bergen's claims. While their point is theoretically sound, a specific methodological issue in their experiments raises doubts about their results. After addressing this issue, we find no clear pressure either towards or against homophony -- a much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's findings.","tags":null,"title":"On Homophony and Rényi Entropy","type":"publication"},{"authors":["Clara Meister","Tiago Pimentel","Patrick Haller","Lena Jäger","Ryan Cotterell","Roger Levy"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"dd1ebbc4204a38377dbb1ac4707f3eae","permalink":"https://rycolab.io/emnlp21/meisteral-emnlp-21-a/","publishdate":"2021-10-08T07:56:33.149254Z","relpermalink":"/emnlp21/meisteral-emnlp-21-a/","section":"emnlp21","summary":"The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal -- or lack thereof -- should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID's predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document -- a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.","tags":null,"title":"Revisiting the Uniform Information Density Hypothesis","type":"emnlp21"},{"authors":["Clara Meister","Tiago Pimentel","Patrick Haller","Lena Jäger","Ryan Cotterell","Roger Levy"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"cbdbaa011ed25665186348089c920601","permalink":"https://rycolab.io/publication/meisteral-emnlp-21-a/","publishdate":"2023-07-09T14:56:58.820415Z","relpermalink":"/publication/meisteral-emnlp-21-a/","section":"publication","summary":"The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal -- or lack thereof -- should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID's predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document -- a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.","tags":null,"title":"Revisiting the Uniform Information Density Hypothesis","type":"publication"},{"authors":["Tim Vieira","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"14986e40efe72717e8149162d1beece2","permalink":"https://rycolab.io/emnlp21/vieiraal-emnlp-21/","publishdate":"2021-10-08T07:56:33.962958Z","relpermalink":"/emnlp21/vieiraal-emnlp-21/","section":"emnlp21","summary":"Computational models of human language often involve combinatorial problems. For instance, a probabilistic parser may marginalize over exponentially many trees to make predictions.  Algorithms for such problems often employ dynamic programming and are not always unique. Finding one with optimal asymptotic runtime can be unintuitive, time consuming, and error-prone. Our work aims to automate this laborious process.  Given an initial correct declarative program, we search for a sequence of semantics-preserving transformations to improve its running time as much as possible. To this end, we describe a set of program transformations, a simple metric for assessing the efficiency of a transformed program, and a heuristic search procedure to improve this metric. We show that in practice, automated search---like the mental search performed by human programmers---can find substantial improvements to the initial program. Empirically, we show that many speed-ups described in the NLP literature could have been discovered automatically by our system.","tags":null,"title":"Searching for More Efficient Dynamic Programs","type":"emnlp21"},{"authors":["Tim Vieira","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"5d9d6b71ef6bfbb3800200b582025f9b","permalink":"https://rycolab.io/publication/vieiraal-emnlp-21/","publishdate":"2023-07-09T14:57:02.87845Z","relpermalink":"/publication/vieiraal-emnlp-21/","section":"publication","summary":"Computational models of human language often involve combinatorial problems. For instance, a probabilistic parser may marginalize over exponentially many trees to make predictions.  Algorithms for such problems often employ dynamic programming and are not always unique. Finding one with optimal asymptotic runtime can be unintuitive, time consuming, and error-prone. Our work aims to automate this laborious process.  Given an initial correct declarative program, we search for a sequence of semantics-preserving transformations to improve its running time as much as possible. To this end, we describe a set of program transformations, a simple metric for assessing the efficiency of a transformed program, and a heuristic search procedure to improve this metric. We show that in practice, automated search---like the mental search performed by human programmers---can find substantial improvements to the initial program. Empirically, we show that many speed-ups described in the NLP literature could have been discovered automatically by our system.","tags":null,"title":"Searching for More Efficient Dynamic Programs","type":"publication"},{"authors":["Jason Wei","Clara Meister","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"292e718bc257aa4df3d0f87d572098e5","permalink":"https://rycolab.io/publication/weial-acl-21/","publishdate":"2023-07-09T14:57:01.92974Z","relpermalink":"/publication/weial-acl-21/","section":"publication","summary":"The uniform information density (UID) hypothesis, which posits that speakers prefer utterances that distribute information uniformly across the signal, has gained substantial traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. Could we operationalize uniform information density as an inductive bias for statistical language modeling? In this paper, we augment the canonical MLE objective for training language models by encoding UID as regularization. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via analysis of generated sequences, we find that UID-regularized language models are higher-entropy and produce text that is longer and more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.","tags":null,"title":"A cognitive regularizer for language modeling","type":"publication"},{"authors":["Clara Meister","Martina Forster","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"db5df325a5307e8a413dd6099f75a0ff","permalink":"https://rycolab.io/publication/meisteral-acl-21-a/","publishdate":"2023-07-09T14:56:59.79831Z","relpermalink":"/publication/meisteral-acl-21-a/","section":"publication","summary":"Beam search is a go-to strategy for decoding neural sequence models. The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. To address this issue, we propose a reformulation of beam search, which we call determinantal beam search. Determinantal beam search has a natural relationship to determinantal point processes (DPPs), models over sets that inherently encode intra-set interactions. By posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process. In a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model. We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity.","tags":null,"title":"Determinantal Beam Search","type":"publication"},{"authors":["Jennifer C. White","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"63ccda2611a09489178b0aead7d48be0","permalink":"https://rycolab.io/publication/whitecotterell-acl-21/","publishdate":"2023-07-09T14:56:56.669069Z","relpermalink":"/publication/whitecotterell-acl-21/","section":"publication","summary":"Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. We propose a novel method for investigating the inductive biases of language models using artificial languages. These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order. We then use them to train and test language models. This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models. Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others. Further, we find that neither the inductive bias of the LSTM nor that of the transformer appear to reflect any tendencies that we see in attested natural languages","tags":null,"title":"Examining the Inductive Bias of Neural Language Models with Artificial Languages","type":"publication"},{"authors":["Higher-order Derivatives of Weighted Finite-state Machines"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"e6b72c9eb5a3b6ca73d12736d3e80524","permalink":"https://rycolab.io/publication/zmigrodal-acl-21-b/","publishdate":"2023-07-09T14:57:00.700824Z","relpermalink":"/publication/zmigrodal-acl-21-b/","section":"publication","summary":"Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time—from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(Aˆ2 Nˆ4) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.","tags":null,"title":"Higher-order Derivatives of Weighted Finite-state Machines","type":"publication"},{"authors":["Clara Meister","Stefan Lazov","Isabelle Augenstein","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"4cd4d9cdf21d7aa06d76609bef78973d","permalink":"https://rycolab.io/publication/meisteral-acl-21-b/","publishdate":"2023-07-09T14:57:02.30337Z","relpermalink":"/publication/meisteral-acl-21-b/","section":"publication","summary":"Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists -- under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior.","tags":null,"title":"Is Sparse Attention more Interpretable?","type":"publication"},{"authors":["Clara Meister","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"2c6d336cdab04b35f064739fc275d5f5","permalink":"https://rycolab.io/publication/meisteral-acl-21-c/","publishdate":"2023-07-09T14:57:02.592811Z","relpermalink":"/publication/meisteral-acl-21-c/","section":"publication","summary":"We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework--paired with significance tests--for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type--token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.","tags":null,"title":"Language Model Evaluation Beyond Perplexity","type":"publication"},{"authors":["Irene Nikkarinen$^*$","Tiago Pimentel$^*$","Damián Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"f5be89770e418009c5b19229481c04f1","permalink":"https://rycolab.io/publication/nikkarinenal-acl-findings-21/","publishdate":"2023-07-09T14:56:55.445761Z","relpermalink":"/publication/nikkarinenal-acl-findings-21/","section":"publication","summary":"The unigram distribution is the non-contextual probability of finding a specific word form in a corpus. While of central importance to the study of language, it is commonly approximated by each word's sample frequency in the corpus. This approach, being highly dependent on sample size, assigns zero probability to any out-of-vocabulary (oov) word form. As a result, it produces negatively biased probabilities for any oov word form, while positively biased probabilities to in-corpus words. In this work, we argue in favor of properly modeling the unigram distribution -- claiming it should be a central task in natural language processing. With this in mind, we present a novel model for estimating it in a language (a neuralization of Goldwater et al.'s (2011) model) and show it produces much better estimates across a diverse set of 7 languages than the naïve use of neural character-level language models.","tags":null,"title":"Modeling the Unigram Distribution","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"268774ed56f1d854d9db0799af7fb100","permalink":"https://rycolab.io/publication/zmigrodal-acl-21-a/","publishdate":"2023-07-09T14:56:59.129265Z","relpermalink":"/publication/zmigrodal-acl-21-a/","section":"publication","summary":"The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community. However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root. While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees. This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees. Indeed, we show that the rate of root constraint violations increases by an average of 13 times when decoding with K=50 as opposed to K=1. In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al. (1980). Our simplification allows us to obtain a constant time speed-up over the original algorithm. Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint.","tags":null,"title":"On Finding the $K$-best Non-projective Dependency Trees","type":"publication"},{"authors":["Tiago Pimentel","Maria Ryskina","Sabrina J. Mielke","Shijie Wu","Eleanor Chodroff","Brian Leonard","Garrett Nicolai","Yustinus Ghanggo Ate","Salam Khalifa","Nizar Habash","Charbel El-Khaissi","Omer Goldman","Michael Gasser","William Lane","Matt Coler","Arturo Oncevay","Jaime Rafael Montoya Samame","Gema Celeste Silva Villegas","Adam Ek","Jean-Philippe Bernardy","Andrey Shcherbakov","Aziyana Bayyr-ool","Karina Sheifer","Sofya Ganieva","Matvey Plugaryov","Elena Klyachko","Ali Salehi","Andrew Krizhanovsky","Natalia Krizhanovsky","Clara Vania","Sardana Ivanova","Aelita Salchak","Christopher Straughn","Zoey Liu","Jonathan North Washington","Duygu Ataman","Witold Kieraś","Marcin Woliński","Totok Suhardijanto","Niklas Stoehr","Zahroh Nuriah","Shyam Ratan","Francis M. Tyers","Edoardo M. Ponti","Grant Aiton","Richard J. Hatcher","Emily Prud'hommeaux","Ritesh Kumar","Mans Hulden","Botond Barta","Dorina Lakatos","Gábor Szolnok","Judit Ács","Mohit Raj","David Yarowsky","Ryan Cotterell","Ben Ambridge","Ekaterina Vylomova"],"categories":[],"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688914360,"objectID":"5db2792cec4ac80e6156683e88b492bf","permalink":"https://rycolab.io/publication/pimentelal-sigmorphon-21/","publishdate":"2023-07-09T14:57:04.493151Z","relpermalink":"/publication/pimentelal-sigmorphon-21/","section":"publication","summary":"This year's iteration of the SIGMORPHON Shared Task on morphological reinflection focuses on typological diversity and cross-lingual variation of morphosyntactic features. In terms of the task, we enrich UniMorph with new data for 32 languages from 13 language families, with most of them being under-resourced: Kunwinjku, Classical Syriac, Arabic (Modern Standard, Egyptian, Gulf), Hebrew, Amharic, Aymara, Magahi, Braj, Kurdish (Central, Northern, Southern), Polish, Karelian, Livvi, Ludic, Veps, Võro, Evenki, Xibe, Tuvan, Sakha, Turkish, Indonesian, Kodi, Seneca, Asháninka, Yanesha, Chukchi, Itelmen, Eibela. We evaluate six systems on the new data and conduct an extensive error analysis of the systems' predictions. Transformer-based models generally demonstrate superior performance on the majority of languages, achieving 90% accuracy on 65% of them. The languages on which systems yielded low accuracy are mainly under-resourced, with a limited amount of data. Most errors made by the systems are due to allomorphy, honorificity, and form variation. In addition, we observe that systems especially struggle to inflect multiword lemmas. The systems also produce misspelled forms or end up in repetitive loops (e.g., RNN-based models). Finally, we report a large drop in systems' performance on previously unseen lemmas.","tags":[],"title":"SIGMORPHON 2021 Shared Task on Morphological Reinflection: Generalization Across Languages","type":"publication"},{"authors":["Jennifer C. White","Tiago Pimentel","Naomi Saphra","Ryan Cotterell"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688914359,"objectID":"a5b01a48df6028c84e0fb54bdab8a890","permalink":"https://rycolab.io/publication/whiteal-naacl-2021/","publishdate":"2023-07-09T14:57:03.182022Z","relpermalink":"/publication/whiteal-naacl-2021/","section":"publication","summary":"Probes are models devised to investigate the encoding of knowledge—e.g. syntactic structure—in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages—implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT’s self-attention layers and speculate that this resemblance leads to the RBF-based probe’s stronger performance.","tags":[],"title":"A Non-Linear Structural Probe","type":"publication"},{"authors":["Jennifer C. White","Tiago Pimentel","Naomi Saphra","Ryan Cotterell"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"81064504d2ba8dca773aebd9360dd37e","permalink":"https://rycolab.io/publication/whiteal-naacl-21/","publishdate":"2021-10-05T16:25:27.329214Z","relpermalink":"/publication/whiteal-naacl-21/","section":"publication","summary":"Probes are models devised to investigate the encoding of knowledge—e.g. syntactic structure—in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages—implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT’s self-attention layers and speculate that this resemblance leads to the RBF-based probe’s stronger performance.","tags":null,"title":"A Non-Linear Structural Probe","type":"publication"},{"authors":["Rowan Hall Mauslay","Ryan Cotterell"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"15d2c4f4ebbb7bfb3c485294d0006231","permalink":"https://rycolab.io/publication/hall-mauslayal-naacl-21/","publishdate":"2023-07-09T14:56:56.362057Z","relpermalink":"/publication/hall-mauslayal-naacl-21/","section":"publication","summary":"Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model's output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model's linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax?","tags":null,"title":"Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing","type":"publication"},{"authors":["Tiago Pimentel","Brian Roark","Søren Wichmann","Ryan Cotterell","Damián Blasi"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"f4dd65625d1c8a5c507a4a6aee443f2b","permalink":"https://rycolab.io/publication/pimentelal-naacl-21-a/","publishdate":"2023-07-09T14:56:58.124103Z","relpermalink":"/publication/pimentelal-naacl-21-a/","section":"publication","summary":"This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for \\\"tongue\\\" is more likely than chance to contain the phone [l]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned cross-lingual lexicon, we extend methods previously used to detect within language non-arbitrariness (Pimentel et al., 2019) to measure cross-linguistic associations. We find that there is a significant effect of non-arbitrariness, but it is unsurprisingly small (less than 0.5% on average according to our information-theoretic estimate). We also provide a concept-level analysis which shows that a quarter of the concepts considered in our work exhibit a significant level of cross-linguistic non-arbitrariness. In sum, the paper provides new methods to detect cross-linguistic associations at scale.","tags":null,"title":"Finding Concept-specific Biases in Form--Meaning Associations","type":"publication"},{"authors":["Tiago Pimentel$^*$","Irene Nikkarinen$^*$","Kyle Mahowald","Ryan Cotterell","Damián Blasi"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"678503316636b9c24f84634d0d977867","permalink":"https://rycolab.io/publication/pimentelal-naacl-21-b/","publishdate":"2023-07-09T14:57:03.471939Z","relpermalink":"/publication/pimentelal-naacl-21-b/","section":"publication","summary":"The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf's law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world's languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon's optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes -- as measured by code length.","tags":null,"title":"How (Non-)Optimal is the Lexicon?","type":"publication"},{"authors":["Elizabeth Salesky","Badr M. Abdullah","Sabrina Mielke","Elena Klyachko","Oleg Serikov","Edoardo Maria Ponti","Ritesh Kumar","Ryan Cotterell","Ekaterina Vylomova"],"categories":[],"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1688914360,"objectID":"edad362c490e762059d5eefaa86148c3","permalink":"https://rycolab.io/publication/saleskyal-sigtyp-21/","publishdate":"2023-07-09T14:57:04.161102Z","relpermalink":"/publication/saleskyal-sigtyp-21/","section":"publication","summary":"While language identification is a fundamental speech and language processing task, for many languages and language families it remains a challenging task. For many low-resource and endangered languages this is in part due to resource availability: where larger datasets exist, they may be single-speaker or have different domains than desired application scenarios, demanding a need for domain and speaker-invariant language identification systems. This year’s shared task on robust spoken language identification sought to investigate just this scenario: systems were to be trained on largely single-speaker speech from one domain, but evaluated on data in other domains recorded from speakers under different recording circumstances, mimicking realistic low-resource scenarios. We see that domain and speaker mismatch proves very challenging for current methods which can perform above 95% accuracy in-domain, which domain adaptation can address to some degree, but that these conditions merit further investigation to make spoken language identification accessible in many scenarios.","tags":[],"title":"SIGTYP 2021 Shared Task: Robust Spoken Language Identification","type":"publication"},{"authors":["Josef Valvoda","Tiago Pimentel","Niklas Stoehr","Ryan Cotterell","Simone Teufel"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"8d79bfd3c3c02b61d3ff68ec34c707e9","permalink":"https://rycolab.io/publication/valvodaal-naacl-21/","publishdate":"2023-07-09T14:57:01.652045Z","relpermalink":"/publication/valvodaal-naacl-21/","section":"publication","summary":"In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury's, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart's, who believes that what matters most is the precedent's facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges' arguments (i.e. the precedent cases).  Taking an information-theoretic view, and modelling the question as a case outcome classification task, we find that the precedent's arguments share 0.38 nats of information with the case's outcome, whereas precedent's facts only share 0.18 nats of information (i.e., 58% less); suggesting Halsbury's view may be more accurate in this specific court.  We found however in a qualitative analysis that there are specific statues where Goodhart's view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward.","tags":null,"title":"What About the Precedent: An Information-Theoretic Analysis of Common Law","type":"publication"},{"authors":["Dan Roth"],"categories":null,"content":" Bio Dan Roth is the Eduardo D. Glandt Distinguished Professor at the Department of Computer and Information Science, University of Pennsylvania, and a Fellow of the AAAS, the ACM, AAAI, and the ACL. In 2017 Roth was awarded the John McCarthy Award, the highest award the AI community gives to mid-career AI researchers. Roth was recognized “for major conceptual and theoretical advances in the modeling of natural language understanding, machine learning, and reasoning.” Roth has published broadly in machine learning, natural language processing, knowledge representation and reasoning, and learning theory, and has developed advanced machine learning based tools for natural language applications that are being used widely. Until February 2017 Roth was the Editor-in-Chief of the Journal of Artificial Intelligence Research (JAIR). Roth has been involved in several startups; most recently he was a co-founder and chief scientist of NexLP, a startup that leverages the latest advances in Natural Language Processing (NLP), Cognitive Analytics, and Machine Learning in the legal and compliance domains. NexLP was sold to Reveal in 2020. Prof. Roth received his B.A Summa cum laude in Mathematics from the Technion, Israel, and his Ph.D. in Computer Science from Harvard University in 1995.\n","date":1619446500,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619446500,"objectID":"25f162eb26e2aecdacffd33e784f2c01","permalink":"https://rycolab.io/talk/roth-apr-26-21/","publishdate":"2022-11-21T19:00:00+02:00","relpermalink":"/talk/roth-apr-26-21/","section":"talk","summary":"This talk delves into some of the challenges underlying reasoning – making natural language understanding decisions that depend on multiple, interdependent, models, and exemplify it using the domain of Reasoning about Time, as it is expressed in natural language.","tags":[],"title":"It’s Time for Reasoning","type":"talk"},{"authors":["Shijie Wu","Mans Hulden","Ryan Cotterell"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"a69473188b6d75bd8a5e12db43511a77","permalink":"https://rycolab.io/publication/wual-eacl-21/","publishdate":"2023-07-09T14:57:00.979842Z","relpermalink":"/publication/wual-eacl-21/","section":"publication","summary":"The transformer has been shown to outperform recurrent neural network-based sequence-to-sequence models in various word-level NLP tasks. The model offers other benefits as well: It trains faster and has fewer parameters. Yet for character-level transduction tasks, eg morphological inflection generation and historical text normalization, few shows success on outperforming recurrent models with the transformer. In an empirical study, we uncover that, in contrast to recurrent sequence-to-sequence models, the batch size plays a crucial role in the performance of the transformer on character-level tasks, and we show that with a large enough batch size, the transformer does indeed outperform recurrent models. We also introduce a simple technique to handle feature-guided character-level transduction that further improves performance. With these insights, we achieve state-of-the-art performance on morphological inflection and historical text normalization. We also show that the transformer outperforms a strong baseline on two other character-level transduction tasks: grapheme-to-phoneme conversion and transliteration.","tags":null,"title":"Applying the Transformer to Character-level Transduction","type":"publication"},{"authors":["Tiago Pimentel","Ryan Cotterell","Brian Roark"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"acb5f67a068d4761ecd5c5bf41860404","permalink":"https://rycolab.io/publication/pimentelal-eacl-21/","publishdate":"2023-07-09T14:57:01.332537Z","relpermalink":"/publication/pimentelal-eacl-21/","section":"publication","summary":"Psycholinguistic studies of human word processing and lexical access provide ample evidence of the preferred nature of word-initial versus word-final segments, e.g., in terms of attention paid by listeners (greater) or the likelihood of reduction by speakers (lower). This has led to the conjecture -- as in Wedel et al. (2019b), but common elsewhere -- that languages have evolved to provide more information earlier in words than later. Information-theoretic methods to establish such tendencies in lexicons have suffered from several methodological shortcomings that leave open the question of whether this high word-initial informativeness is actually a property of the lexicon or simply an artefact of the incremental nature of recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words.","tags":null,"title":"Disambiguatory signals are stronger in word initial positions","type":"publication"},{"authors":["Martina Forster","Clara Meister","Ryan Cotterell"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"ec9df7e414d8bdebbf0c0fa12fec564a","permalink":"https://rycolab.io/publication/forsteral-eacl-21/","publishdate":"2023-07-09T14:56:55.980958Z","relpermalink":"/publication/forsteral-eacl-21/","section":"publication","summary":"Neural sequence-to-sequence models are currently the predominant choice for language generation tasks. Yet, on word-level tasks, exact inference of these models reveals the empty string is often the global optimum. Prior works have speculated this phenomenon is a result of the inadequacy of neural models for language generation. However, in the case of morphological inflection, we find that the empty string is almost never the most probable solution under the model. Further, greedy search often finds the global optimum. These observations suggest that the poor calibration of many neural models may stem from characteristics of a specific subset of tasks rather than general ill-suitedness of such models for language generation.","tags":null,"title":"Searching for Search Errors in Neural Morphological Inflection","type":"publication"},{"authors":["Jiaoda Li","Ryan Cotterell","Mrinmaya Sachan"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"d2bf60ebd17f59db73387e848801b747","permalink":"https://rycolab.io/publication/lial-tacl-21/","publishdate":"2023-07-09T14:56:57.818902Z","relpermalink":"/publication/lial-tacl-21/","section":"publication","summary":"Multi-head attention, a collection of several attention mechanisms that independently attend to different parts of the input, is the key ingredient in the Transformer. Recent work has shown, however, that a large proportion of the heads in a Transformer’s multi-head attention mechanism can be safely pruned away without significantly harming the performance of the model; such pruning leads to models that are noticeably smaller and faster in practice. Our work introduces a new head pruning technique that we term differentiable subset pruning. ntuitively, our method learns per- head importance variables and then enforces a user-specified hard constraint on the number of unpruned heads. he importance variables are learned via stochastic gradient descent. e conduct experiments on natural language inference and machine translation; we show that differentiable subset pruning performs comparably or better than previous works while offering precise control of the sparsity level.","tags":null,"title":"Differentiable Subset Pruning of Transformer Heads","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"bd618c3c802e3eefc5f846e2e6fa5fa3","permalink":"https://rycolab.io/publication/zmigrodal-tacl-21/","publishdate":"2023-07-09T14:57:00.083634Z","relpermalink":"/publication/zmigrodal-tacl-21/","section":"publication","summary":"We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement with or without automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the expected attachment score, entropy, and generalized expectation criteria. As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence. In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 15 and 9 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively.","tags":null,"title":"Efficient Computation of Expectations under Spanning Tree Distributions","type":"publication"},{"authors":["Emanuele Bugliarello","Ryan Cotterell","Naoaki Okazaki","Desmond Elliott"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"52c57b1e23ec68c83e2b0a32be10dde4","permalink":"https://rycolab.io/publication/bugliarelloal-tacl-21/","publishdate":"2023-07-09T14:56:55.144251Z","relpermalink":"/publication/bugliarelloal-tacl-21/","section":"publication","summary":"Large-scale pretraining and task-specific fine-tuning is now the standard methodology for many tasks in computer vision and natural language processing. Recently, a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI. These models can be categorized into either single-stream or dual-stream encoders. We study the differences between these two categories, and show how they can be unified under a single theoretical framework. We then conduct controlled experiments to discern the empirical differences between five vision and language BERTs. Our experiments show that training data and hyperparameters are responsible for most of the differences between the reported results, but they also reveal that the embedding layer plays a crucial role in these massive models.","tags":null,"title":"Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs","type":"publication"},{"authors":["Adina Williams","Ryan Cotterell","Lawrence Wolf-Sonkin","Damián Blasi","Hanna Wallach"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"647f8242e2378189a60913210c69e277","permalink":"https://rycolab.io/publication/williamsal-tacl-21/","publishdate":"2023-07-09T14:57:03.831723Z","relpermalink":"/publication/williamsal-tacl-21/","section":"publication","summary":"We use large-scale corpora in six different gendered languages, along with tools from NLP and information theory, to test whether there is a relationship between the grammatical genders of inanimate nouns and the adjectives used to describe those nouns. For all six languages, we find that there is a statistically significant relationship. We also find that there are statistically significant relationships between the grammatical genders of inanimate nouns and the verbs that take those nouns as direct objects, as indirect objects, and as subjects. We defer a deeper investigation of these relationships for future work.","tags":null,"title":"On the Relationships Between the Grammatical Genders of Inanimate Nouns and Their Co-Occurring Adjectives and Verbs","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Ryan Cotterell","Marinela Parović","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"3b871b2a579c4fffa97444fa9ce329f7","permalink":"https://rycolab.io/publication/pontial-tacl-21/","publishdate":"2023-07-09T14:56:54.176339Z","relpermalink":"/publication/pontial-tacl-21/","section":"publication","summary":"Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task-language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task-language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods. Our code is available at https://github.com/cambridgeltl/parameter-factorization.","tags":null,"title":"Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages","type":"publication"},{"authors":["Paula Czarnowska","Sebastian Ruder","Ryan Cotterell","Ann Copestake"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"a76d128f3056daaec67202402873bea1","permalink":"https://rycolab.io/publication/czarnowskaal-coling-20/","publishdate":"2021-10-08T07:56:43.006344Z","relpermalink":"/publication/czarnowskaal-coling-20/","section":"publication","summary":"We propose a novel morphologically aware probability model for bilingual lexicon induction, which jointly models lexeme translation and inflectional morphology in a structured way. Our model exploits the basic linguistic intuition that the lexeme is the key lexical unit of meaning, while inflectional morphology provides additional syntactic information. This approach leads to substantial performance improvements—19% average improvement in accuracy across 6 language pairs over the state of the art in the supervised setting and 16% in the weakly supervised setting. As another contribution, we highlight issues associated with modern BLI that stem from ignoring inflectional morphology, and propose three suggestions for improving the task.","tags":null,"title":"Morphologically Aware Word-Level Translation","type":"publication"},{"authors":["Francisco Vargas","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"7d4a36bf3e5fcd094021aaacb9bdb562","permalink":"https://rycolab.io/publication/vargascotterell-emnlp-20/","publishdate":"2021-10-08T07:56:40.688452Z","relpermalink":"/publication/vargascotterell-emnlp-20/","section":"publication","summary":"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).","tags":null,"title":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation","type":"publication"},{"authors":["Clara Meister","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"00b67b122cbde7f1fd036ec9b7fcfec9","permalink":"https://rycolab.io/publication/meisteral-emnlp-20/","publishdate":"2021-10-08T07:56:41.87351Z","relpermalink":"/publication/meisteral-emnlp-20/","section":"publication","summary":"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.","tags":null,"title":"If Beam Search is the Answer, What was the Question?","type":"publication"},{"authors":["Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"e0cf51125dafd47351851f86c166e2a8","permalink":"https://rycolab.io/publication/hennigenal-emnlp-20/","publishdate":"2021-08-20T18:07:37.764937Z","relpermalink":"/publication/hennigenal-emnlp-20/","section":"publication","summary":"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.","tags":null,"title":"Intrinsic Probing through Dimension Selection","type":"publication"},{"authors":["Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"06d17cff0b9422a1d5e37b5bb285eae6","permalink":"https://rycolab.io/publication/torroba-hennigenal-emnlp-20/","publishdate":"2021-10-08T07:56:41.418752Z","relpermalink":"/publication/torroba-hennigenal-emnlp-20/","section":"publication","summary":"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.","tags":null,"title":"Intrinsic Probing through Dimension Selection","type":"publication"},{"authors":["Jun Yen Leung","Guy Emerson","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"e215838e46b34bea56143b0e9d9d9ff7","permalink":"https://rycolab.io/publication/leungal-emnlp-20/","publishdate":"2021-10-08T07:56:43.154121Z","relpermalink":"/publication/leungal-emnlp-20/","section":"publication","summary":"Across languages, multiple consecutive adjectives modifying a noun (e.g. ``the big red dog″) follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data. We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different. We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.","tags":null,"title":"Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model","type":"publication"},{"authors":["Arya D. McCarthy","Adina Williams","Shijia Liu","David Yarowsky","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"7ca684e99b099ca2409ebd8614e13a38","permalink":"https://rycolab.io/publication/mccarthyal-emnlp-20/","publishdate":"2021-10-08T07:56:43.342264Z","relpermalink":"/publication/mccarthyal-emnlp-20/","section":"publication","summary":"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages’ gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information-theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. Finally, we ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.","tags":null,"title":"Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions","type":"publication"},{"authors":["Tiago Pimentel","Naomi Saphra","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"ccd37a475ea91c7644701517d10adf20","permalink":"https://rycolab.io/publication/pimentelal-emnlp-20/","publishdate":"2021-10-08T07:56:41.577101Z","relpermalink":"/publication/pimentelal-emnlp-20/","section":"publication","summary":"The question of how to probe contextual word representations for linguistic structure in a way that is both principled and useful has seen significant attention recently in the NLP literature. In our contribution to this discussion, we argue for a probe metric that reflects the fundamental trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments using Pareto hypervolume as an evaluation metric show that probes often do not conform to our expectations---e.g., why should the non-contextual fastText representations encode more morpho-syntactic information than the contextual BERT representations? These results suggest that common, simplistic probing tasks, such as part-of-speech labeling and dependency arc labeling, are inadequate to evaluate the linguistic structure encoded in contextual word representations. This leads us to propose full dependency parsing as a probing task. In support of our suggestion that harder probing tasks are necessary, our experiments with dependency parsing reveal a wide gap in syntactic knowledge between contextual and non-contextual representations.","tags":null,"title":"Pareto Probing: Trading Off Accuracy for Simplicity","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"ef7a081c8b13acb03471ad23994af5e7","permalink":"https://rycolab.io/publication/zmigrodal-emnlp-20/","publishdate":"2021-10-08T07:56:41.728423Z","relpermalink":"/publication/zmigrodal-emnlp-20/","section":"publication","summary":"The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree. We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.In fact, the worst constraint-violation rate we observe is 24%. Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime. We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime.","tags":null,"title":"Please Mind the Root: Decoding Arborescences for Dependency Parsing","type":"publication"},{"authors":["Johannes Bjerva","Elizabeth Salesky","Sabrina J. Mielke","Aditi Chaudhary","Giuseppe G. A. Celano","Edoardo M. Ponti","Ekaterina Vylomova","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"adc24c0cb72c2c158a274d0ca277e2a9","permalink":"https://rycolab.io/publication/bjervaal-sigtyp-20/","publishdate":"2021-10-08T07:56:42.192518Z","relpermalink":"/publication/bjervaal-sigtyp-20/","section":"publication","summary":"Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some features, and skewed, in that few features have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the task attracted 8 submissions from 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.","tags":null,"title":"SIGTYP 2020 Shared Task: Prediction of Typological Features","type":"publication"},{"authors":["Tiago Pimentel","Rowan Hall Maudslay","Damián Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"1dea2434a38fd242a587b610d625d717","permalink":"https://rycolab.io/publication/pimentel-2-al-emnlp-20/","publishdate":"2021-10-08T07:56:42.36775Z","relpermalink":"/publication/pimentel-2-al-emnlp-20/","section":"publication","summary":"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear---resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this---one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. ρ=0.40 in English). We then test our main hypothesis---that a word's lexical ambiguity should negatively correlate with its contextual uncertainty---and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative.","tags":null,"title":"Speakers Fill Semantic Gaps with Context","type":"publication"},{"authors":["Elizabeth Salesky","Eleanor Chodroff","Tiago Pimentel","Matthew Wiesner","Ryan Cotterell","Alan W Black","Jason Eisner"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"630bf245688581fe745a2fc35fde7c29","permalink":"https://rycolab.io/publication/saleskyal-acl-20/","publishdate":"2021-10-08T07:56:42.707781Z","relpermalink":"/publication/saleskyal-acl-20/","section":"publication","summary":"A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.","tags":null,"title":"A Corpus for Large-Scale Phonetic Typology","type":"publication"},{"authors":["Rowan Hall Maudslay","Josef Valvoda","Tiago Pimentel","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"553515b076cfd79cf6ac3caab7b3fc42","permalink":"https://rycolab.io/publication/hall-maudslayal-acl-20/","publishdate":"2021-10-08T07:56:42.854322Z","relpermalink":"/publication/hall-maudslayal-acl-20/","section":"publication","summary":"Measuring what linguistic information is encoded in continuous representations of language has become a popular area of research. To do this, researchers train \"probes\"— supervised models designed to extract linguistic structure from embeddings. The line between what constitutes a probe and a model designed to achieve a particular task is often blurred. To fully understand what we are learning about the target language representation—or the instrument with which we performing measurement with for that matter—we would do well to compare probes to classic parsers. As a case study, we consider the structural probe (Hewitt and Manning, 2019), designed to quantify the presence of syntactic information. We create a simple parser that improves upon the performance of the structural probe by 11.4% on UUAS, despite having an identical lightweight parameterization. Under a second less common metric, however, the structural probe outperforms traditional parsers. This begs the question: why should some metrics be preferred for probing and others for parsing?","tags":null,"title":"A Tale of a Probe and a Parser","type":"publication"},{"authors":["Clara Meister","Elizabeth Salesky","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"65df8ea6f2757b720fbb48bafcdc38a4","permalink":"https://rycolab.io/publication/meisteral-acl-20/","publishdate":"2021-10-08T07:56:41.128518Z","relpermalink":"/publication/meisteral-acl-20/","section":"publication","summary":"Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. We also find that variance in model performance can be explained largely by the resulting entropy of the model. Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place. Our code is available online at https://github.com/rycolab/entropyRegularization.","tags":null,"title":"Generalized Entropy Regularization or: There's Nothing Special about Label Smoothing","type":"publication"},{"authors":["Tiago Pimentel","Josef Valvoda","Rowan Hall Maudslay","Ran Zmigrod","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"96c6d62c85e714712e9a3cd7dfcf94c8","permalink":"https://rycolab.io/publication/pimentelal-acl-20/","publishdate":"2021-10-08T07:56:41.274372Z","relpermalink":"/publication/pimentelal-acl-20/","section":"publication","summary":"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually know about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task.  A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic formalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inhering in the contextualized representation. The empirical portion of our paper focuses on obtaining tight estimates for how much information BERT knows about both parts of speech and dependency labels, evaluating it in a set of ten typologically diverse languages often under-represented in parsing research, plus English, totalling eleven languages.  We find BERT only accounts for more information about parts of speech than a traditional type-based word embedding in five of the eleven analysed languages. When we look at dependency labels, BERT does improve upon type-based embeddings in all analysed languages, but accounting for at most 12% more information.","tags":null,"title":"Information-Theoretic Probing for Linguistic Structure","type":"publication"},{"authors":["Emanuele Bugliarello","Sabrina J. Mielke","Antonios Anastasopoulos","Ryan Cotterell","Naoaki Okazaki"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"10a46a0454ae8759bd3c1cf7f3c4f89a","permalink":"https://rycolab.io/publication/bugliarelloal-acl-20/","publishdate":"2021-10-08T07:56:40.544221Z","relpermalink":"/publication/bugliarelloal-acl-20/","section":"publication","summary":"The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.","tags":null,"title":"It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information","type":"publication"},{"authors":["Rowan Hall Maudslay","Tiago Pimentel","Ryan Cotterell","Simone Teufel"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8364344a4166a666333052c13e0e17e6","permalink":"https://rycolab.io/publication/hall-maudslayal-wflp-20/","publishdate":"2021-10-08T07:56:43.731741Z","relpermalink":"/publication/hall-maudslayal-wflp-20/","section":"publication","summary":"We report the results of our system on the Metaphor Detection Shared Task at the Second Workshop on Figurative Language Processing 2020. Our model is an ensemble, utilising contextualised and static distributional semantic representations, along with word-type concreteness ratings. Using these features, it predicts word metaphoricity with a deep multi-layer perceptron. We are able to best the state-of-the-art from the 2018 Shared Task by an average of 8.0% F1, and finish fourth in both sub-tasks in which we participate.","tags":null,"title":"Metaphor Detection Using Context and Concreteness","type":"publication"},{"authors":["Adina Williams","Tiago Pimentel","Arya McCarthy","Hagen Blix","Eleanor Chodroff","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"628b1f074b060a453a0afee5af15adce","permalink":"https://rycolab.io/publication/williamsal-acl-20/","publishdate":"2021-10-08T07:56:40.978533Z","relpermalink":"/publication/williamsal-acl-20/","section":"publication","summary":"The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties. Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues. Here, we investigate the strength of those clues. More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns. We know that form and meaning are often also indicative of grammatical gender \u0026mdash; which, as we quantitatively verify, can itself share information with declension class \u0026mdash; so we also control for gender. We find for two Indo-European languages (Czech and German) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender). The three-way interaction between class, form, and meaning (given gender) is also significant. Our study is important for two reasons: First, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions. Secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages. The code is publicly available at https://github.com/rycolab/declension-mi.","tags":null,"title":"Predicting Declension Class from Form and Meaning","type":"publication"},{"authors":["Ekaterina Vylomova","Jennifer White","Elizabeth Salesky","Sabrina J. Mielke","Shijie Wu","Edoardo Maria Ponti","Rowan Hall Maudslay","Ran Zmigrod","Josef Valvoda","Svetlana Toldova","Francis Tyers","Elena Klyachko","Ilya Yegorov","Natalia Krizhanovsky","Paula Czarnowska","Irene Nikkarinen","Andrew Krizhanovsky","Tiago Pimentel","Lucas Torroba Hennigen","Christo Kirov","Garrett Nicolai","Adina Williams","Antonios Anastasopoulos","Hilaria Cruz","Eleanor Chodroff","Ryan Cotterell","Miikka Silfverberg","Mans Hulden"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"5ce06cc7342c98b30fcdfe8e326b1930","permalink":"https://rycolab.io/publication/vylomovaal-sigm-20/","publishdate":"2021-10-08T07:56:42.023336Z","relpermalink":"/publication/vylomovaal-sigm-20/","section":"publication","summary":"A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language. Most systems, however, are developed using data from just one language such as English. The SIGMORPHON 2020 shared task on morphological reinflection aims to investigate systems’ ability to generalize across typologically distinct languages, many of which are low resource. Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages. A total of 22 systems (19 neural) from 10 teams were submitted to the task. All four winning systems were neural (two monolingual transformers and two massively multilingual RNN-based models with gated attention). Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages. Non-neural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.","tags":null,"title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection","type":"publication"},{"authors":["Alexander Erdmann","Micha Elsner","Shijie Wu","Ryan Cotterell","Nizar Habash"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"05b7c75176d48aa04a24594196d68eb3","permalink":"https://rycolab.io/publication/erdmannal-acl-20/","publishdate":"2021-10-08T07:56:40.391816Z","relpermalink":"/publication/erdmannal-acl-20/","section":"publication","summary":"This work treats the paradigm discovery problem (PDP)—the task of learning an inflectional morphological system from unannotated sentences. We formalize the PDP and develop evaluation metrics for judging systems. Using currently available resources, we construct datasets for the task. We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages. Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm. Then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots. An error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work. Our code and data are available at https://github.com/alexerdmann/ParadigmDiscovery.","tags":null,"title":"The Paradigm Discovery Problem","type":"publication"},{"authors":["Clara Meister","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a583d975602b3880816be059c3edceb7","permalink":"https://rycolab.io/publication/meisteral-tacl-20/","publishdate":"2021-10-08T07:56:40.833956Z","relpermalink":"/publication/meisteral-tacl-20/","section":"publication","summary":"Decoding for many NLP tasks requires a heuristic algorithm for approximating exact search since the full search space is often intractable if not simply too large to traverse efficiently. The default algorithm for this job is beam search---a pruned version of breadth-first search---which in practice, returns better results than exact inference due to beneficial search bias. In this work, we show that standard beam search is a computationally inefficient choice for many decoding tasks; specifically, when the scoring function is a monotonic function in sequence length, other search algorithms can be used to reduce the number of calls to the scoring function (e.g., a neural network), which is often the bottleneck computation. We propose best-first beam search, an algorithm that provably returns the same set of results as standard beam search, albeit in the minimum number of scoring function calls to guarantee optimality (modulo beam size).  We show that best-first beam search can be used with length normalization and mutual information decoding, among other rescoring functions.  Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar search bias in terms of downstream performance, but runs in a fraction of the time.","tags":null,"title":"Best-First Beam Search","type":"publication"},{"authors":["Tiago Pimentel","Brian Roark","Ryan Cotterell"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fc0a72e5e26865b4a0609143cb55a02c","permalink":"https://rycolab.io/publication/pimentelal-tacl-20/","publishdate":"2021-10-08T07:56:42.528191Z","relpermalink":"/publication/pimentelal-tacl-20/","section":"publication","summary":"We present methods for calculating a measure of phonotactic complexity—bits per phoneme—that permits a straightforward cross-linguistic comparison. When given a word, represented as a sequence of phonemic segments such as symbols in the international phonetic alphabet, and a statistical model trained on a sample of word types from the language, we can approximately measure bits per phoneme using the negative log-probability of that word under the model. This simple measure allows us to compare the entropy across languages, giving insight into how complex a language’s phonotactics are. Using a collection of 1016 basic concept words across 106 languages, we demonstrate a very strong negative correlation of −0.74 between bits per phoneme and the average length of words.","tags":null,"title":"Phonotactic Complexity and its Trade-offs","type":"publication"},{"authors":["Martina Forster","Clara Meister"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"2aab8232131f5db34d63f575d7a3e8ce","permalink":"https://rycolab.io/publication/forstermeister-sigm-20/","publishdate":"2021-10-08T07:56:43.877166Z","relpermalink":"/publication/forstermeister-sigm-20/","section":"publication","summary":"This paper presents our system for the SIGMORPHON 2020 Shared Task. We build off of the baseline systems, performing exact inference on models trained on language family data. Our systems return the globally best solution under these models. Our two systems achieve 80.9% and 75.6% accuracy on the test set. We ultimately find that, in this setting, exact inference does not seem to help or hinder the performance of morphological inflection generators, which stands in contrast to its affect on Neural Machine Translation (NMT) models.","tags":null,"title":"SIGMORPHON 2020 Task 0 System Description: ETH Zürich Team","type":"publication"},{"authors":["Arya D. McCarthy","Christo Kirov","Matteo Grella","Amrit Nidhi","Patrick Xia","Kyle Gorman","Ekaterina Vylomova","Sabrina J. Mielke","Garrett Nicolai","Miikka Silfverberg","Timofey Arkhangelskiy","Nataly Krizhanovsky","Andrew Krizhanovsky","Elena Klyachko","Alexey Sorokin","John Mansfield","Valts Ernštreits","Yuval Pinter","Cassandra L. Jacobs","Ryan Cotterell","Mans Hulden","David Yarowsky"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"c645aa83fdb3089bec99c60b4d983b61","permalink":"https://rycolab.io/publication/mccarthyal-lrec-20/","publishdate":"2021-10-08T07:56:43.577111Z","relpermalink":"/publication/mccarthyal-lrec-20/","section":"publication","summary":"The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.","tags":null,"title":"UniMorph 3.0: Universal Morphology","type":"publication"},{"authors":["Paula Czarnowska","Sebastian Ruder","Edouard Grave","Ryan Cotterell","Ann Copestake"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"296b08393c2f8c8422cb74edca0e8730","permalink":"https://rycolab.io/publication/czarnowskaal-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:29.66145Z","relpermalink":"/publication/czarnowskaal-emnlp-ijcnlp-19/","section":"publication","summary":"Human translators routinely have to translate rare inflections of words--due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habláramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the best performing models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.","tags":null,"title":"Don’t Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction","type":"publication"},{"authors":["Pei Zhou","Weijia Shi","Jieyu Zhao","Kuan-Hao Huang","Muhao Chen","Ryan Cotterell","Kai-Wei Chang"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"25f9a7f3afd4937aa991707ae9d06c12","permalink":"https://rycolab.io/publication/zhoual-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:31.734774Z","relpermalink":"/publication/zhoual-emnlp-ijcnlp-19/","section":"publication","summary":"Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding of these languages under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.","tags":null,"title":"Examining Gender Bias in Languages with Grammatical Gender","type":"publication"},{"authors":["Rowan Hall Maudslay","Hila Gonen","Ryan Cotterell","Simone Teufel"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"74d11d07da4859c3b2be113fbd5adbda","permalink":"https://rycolab.io/publication/hall-maudslayal-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:29.815618Z","relpermalink":"/publication/hall-maudslayal-emnlp-ijcnlp-19/","section":"publication","summary":"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.","tags":null,"title":"It’s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution","type":"publication"},{"authors":["Adina Williams","Ryan Cotterell","Lawrence Wolf-Sonkin","Damian Blasi","Hanna Wallach"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"b7f65d5b01c4b86c378d10100edac08d","permalink":"https://rycolab.io/publication/williamsal-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:31.289191Z","relpermalink":"/publication/williamsal-emnlp-ijcnlp-19/","section":"publication","summary":"Many of the world's languages employ grammatical gender on the lexeme. For instance, in Spanish, house ''casa'' is feminine, whereas the word for paper ''papel'' is masculine. To a speaker of a genderless language, this categorization seems to exist with neither rhyme nor reason. But, is the association of nouns to gender classes truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.","tags":null,"title":"Quantifying the Semantic Core of Gender Systems","type":"publication"},{"authors":["Edoardo Maria Ponti","Ivan Vulić","Ryan Cotterell","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"bd8665dc8c25e8141fa3a624c5fd0955","permalink":"https://rycolab.io/publication/pontial-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:30.986851Z","relpermalink":"/publication/pontial-emnlp-ijcnlp-19/","section":"publication","summary":"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace's method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world's languages, we hope that these insights will broaden the scope of applications for language technology.","tags":null,"title":"Towards Zero-Shot Language Modeling","type":"publication"},{"authors":["Arya D. McCarthy","Ekaterina Vylomova","Shijie Wu","Chaitanya Malaviya","Lawrence Wolf-Sonkin","Garrett Nicolai","Christo Kirov","Miikka Silfverberg","Sabrina Mielke","Jeffrey Heinz","Ryan Cotterell","Mans Hulden"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"da6c1c8cf0e58894242c01b498a5fdd8","permalink":"https://rycolab.io/publication/mccarthyal-tacl-19/","publishdate":"2021-08-20T18:07:30.548684Z","relpermalink":"/publication/mccarthyal-tacl-19/","section":"publication","summary":"The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years' inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year's strong baselines or highly ranked systems from previous years' shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.","tags":null,"title":"The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection","type":"publication"},{"authors":["Ran Zmigrod","Sabrina Mielke","Hanna Wallach","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"02c577382eddda3a8c17afd5facc6144","permalink":"https://rycolab.io/publication/zmigrodal-acl-19/","publishdate":"2021-08-20T18:07:32.031802Z","relpermalink":"/publication/zmigrodal-acl-19/","section":"publication","summary":"Gender stereotypes are manifest in most of the world's languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.","tags":null,"title":"Counterfactual Data Augmentation for Mitigating Gender Bias in Languages with Rich Morphology","type":"publication"},{"authors":["Shijie Wu","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"7547a9341238e368f994d0782bb943a4","permalink":"https://rycolab.io/publication/wucotterell-acl-19/","publishdate":"2021-08-20T18:07:31.581363Z","relpermalink":"/publication/wucotterell-acl-19/","section":"publication","summary":"Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.","tags":null,"title":"Exact Hard Monotonic Attention for Character-Level Transduction","type":"publication"},{"authors":["Tiago Pimentel","Arya McCarthy","Damian Blasi","Brian Roark","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2aec702aa08c71f0b3cfeb2cc63e90ba","permalink":"https://rycolab.io/publication/pimentelal-acl-19/","publishdate":"2021-08-20T18:07:30.842823Z","relpermalink":"/publication/pimentelal-acl-19/","section":"publication","summary":"A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram `gl′ have any systematic relationship to the meaning of words like `glisten′, `gleam′ and `glow′? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small---despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.","tags":null,"title":"Meaning to Form: Measuring Systematicity as Information","type":"publication"},{"authors":["Shijie Wu","Ryan Cotterell","Timothy J. O'Donnell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"19ada103f8247e8e1a50fb490f672367","permalink":"https://rycolab.io/publication/wual-acl-19/","publishdate":"2021-08-20T18:07:31.437492Z","relpermalink":"/publication/wual-acl-19/","section":"publication","summary":"We present a study of morphological irregularity. Following recent work, we define an information-theoretic measure of irregularity based on the predictability of forms in a language. Using a neural transduction model, we estimate this quantity for the forms in 28 languages. We first present several validatory and exploratory analyses of irregularity. We then show that our analyses provide evidence for a correlation between irregularity and frequency: higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent. To our knowledge, this result is the first of its breadth and confirms longstanding proposals from the linguistics literature. The correlation is more robust when aggregated at the level of whole paradigms---providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes.","tags":null,"title":"Measuring Morphological Irregularity","type":"publication"},{"authors":["Damian Blasi","Ryan Cotterell","Lawrence Wolf-Sonkin","Sabine Stoll","Balthasar Bickel","Marco Baroni"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"93639342f841bc5e61c57825210d6019","permalink":"https://rycolab.io/publication/blasial-acl-19/","publishdate":"2021-08-20T18:07:29.368073Z","relpermalink":"/publication/blasial-acl-19/","section":"publication","summary":"Embedding a clause inside another (``the girl [who likes cars [that run fast]] has arrived″) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics.","tags":null,"title":"On the distribution of deep clausal embeddings: A large cross-linguistic study","type":"publication"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"84cc0a9e2fb24ebde73ba952790fd562","permalink":"https://rycolab.io/publication/bjervaal-acl-19/","publishdate":"2021-08-20T18:07:29.066953Z","relpermalink":"/publication/bjervaal-acl-19/","section":"publication","summary":"The study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. In this paper, we present a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. Our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population.","tags":null,"title":"Uncovering Typological Implications with Belief Nets","type":"publication"},{"authors":["Alexander M. Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Isabelle Augenstein","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"88feb9579b10f90d0805e8ee27e849a8","permalink":"https://rycolab.io/publication/hoyleal-acl-19/","publishdate":"2021-08-20T18:07:29.95873Z","relpermalink":"/publication/hoyleal-acl-19/","section":"publication","summary":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","tags":null,"title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","type":"publication"},{"authors":["Sabrina Mielke","Ryan Cotterell","Kyle Gorman","Brian Roark","Jason Eisner"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"f4b1b6ea089e83dd43526718672dc3d0","permalink":"https://rycolab.io/publication/mielkeal-acl-19/","publishdate":"2021-08-20T18:07:30.693441Z","relpermalink":"/publication/mielkeal-acl-19/","section":"publication","summary":"How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that ``translationese″ is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.","tags":null,"title":"What Kind of Language Is Hard to Language-Model?","type":"publication"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"5d53e7002cc0533fd0478a4bc7673df1","permalink":"https://rycolab.io/publication/bjervaal-naacl-19/","publishdate":"2021-08-20T18:07:29.222293Z","relpermalink":"/publication/bjervaal-naacl-19/","section":"publication","summary":"In the principles-and-parameters framework, the structural features of languages depend on parameters that may be toggled on or off, with a single parameter often dictating the status of multiple features. The implied covariance between features inspires our probabilisation of this line of linguistic inquiry---we develop a generative model of language based on exponential-family matrix factorisation. By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, outperforming several baselines on the task of predicting held-out features. Furthermore, we show that language embeddings pre-trained on monolingual text allow for generalisation to unobserved languages. This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typological features and languages.","tags":null,"title":"A Probabilistic Generative Model of Linguistic Typology","type":"publication"},{"authors":["Chaitanya Malaviya","Shijie Wu","Ryan Cotterell"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"7cfbe19e49c0f05cd5a7b3beaa3f894e","permalink":"https://rycolab.io/publication/malaviyanaal-acl-19/","publishdate":"2021-08-20T18:07:30.405013Z","relpermalink":"/publication/malaviyanaal-acl-19/","section":"publication","summary":"English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of lemmatization seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for lemmatization and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the model in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and lemmatization is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.","tags":null,"title":"A Simple Joint Model for Improved Contextual Neural Lemmatization","type":"publication"},{"authors":["Alexander Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"dc6d51404dc2effc57d9764c923031ec","permalink":"https://rycolab.io/publication/hoyleal-naacl-19/","publishdate":"2021-08-20T18:07:30.108211Z","relpermalink":"/publication/hoyleal-naacl-19/","section":"publication","summary":"When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.","tags":null,"title":"Combining Sentiment Lexica with a Multi-View Variational Autoencoder","type":"publication"},{"authors":["Ekaterina Vylomova","Ryan Cotterell","Timothy Baldwin","Trevor Cohn","Jason Eisner"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"3f8612ffd7dc7fc048ff998b6a81e076","permalink":"https://rycolab.io/publication/vylomovaal-naacl-19/","publishdate":"2021-08-20T18:07:31.134744Z","relpermalink":"/publication/vylomovaal-naacl-19/","section":"publication","summary":"Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide ''gold'' tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.","tags":null,"title":"Contextualization of Morphological Inflection","type":"publication"},{"authors":["Jieyu Zhao","Tianlu Wang","Mark Yatskar","Ryan Cotterell","Vicente Ordonez","Kai-Wei Chang"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"07cd7213afdbf397a75bca00c039c3ee","permalink":"https://rycolab.io/publication/zhoual-naacl-19/","publishdate":"2021-08-20T18:07:31.879823Z","relpermalink":"/publication/zhoual-naacl-19/","section":"publication","summary":"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","tags":null,"title":"Gender Bias in Contextualized Word Embeddings","type":"publication"},{"authors":["Shijia Liu","Adina Williams","Hongyuan Mei","Ryan Cotterell"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"06e41617c9ae911725b3fc26bd63b47d","permalink":"https://rycolab.io/publication/liual-naacl-19/","publishdate":"2021-08-20T18:07:30.261308Z","relpermalink":"/publication/liual-naacl-19/","section":"publication","summary":"While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the classifiers modify. Using the empirical distribution of classifiers from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the mutual information (in bits) between the distribution over classifiers and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.","tags":null,"title":"On the Idiosyncrasies of the Mandarin Chinese Classifier System","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Mans Hulden","Jason Eisner"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"4df7e71c0378509ff7f0bb1850a5990f","permalink":"https://rycolab.io/publication/cotterellal-tacl-19/","publishdate":"2021-08-20T18:07:29.514482Z","relpermalink":"/publication/cotterellal-tacl-19/","section":"publication","summary":"We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language's inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm---how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.","tags":null,"title":"On the Complexity and Typology of Inflectional Morphological Systems","type":"publication"},{"authors":["Sebastian Ruder$^*$","Ryan Cotterell$^*$","Yova Kementchedjhieva","Anders Søgaard"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"2f067d72799894553dc0f72584bb1252","permalink":"https://rycolab.io/publication/ruderal-emnlp-18/","publishdate":"2021-08-20T18:07:25.332333Z","relpermalink":"/publication/ruderal-emnlp-18/","section":"publication","summary":"We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.","tags":null,"title":"A Discriminative Latent-Variable Model for Bilingual Lexicon Induction","type":"publication"},{"authors":["Yova Kementchedjhieva","Sebastian Ruder","Ryan Cotterell","Anders Søgaard"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"12558fbbd9611f5da65ff93738691b22","permalink":"https://rycolab.io/publication/kementchedjhievaal-conll-18/","publishdate":"2021-08-20T18:07:24.739743Z","relpermalink":"/publication/kementchedjhievaal-conll-18/","section":"publication","summary":"Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings.","tags":null,"title":"Generalizing Procrustes Analysis for Better Bilingual Dictionary Induction","type":"publication"},{"authors":["Shijie Wu","Pamela Shapiro","Ryan Cotterell"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"898bd87faa75ca4cc9c323a70eb39ebf","permalink":"https://rycolab.io/publication/wual-emnlp-18/","publishdate":"2021-08-20T18:07:25.627764Z","relpermalink":"/publication/wual-emnlp-18/","section":"publication","summary":"Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.","tags":null,"title":"Hard Non-Monotonic Attention for Character-Level Transduction","type":"publication"},{"authors":["Arya D. McCarthy","Miikka Silfverberg","Ryan Cotterell","Mans Hulden","David Yarowsky"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"a150607e03d1cc2b9cc4a3a8da3fad7a","permalink":"https://rycolab.io/publication/mccarthyal-udw-18/","publishdate":"2021-08-20T18:07:25.172465Z","relpermalink":"/publication/mccarthyal-udw-18/","section":"publication","summary":"The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languages---UD at the token level and UniMorph at the type level. As each corpus is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project's annotations could be used to validate the other's. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as parsing and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13% recall. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.","tags":null,"title":"Marrying Universal Dependencies and Universal Morphology","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Arya D. McCarthy","Katharina Kann","Sabrina Mielke","Garrett Nicolai","Miikka Silfverberg","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"3af314661f7a50d23092c996ac3531de","permalink":"https://rycolab.io/publication/cotterellal-conll-18/","publishdate":"2021-08-20T18:07:23.865116Z","relpermalink":"/publication/cotterellal-conll-18/","section":"publication","summary":"The CoNLL-SIGMORPHON 2018 shared task on supervised learning of morphological generation featured data sets from 103 typologically diverse languages. Apart from extending the number of languages involved in earlier supervised tasks of generating inflected forms, this year the shared task also featured a new second task which asked participants to inflect words in sentential context, similar to a cloze task. This second task featured seven languages. Task 1 received 27 submissions and task 2 received 6 submissions. Both tasks featured a low, medium, and high data condition. Nearly all submissions featured a neural component and built on highly-ranked systems from the earlier 2017 shared task. In the inflection task (task 1), 41 of the 52 languages present in last year’s inflection task showed improvement by the best systems in the low-resource setting. The cloze task (task 2) proved to be difficult, and few submissions managed to consistently improve upon both a simple neural baseline system and a lemmarepeating baseline.","tags":null,"title":"The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection","type":"publication"},{"authors":["Lawrence Wolf-Sonkin$^*$","Jason Naradowsky$^*$","Sabrina J. Mielke$^*$","Ryan Cotterell$^*$"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"81c65001211ead5c4bae2ff60e6a56c7","permalink":"https://rycolab.io/publication/wolf-sonkin-acl-18/","publishdate":"2021-08-20T18:07:25.484794Z","relpermalink":"/publication/wolf-sonkin-acl-18/","section":"publication","summary":"Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.","tags":null,"title":"A Structured Variational Autoencoder for Contextual Morphological Inflection","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"3ae1326e83220b4c8f5d6afadaa7e6a8","permalink":"https://rycolab.io/publication/cotterelleisner-naacl-18/","publishdate":"2021-08-20T18:07:24.307085Z","relpermalink":"/publication/cotterelleisner-naacl-18/","section":"publication","summary":"What makes some types of languages more probable than others? For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be? The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language. In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains. In contrast to previous work, we work directly with the acoustic information---the first two formant values---rather than modeling discrete sets of symbols from the international phonetic alphabet. We develop a novel generative probability model and report results on over 200 languages.","tags":null,"title":"A Deep Generative Model of Vowel Formant Typology","type":"publication"},{"authors":["Ryan Cotterell","Sabrina J. Mielke","Jason Eisner","Brian Roark"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"d436f65260058640e7930be367a0e8cb","permalink":"https://rycolab.io/publication/cotterellal-naacl-18-a/","publishdate":"2021-08-20T18:07:24.012768Z","relpermalink":"/publication/cotterellal-naacl-18-a/","section":"publication","summary":"For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.","tags":null,"title":"Are All Languages Equally Hard to Language-Model?","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Sabrina J. Mielke","Jason Eisner"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9ee0d90db50ad67fb3060518130a3c42","permalink":"https://rycolab.io/publication/cotterellal-naacl-18-b/","publishdate":"2021-08-20T18:07:24.162106Z","relpermalink":"/publication/cotterellal-naacl-18-b/","section":"publication","summary":"Lexical ambiguity makes it difficult to compute useful statistics of a corpus. A given word form might represent any of several morphological feature bundles. One can, however, use unsupervised learning (as in EM) to fit a model that probabilistically disambiguates word forms. We present such an approach, which employs a neural network to smoothly model a prior distribution over feature bundles (even rare ones). Although this basic model does not consider a token's context, that very property allows it to operate on a simple list of unigram type counts, partitioning each count among different analyses of that unigram. We discuss evaluation metrics for this novel task and report results on 5 languages.","tags":null,"title":"Unsupervised Disambiguation of Syncretism in Inflected Lexicons","type":"publication"},{"authors":["Christo Kirov","Ryan Cotterell","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Patrick Xia","Manaal Faruqui","Sabrina Mielke","Arya McCarthy","Sandra Kübler","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"89cfa7b59e28d9a972a4e49489e18e08","permalink":"https://rycolab.io/publication/kiroval-lrec-18/","publishdate":"2021-08-20T18:07:24.884129Z","relpermalink":"/publication/kiroval-lrec-18/","section":"publication","summary":"The Universal Morphology (UniMorph) project is a collaborative effort to improve how NLP handles complex morphology across the world’s languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the collection, annotation, and dissemination of project resources since the initial UniMorph release described at LREC 2016.","tags":null,"title":"UniMorph 2.0: Universal Morphology","type":"publication"},{"authors":["Ryan Cotterell","Julia Kreutzer"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"8f9f94018a73e2ff526a65aeedc1d3ec","permalink":"https://rycolab.io/publication/cotterellkreutzer-arxiv-18/","publishdate":"2021-08-20T18:07:24.448949Z","relpermalink":"/publication/cotterellkreutzer-arxiv-18/","section":"publication","summary":"","tags":null,"title":"Explaining and Generalizing Back-Translation through Wake-Sleep","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1030c6c628ef205a3f67724e7d809180","permalink":"https://rycolab.io/publication/cotterellschuetze-tacl-18/","publishdate":"2021-08-20T18:07:24.593742Z","relpermalink":"/publication/cotterellschuetze-tacl-18/","section":"publication","summary":"Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word′s meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituent segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DErivBase (Zeller et al., 2013) data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3% and 5%. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist′s notion of morphological productivity.","tags":null,"title":"Joint Semantic Synthesis and Morphological Analysis of the Derived Word","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Mans Hulden","Jason Eisner"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1fa421ac33a261ae3d663c5a02cdb76a","permalink":"https://rycolab.io/publication/cotterellal-arxiv-18/","publishdate":"2021-08-20T18:07:23.719115Z","relpermalink":"/publication/cotterellal-arxiv-18/","section":"publication","summary":"","tags":null,"title":"On the Diachronic Stability of Irregularity in Inflectional Morphology","type":"publication"},{"authors":["Christo Kirov","Ryan Cotterell"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"15717ae85e8cb20882e5431a78d9e060","permalink":"https://rycolab.io/publication/kirovcotterell-tacl-18/","publishdate":"2021-08-20T18:07:25.02946Z","relpermalink":"/publication/kirovcotterell-tacl-18/","section":"publication","summary":"Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter in 1988, Pinker and Prince presented a comprehensive rebuttal of many of Rumelhart and McClelland's claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince's criticisms without requiring any simplification of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a reexamination of their utility in linguistic and cognitive modeling.","tags":null,"title":"Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate","type":"publication"},{"authors":["Ryan Cotterell","Kevin Duh."],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"1f118c7c6d7e9f331838935ea2fed573","permalink":"https://rycolab.io/publication/cotterellduh-ijcnlp-17/","publishdate":"2021-08-20T18:07:19.338782Z","relpermalink":"/publication/cotterellduh-ijcnlp-17/","section":"publication","summary":"Low-resource named entity recognition is still an open problem in NLP. Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world's languages it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows knowledge transfer from the high-resource languages to the low-resource ones, improving F1 by up to 9.8 points.","tags":null,"title":"Low-Resource Named Entity Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields","type":"publication"},{"authors":["Ryan Cotterell","Georg Heigold"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"4174111fd88ca9bd66c5290666730be9","permalink":"https://rycolab.io/publication/cotterellheigold-emnlp-17/","publishdate":"2021-08-20T18:07:19.625699Z","relpermalink":"/publication/cotterellheigold-emnlp-17/","section":"publication","summary":"Even for common NLP tasks, sufficient supervision is not available in many languages--morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones.","tags":null,"title":"Cross-lingual, Character-Level Neural Morphological Tagging","type":"publication"},{"authors":["Ryan Cotterell","Ekaterina Vylomova","Huda Khayrallah","Christo Kirov","David Yarowsky"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"d1c98d2aefe8457ea6296afaa5dc2204","permalink":"https://rycolab.io/publication/cotterellal-emnlp-17/","publishdate":"2021-08-20T18:07:18.883532Z","relpermalink":"/publication/cotterellal-emnlp-17/","section":"publication","summary":"The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models adapted from the inflection task are able to learn the range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems.","tags":null,"title":"Paradigm Completion for Derivational Morphology","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Patrick Xia","Manaal Faruqui","Sandra Kübler","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"c09bf1108b8a9f99886af0418cb4f453","permalink":"https://rycolab.io/publication/cotterellal-conll-17/","publishdate":"2021-08-20T18:07:18.740686Z","relpermalink":"/publication/cotterellal-conll-17/","section":"publication","summary":"The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation required systems to be trained and tested in each of 52 typologically diverse languages. In sub-task 1, submitted systems were asked to predict a specific inflected form of a given lemma. In sub-task 2, systems were given a lemma and some of its specific inflected forms, and asked to complete the inflectional paradigm by predicting all of the remaining inflected forms. Both sub-tasks included high, medium, and low-resource conditions. Sub-task 1 received 24 system submissions, while sub-task 2 received 3 system submissions. Following the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared task, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in non-identical sets of inflected forms being predicted correctly, suggesting that there is room for future improvement.","tags":null,"title":"CoNLL--SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection in 52 Languages","type":"publication"},{"authors":["Francis Ferraro","Adam Poliak","Ryan Cotterell","Benjamin Van Durme"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"087c51922a795fbc97864e39def05304","permalink":"https://rycolab.io/publication/ferraroal-starsem-17/","publishdate":"2021-08-20T18:07:19.782117Z","relpermalink":"/publication/ferraroal-starsem-17/","section":"publication","summary":"We study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10% gains over baselines.","tags":null,"title":"Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"bcf915476f31940a15d842647e3f1001","permalink":"https://rycolab.io/publication/kannal-acl-17/","publishdate":"2021-08-20T18:07:19.92643Z","relpermalink":"/publication/kannal-acl-17/","section":"publication","summary":"We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.","tags":null,"title":"One-Shot Neural Cross-Lingual Transfer for Paradigm Completion","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"71b13a4d0772b4d54b40f69f2e93feaf","permalink":"https://rycolab.io/publication/cotterelleisner-acl-17/","publishdate":"2021-08-20T18:07:19.482639Z","relpermalink":"/publication/cotterelleisner-acl-17/","section":"publication","summary":"Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.","tags":null,"title":"Probabilistic Typology: Deep Generative Models of Vowel Inventories","type":"publication"},{"authors":["Christo Kirov","John Sylak-Glassman","Rebecca Knowles","Ryan Cotterell","Matt Post"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"cd8133a88495b62b72d13bc2400f38a2","permalink":"https://rycolab.io/publication/kiroval-eacl-17/","publishdate":"2021-08-20T18:07:20.22186Z","relpermalink":"/publication/kiroval-eacl-17/","section":"publication","summary":"A traditional claim in linguistics is that all human languages are equally expressive---able to convey the same wide range of meanings. Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech. The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation.","tags":null,"title":"A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax","type":"publication"},{"authors":["Ekaterina Vylomova","Ryan Cotterell","Timothy Baldwin","Trevor Cohn"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"88dd0bbfa4129bc86b6c49d1d0fdf629","permalink":"https://rycolab.io/publication/vylomovaal-eacl-17/","publishdate":"2021-08-20T18:07:20.536737Z","relpermalink":"/publication/vylomovaal-eacl-17/","section":"publication","summary":"Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.","tags":null,"title":"Context-Aware Prediction of Derivational Word-forms","type":"publication"},{"authors":["Ryan Cotterell","Adam Poliak","Ben Van Durme","Jason Eisner"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"62e68751e7f01daaccf799f3031f75b2","permalink":"https://rycolab.io/publication/cotterellalb-eacl-17/","publishdate":"2021-08-20T18:07:19.174746Z","relpermalink":"/publication/cotterellalb-eacl-17/","section":"publication","summary":"The popular skip-gram model induces word embeddings by exploiting the signal from word-context coocurrence. We offer a new interpretation of skip-gram based on exponential family PCA-a form of matrix factorization to generalize the skip-gram model to tensor factorization. In turn, this lets us train embeddings through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or morphological information (to share parameters across related words). We experiment on 40 languages and show our model improves upon skip-gram.","tags":null,"title":"Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis","type":"publication"},{"authors":["Arun Kumar","Ryan Cotterell","Lluís Padró","Antoni Oliver"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"950429cf9d367338236f6c9801bbdeb1","permalink":"https://rycolab.io/publication/kumaral-eacl-17/","publishdate":"2021-08-20T18:07:20.385595Z","relpermalink":"/publication/kumaral-eacl-17/","section":"publication","summary":"The Dravidian languages are one of the most widely spoken language families in the world, yet there are very few annotated resources available to NLP researchers. To remedy this, we create DravMorph, a corpus annotated for morphological segmentation and part-of-speech. Additionally, we exploit novel features and higher-order models to set state-of-the-art results on these corpora on both tasks, beating techniques proposed in the literature by as much as 4 points in segmentation F1.","tags":null,"title":"Morphological Analysis of the Dravidian Language Family","type":"publication"},{"authors":["Ryan Cotterell","John Sylak-Glassman","Christo Kirov"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"fc54a64fe12dd9d57beea964e7ce8876","permalink":"https://rycolab.io/publication/cotterellala-eacl-17/","publishdate":"2021-08-20T18:07:19.030607Z","relpermalink":"/publication/cotterellala-eacl-17/","section":"publication","summary":"Many of the world's languages contain an abundance of inflected forms for each lexeme. A critical task in processing such languages is predicting these inflected forms. We develop a novel statistical model for the problem, drawing on graphical modeling techniques and recent advances in deep learning. We derive a Metropolis-Hastings algorithm to jointly decode the model. Our Bayesian network draws inspiration from principal parts morphological analysis. We demonstrate improvements on 5 languages.","tags":null,"title":"Neural Graphical Models over Strings for Principal Parts Morphological Paradigm Completion","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"919462c9664194568756fd3006526dba","permalink":"https://rycolab.io/publication/kannal-eacl-17/","publishdate":"2021-08-20T18:07:20.076832Z","relpermalink":"/publication/kannal-eacl-17/","section":"publication","summary":"We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.","tags":null,"title":"Neural Multi-Source Morphological Reinflection","type":"publication"},{"authors":["Ryan Cotterell","Arun Kumar","Hinrich Schütze"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"a39aaa943290d5f6face02f3d9d47983","permalink":"https://rycolab.io/publication/cotterellal-emnlp-16/","publishdate":"2021-08-20T18:07:14.345665Z","relpermalink":"/publication/cotterellal-emnlp-16/","section":"publication","summary":"Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure---especially in the case of derivational morphology. In this work, we introduce a discriminative joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.","tags":null,"title":"Morphological Segmentation Inside-Out","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"1feb9c000aedb9cbb4a319955397bc86","permalink":"https://rycolab.io/publication/kannal-emnlp-16/","publishdate":"2021-08-20T18:07:14.92185Z","relpermalink":"/publication/kannal-emnlp-16/","section":"publication","summary":"Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoderdecoder model for this task. Additionally, we extend our model to include morphemelevel and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian.","tags":null,"title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments","type":"publication"},{"authors":["Tim Vieira$^*$","Ryan Cotterell$^*$","Jason Eisner"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"1906bb6181348303c0d677575abad1ce","permalink":"https://rycolab.io/publication/vieiraal-emnlp-16/","publishdate":"2021-08-20T18:07:15.388898Z","relpermalink":"/publication/vieiraal-emnlp-16/","section":"publication","summary":"We propose a method for learning the structure of variable-order CRFs, a more flexible variant of higher-order linear-chain CRFs. Variableorder CRFs achieve faster inference by including features for only some of the tag ngrams. Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer. An active-set outer loop allows the feature set to grow as far as needed. On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2--6x speedup over a baseline, with no significant drop in accuracy.","tags":null,"title":"Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze","Jason Eisner"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"d5e258b2cfce483a0a8e30a31119cd1d","permalink":"https://rycolab.io/publication/cotterellal-acl-16/","publishdate":"2021-08-20T18:07:14.202616Z","relpermalink":"/publication/cotterellal-acl-16/","section":"publication","summary":"Languages with rich inflectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For instance, each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are unlikely to observe all inflections of a given lemma. This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information. We solve this problem by exploiting existing morphological resources that can enumerate a word’s component morphemes. We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create embeddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy, and word similarity","tags":null,"title":"Morphological Smoothing and Extrapolation of Word Embeddings","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"0dae3427f8a795d86396e714ddca0095","permalink":"https://rycolab.io/publication/cotterellal-sigmorphon-16/","publishdate":"2021-08-20T18:07:14.636438Z","relpermalink":"/publication/cotterellal-sigmorphon-16/","section":"publication","summary":"The 2016 SIGMORPHON Shared Task was devoted to the problem of morphological reinflection. It introduced morphological datasets for 10 languages with diverse typological characteristics. The shared task drew submissions from 9 teams representing 11 institutions reflecting a variety of approaches to addressing supervised learning of reinflection. For the simplest task, inflection generation from lemmas, the best system averaged 95.56% exact-match accuracy across all languages, ranging from Maltese (88.99%) to Hungarian (99.30%). With the relatively large training datasets provided, recurrent neural network architectures consistently performed best—in fact, there was a significant margin between neural and non-neural approaches. The best neural approach, averaged over all tasks and languages, outperformed the best nonneural one by 13.76% absolute; on individual tasks and languages the gap in accuracy sometimes exceeded 60%. Overall, the results show a strong state of the art, and serve as encouragement for future shared tasks that explore morphological analysis and generation with varying degrees of supervision.","tags":null,"title":"The SIGMORPHON 2016 Shared Task—Morphological Reinflection","type":"publication"},{"authors":["Ryan Cotterell","Tim Vieira","Hinrich Schütze"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"7fa0b09b73010847f89c913ed7de741a","permalink":"https://rycolab.io/publication/cotterellal-naacl-16/","publishdate":"2021-08-20T18:07:14.49089Z","relpermalink":"/publication/cotterellal-naacl-16/","section":"publication","summary":"We present a model of morphological segmentation that jointly learns to segment and restore orthographic changes, e.g., funniest 7 → fun-y-est. We term this form of analysis canonical segmentation and contrast it with the traditional surface segmentation, which segments a surface form into a sequence of substrings, e.g., funniest 7 → funn-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian.","tags":null,"title":"A Joint Model of Orthography and Morphological Segmentation","type":"publication"},{"authors":["Pushpendre Rastogi","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"bffcb6da09c3c39cf85ea1db79e3a5b8","permalink":"https://rycolab.io/publication/rastogial-naacl-16/","publishdate":"2021-08-20T18:07:15.241797Z","relpermalink":"/publication/rastogial-naacl-16/","section":"publication","summary":"How should one apply deep learning to tasks such as morphological reinflection, which stochastically edit one string to get another? A recent approach to such sequence-to-sequence tasks is to compress the input string into a vector that is then used to generate the output string, using recurrent neural networks. In contrast, we propose to keep the traditional architecture, which uses a finite-state transducer to score all possible output strings, but to augment the scoring function with the help of recurrent networks. A stack of bidirectional LSTMs reads the input string from leftto-right and right-to-left, in order to summarize the input context in which a transducer arc is applied. We combine these learned features with the transducer to define a probability distribution over aligned output strings, in the form of a weighted finite-state automaton. This reduces hand-engineering of features, allows learned features to examine unbounded context in the input string, and still permits exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization.","tags":null,"title":"Weighting Finite-State Transductions With Neural Context","type":"publication"},{"authors":["Chandler May","Ryan Cotterell","Benjamin Van Durme"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7c53d024a335da48d7abaa47844cbe98","permalink":"https://rycolab.io/publication/mayal-arxiv-16/","publishdate":"2021-08-20T18:07:15.067282Z","relpermalink":"/publication/mayal-arxiv-16/","section":"publication","summary":"","tags":null,"title":"Analysis of Morphology in Topic Modeling","type":"publication"},{"authors":["John Sylak-Glassman","Ryan Cotterell"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ccfb86dfac9259d2a0cd43a651f99e75","permalink":"https://rycolab.io/publication/glassmanal-cls-16/","publishdate":"2021-08-20T18:07:14.778626Z","relpermalink":"/publication/glassmanal-cls-16/","section":"publication","summary":"","tags":null,"title":"Contrastive Morphological Typology and Logical Hierarchies","type":"publication"},{"authors":["Nanyun Peng","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"27421098a0f9e3c451ea02557297cb9c","permalink":"https://rycolab.io/publication/pengal-emnlp-15/","publishdate":"2021-08-20T18:07:10.261959Z","relpermalink":"/publication/pengal-emnlp-15/","section":"publication","summary":"We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming. We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of variable-length n-gram count features.  This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.","tags":null,"title":"Dual Decomposition Inference for Graphical Models over Strings","type":"publication"},{"authors":["Thomas Müller","Ryan Cotterell","Alexander Fraser","Hinrich Schütze"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"307e76fe83844df78b8722e4c4925865","permalink":"https://rycolab.io/publication/muelleral-conll-15/","publishdate":"2021-08-20T18:07:10.405404Z","relpermalink":"/publication/muelleral-conll-15/","section":"publication","summary":"We present Lemming, a modular log-linear model that jointly models lemmatization and tagging and supports the integration of arbitrary global features. It is trainable on corpora annotated with gold standard tags and lemmata and does not rely on morphological dictionaries or analyzers. Lemming sets the new state of the art in token-based statistical lemmatization on six languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05 to 1.58. We also give empirical evidence that jointly modeling morphological tags and lemmata is mutually beneficial.","tags":null,"title":"Joint Lemmatization and Morphological Tagging with Lemming","type":"publication"},{"authors":["Ryan Cotterell","Thomas Müller","Alexander Fraser","Hinrich Schütze"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"457b1282971eef22ec6477186cc6f4c1","permalink":"https://rycolab.io/publication/cotterellal-conll-15/","publishdate":"2021-08-20T18:07:10.548594Z","relpermalink":"/publication/cotterellal-conll-15/","section":"publication","summary":"We present labeled morphological segmentation—an alternative view of morphological processing that unifies several tasks. We introduce a new hierarchy of morphotactic tagsets and Chipmunk, a discriminative morphological segmentation system that, contrary to previous work, explicitly models morphotactics. We show improved performance on three tasks for all six languages: (i) morphological segmentation, (ii) stemming and (iii)  morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline.","tags":null,"title":"Labeled Morphological Segmentation with Semi-Markov Models","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"58c9c1b17d1c3c1e7deb1d36d1765e88","permalink":"https://rycolab.io/publication/cotterellschuetze-naacl-15/","publishdate":"2021-08-20T18:07:10.980625Z","relpermalink":"/publication/cotterellschuetze-naacl-15/","section":"publication","summary":"Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semi-supervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study.","tags":null,"title":"Morphological Word Embeddings","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"db153a927a072c4049d322aa1f4dfc77","permalink":"https://rycolab.io/publication/cotterelleisner-naacl-15/","publishdate":"2021-08-20T18:07:10.836487Z","relpermalink":"/publication/cotterelleisner-naacl-15/","section":"publication","summary":"We present penalized expectation propagation (PEP), a novel algorithm for approximate inference in graphical models. Expectation propagation is a variant of loopy belief propagation that keeps messages tractable by projecting them back into a given family of functions. Our extension, PEP, uses a structuredsparsity penalty to encourage simple messages, thus balancing speed and accuracy. We specifically show how to instantiate PEP in the case of string-valued random variables, where we adaptively approximate finite-state distributions by variable-order n-gram models. On phonological inference problems, we obtain substantial speedup over previous related algorithms with no significant loss in accuracy.","tags":null,"title":"Penalized Expectation Propagation for Graphical Models over Strings","type":"publication"},{"authors":["Ryan Cotterell","Nanyun Peng","Jason Eisner"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"585f26f2d387f8189154c38ea4fd00a6","permalink":"https://rycolab.io/publication/cotterellal-tacl-15/","publishdate":"2021-08-20T18:07:10.693131Z","relpermalink":"/publication/cotterellal-tacl-15/","section":"publication","summary":"The observed pronunciations or spellings of words are often explained as arising from the ''underlying forms'' of their morphemes. These forms are latent strings that linguists try to reconstruct by hand. We propose to reconstruct them automatically at scale, enabling generalization to new words. Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express, we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form. Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite-state machines with trainable weights. We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages.","tags":null,"title":"Modeling Word Forms Using Latent Underlying Morphs and Phonology","type":"publication"},{"authors":["Ryan Cotterell","Nanyun Peng","Jason Eisner"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"98faa9c135408e6a0e6802b0a788931b","permalink":"https://rycolab.io/publication/cotterellal-acl-14/","publishdate":"2021-08-20T18:07:06.497496Z","relpermalink":"/publication/cotterellal-acl-14/","section":"publication","summary":"String similarity is most often measured by weighted or unweighted edit distance d(x, y). Ristad and Yianilos (1998) defined stochastic edit distance—a probability distribution p(y | x) whose parameters can be trained from data. We generalize this so that the probability of choosing each edit operation can depend on contextual features. We show how to construct and train a probabilistic finite-state transducer that computes our stochastic  ontextual edit distance. To illustrate the improvement from conditioning on context, we model typos found in social media text.","tags":null,"title":"Stochastic Contextual Edit Distance and Probabilistic FSTs","type":"publication"},{"authors":["Ryan Cotterell","Chris Callison-Burch"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"7b97832957e1076cee3b3f8e655b5dbe","permalink":"https://rycolab.io/publication/cotterellcallison-burch-lrec-14/","publishdate":"2021-08-20T18:07:06.640488Z","relpermalink":"/publication/cotterellcallison-burch-lrec-14/","section":"publication","summary":"","tags":null,"title":"A Multi-Dialect, Multi-Genre Corpus of Informal Written Arabic","type":"publication"},{"authors":["Ryan Cotterell","Adithya Renduchintala","Naomi Saphra","Chris Callison-Burch"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"4e4d9767fc5a4c61544a2c7c302d436d","permalink":"https://rycolab.io/publication/cotterellal-lrec-14/","publishdate":"2021-08-20T18:07:06.784596Z","relpermalink":"/publication/cotterellal-lrec-14/","section":"publication","summary":"","tags":null,"title":"An Algerian Arabic-French Code-Switched Corpus","type":"publication"},{"authors":["Gaurav Kumar","Yuan Cao","Ryan Cotterell","Chris Callison-Burch","Daniel Povey","Sanjeev Khudanpur"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"d05928c8ad19690bf4167afe768a1dc6","permalink":"https://rycolab.io/publication/kumaral-iwslt-14/","publishdate":"2021-08-20T18:07:06.354263Z","relpermalink":"/publication/kumaral-iwslt-14/","section":"publication","summary":"","tags":null,"title":"Translation of the CALLHOME Egyptian Arabic Corpus For Conversational Speech Translation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1b5731b483ae8dd0ebcded987d15d758","permalink":"https://rycolab.io/classes/aflt-s22/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/aflt-s22/","section":"classes","summary":"This course serves as an introduction to various advanced topics in formal language theory. The primary focus of the course is on weighted formalisms, which can easily be applied in machine learning. Topics include finite-state machines as well as the algorithms that are commonly used for their manipulation. We will also cover weighted context-free grammars, weighted tree automata, and weighted mildly context-sensitive formalisms.","tags":null,"title":"Advanced Formal Language Theory, Spring 2022","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7ce7eb97a129f3cef6bf9e82a90560e9","permalink":"https://rycolab.io/classes/aflt-s23/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/aflt-s23/","section":"classes","summary":"This course serves as an introduction to various advanced topics in formal language theory. The primary focus of the course is on weighted formalisms, which can easily be applied in machine learning. Topics include finite-state machines as well as the algorithms that are commonly used for their manipulation. We will also cover weighted context-free grammars, weighted tree automata, and weighted mildly context-sensitive formalisms.","tags":null,"title":"Advanced Formal Language Theory, Spring 2023","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"951c2c183f777b1084f4d02305b8e10c","permalink":"https://rycolab.io/classes/aflt-s24/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/aflt-s24/","section":"classes","summary":"This course serves as an introduction to various advanced topics in formal language theory. The primary focus of the course is on weighted formalisms, which can easily be applied in machine learning. Topics include finite-state machines as well as the algorithms that are commonly used for their manipulation. We will also cover weighted context-free grammars, weighted pushdown automata, weighted tree automata, and weighted mildly context-sensitive formalisms.","tags":null,"title":"Advanced Formal Language Theory, Spring 2024","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f83406609d3f635abc72388910ba3238","permalink":"https://rycolab.io/classes/info-theory-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/info-theory-tutorial/","section":"classes","summary":"The *Information Theory in Linguistics* course focuses on the application of information-theoretic methods to natural language processing, emphasizing interdisciplinary connections with the field of linguistics.","tags":null,"title":"COLING 2022","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4535e802ff19249b73859ad06460880f","permalink":"https://rycolab.io/classes/dep-parsing-sem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/dep-parsing-sem/","section":"classes","summary":"This seminar explores a variety of algorithms for efficient dependency parsing and their derivatioin in a unified algebraic framework.","tags":null,"title":"Dependency Structures and Lexicalized Grammars","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1c4ddc1d8e31eeb7d1abec4327e49d10","permalink":"https://rycolab.io/classes/esslli-21/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/esslli-21/","section":"classes","summary":"The *Information Theory in Linguistics* course focuses on the application of information-theoretic methods to natural language processing, emphasizing interdisciplinary connections with the field of linguistics.","tags":null,"title":"ESSLLI 2021","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2cc55ee4d26bc9ba7cced224e755b7ad","permalink":"https://rycolab.io/classes/esslli-23/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/esslli-23/","section":"classes","summary":"This tutorial is a comprehensive introduction to neural network language models, focusing on those based on recurrent neural networks (RNNs) and Transformers (Vaswani et al., 2017), and their relationship to formal language theory. We teach how tools from weighted formal language theory can be useful for understanding the inner workings of and predicting the generalization of modern neural architectures. Over the course of five days, we will explore the theoretical properties of RNNs and their representational capacity in relation to different levels of the weighted Chomsky hierarchy, starting with finite-state automata and the special case of bounded-depth hierarchical languages, and then move on to more complex formalisms such as context-free languages and Turing machines. We will prove multiple theoretical properties of RNNs, including the fact that simple RNNs with infinite precision arithmetic and unbounded computation time can emulate a Turing machine and show how RNNs can optimally represent finite-state automata. We will also discuss recent results in the study of Transformer-based language models from the perspective of formal language theory. Finally, we will discuss the implications of these results for the analysis and practical deployment of language models.","tags":null,"title":"Formal Language Theory and Neural Networks","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"c7b5d080ecd15e3504774cb40693f820","permalink":"https://rycolab.io/classes/acl-2023-tutorial/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/acl-2023-tutorial/","section":"classes","summary":"An increasingly large percentage of natural language processing (NLP) tasks center around the generation of text from probabilistic language models. Despite this trend, techniques for improving or specifying preferences in these generated texts rely mostly on intuition-based heuristics. Further, there lacks a unified presentation of their motivations, practical implementation, successes and pitfalls. Practitioners must, therefore, choose somewhat blindly between generation algorithms—like top-p sampling or beam search—which can lead to wildly different results. At the same time, language generation research continues to criticize and improve the standard toolboxes, further adding entropy to the state of the field. In this tutorial, we will provide a centralized and cohesive discussion of critical considerations when choosing how to generate from a language model. We will cover a wide range of empirically-observed problems (like degradation, hallucination, repetition) and their corresponding proposed algorithmic solutions from recent research (like top-p sampling and its successors). We will then discuss a subset of these algorithms under a unified light; most stochastic generation strategies can be framed as locally adapting the probabilities of a model to avoid failure cases. Finally, we will then cover methods in controlled generation, that go beyond just ensuring coherence to ensure text exhibits specific desired properties. We aim for NLP practitioners and researchers to leave our tutorial with a unified framework which they can use to evaluate and contribute to the latest research in language generation.","tags":null,"title":"Generating Text from Language Models","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"03f109e74abe6d5ee07759b2ced48d23","permalink":"https://rycolab.io/classes/llm-s23/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/llm-s23/","section":"classes","summary":"Large language models have become one of the most commonly deployed NLP inventions. In the past half-decade, their integration into core natural language processing tools has dramatically increased the performance of such tools, and they have entered the public discourse surrounding artificial intelligence. In this course, we start with the probabilistic foundations of language models, i.e., covering what constitutes a language model from a formal, theoretical perspective. We then discuss how to construct and curate training corpora, and introduce many of the neural-network architectures often used to instantiate language models at scale. The course covers aspects of systems programming, discussion of privacy and harms, as well as applications of language models in NLP and beyond.","tags":null,"title":"Large Language Models, Spring 2023","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"70542dc0c233f872c433917861f9f922","permalink":"https://rycolab.io/classes/llm-s24/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/llm-s24/","section":"classes","summary":"Large language models have become one of the most commonly deployed NLP inventions. In the past half-decade, their integration into core natural language processing tools has dramatically increased the performance of such tools, and they have entered the public discourse surrounding artificial intelligence. In this course, we start with the probabilistic foundations of language models, i.e., covering what constitutes a language model from a formal, theoretical perspective. We then discuss how to construct and curate training corpora, and introduce many of the neural-network architectures often used to instantiate language models at scale. The course covers aspects of systems programming, discussion of privacy and harms, as well as applications of language models in NLP and beyond.","tags":null,"title":"Large Language Models, Spring 2024","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"faf78b4c5b33c1d2bc0ac81d00bcaef2","permalink":"https://rycolab.io/classes/intro-nlp-f20/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp-f20/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing, Autumn 2020","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"197234ea5a7a4ba940cb8395f125159c","permalink":"https://rycolab.io/classes/intro-nlp-f21/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp-f21/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing, Fall 2021","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"f47eddbbcdfbfaf901518b26b132e896","permalink":"https://rycolab.io/classes/intro-nlp-f22/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp-f22/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing, Fall 2022","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"356fc2e39abbb74f33f7f23d88324167","permalink":"https://rycolab.io/classes/intro-nlp-f23/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp-f23/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing, Fall 2023","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"37cbbccf26c1ff0827e5ddb2e1bc0186","permalink":"https://rycolab.io/classes/intro-nlp-s21/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp-s21/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing, Spring 2021","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"46d915a0a27bfd43c50baa95785bf782","permalink":"https://rycolab.io/classes/phil-s23/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/phil-s23/","section":"classes","summary":"This graduate class, taught like a seminar, is designed to help you understand the philosophical underpinnings of modern work in natural language processing (NLP), most of which centered around statistical machine learning applied to natural language data.","tags":null,"title":"Philosophy of Language and Computation II, Spring 2023","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"436fe49b0452ad7ed91f2f83c827e732","permalink":"https://rycolab.io/classes/phil-f22/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/phil-f22/","section":"classes","summary":"This graduate class, taught like a seminar, is designed to help you understand the philosophical underpinnings of modern work in natural language processing (NLP), most of which centered around statistical machine learning applied to natural language data.","tags":null,"title":"Philosophy of Language and Computation, Autumn 2022","type":"widget_page"}]