[{"authors":["group"],"categories":null,"content":"We are a collocation of collaborators working on a diverse range of topics in computational linguistics, natural language processing and machine learning.\nLab Motto: We put the fun in funicular!\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3d4d5e14ee766925debc0cd6588cd11b","permalink":"https://rycolab.io/authors/group/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/group/","section":"authors","summary":"We are a collocation of collaborators working on a diverse range of topics in computational linguistics, natural language processing and machine learning.\nLab Motto: We put the fun in funicular!","tags":null,"title":"","type":"authors"},{"authors":["adina"],"categories":null,"content":"Animal Form: Numbat\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bfc0fd43f26e67f5f46c4bc0baf09b8e","permalink":"https://rycolab.io/authors/adina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/adina/","section":"authors","summary":"Animal Form: Numbat\n-- ","tags":null,"title":"Adina Williams","type":"authors"},{"authors":["afra"],"categories":null,"content":"Afra is a first-year doctoral fellow at ETH AI center, where she is advised by Ryan and Elliott Ash. She is broadly interested in a variety of topics related to machine learning for NLP and social sciences. Before starting her Ph.D., she received her master’s in computer science at ETH Zürich and bachelor\u0026rsquo;s in software engineering at Sharif University of Technology. In her spare time, she enjoys digital painting, reading and playing the piano.\nNative Language: Persian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2091cc68cd468c5542742f222e5c4e2a","permalink":"https://rycolab.io/authors/afra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/afra/","section":"authors","summary":"Afra is a first-year doctoral fellow at ETH AI center, where she is advised by Ryan and Elliott Ash. She is broadly interested in a variety of topics related to machine learning for NLP and social sciences.","tags":null,"title":"Afra Amini","type":"authors"},{"authors":["aleximmer"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5eb0414be8e932db3af72dc685eed986","permalink":"https://rycolab.io/authors/aleximmer/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/aleximmer/","section":"authors","summary":"","tags":null,"title":"Alex Immer","type":"authors"},{"authors":["alex"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d7149a99f41440e55ea517c3fb5d3c99","permalink":"https://rycolab.io/authors/alex/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alex/","section":"authors","summary":"","tags":null,"title":"Alexander Hoyle","type":"authors"},{"authors":["alexandra"],"categories":null,"content":"Alexandra is a MSc Data Science student at ETH Zürich. Prior to joining ETH, she did a bachelor\u0026rsquo;s degree in Software Engineering at The University of Sheffield, United Kingdom, where she focused mostly on theoretical computer science and machine learning. She is currently doing an internship in NLP at IBM Research Zürich, after which she will do her master’s thesis supervised by Clara. She enjoys a variety of topics, including machine translation, algorithms, stats, information theory, logic and is trying to learn more about others as well, in search of a primary research interest.\nNative Language: Romanian\nAnimal Form: Chipmunk\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"66960bc565a80e3a472ac6720957f252","permalink":"https://rycolab.io/authors/alexandra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alexandra/","section":"authors","summary":"Alexandra is a MSc Data Science student at ETH Zürich. Prior to joining ETH, she did a bachelor\u0026rsquo;s degree in Software Engineering at The University of Sheffield, United Kingdom, where she focused mostly on theoretical computer science and machine learning.","tags":null,"title":"Alexandra Butoi","type":"authors"},{"authors":["andreas"],"categories":null,"content":"Andreas is a Data Science MSc student. He previously attended Chalmers \u0026amp; UC Berkeley. His interests include NLP, causality, representation learning and computational social science\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"040fe87c0b036f4e659fac698e83c5b3","permalink":"https://rycolab.io/authors/andreas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/andreas/","section":"authors","summary":"Andreas is a Data Science MSc student. He previously attended Chalmers \u0026amp; UC Berkeley. His interests include NLP, causality, representation learning and computational social science","tags":null,"title":"Andreas Opedal","type":"authors"},{"authors":["anej"],"categories":null,"content":"Anej is a second year Data Science MSc student with Bachelor\u0026rsquo;s in CS and Math from University in Ljubljana, Slovenia. He is fascinated by the complexity and diversity of human language and the (computational) challenges these bring.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b99e1610c7ea1dbff34a3b8d0c7e725f","permalink":"https://rycolab.io/authors/anej/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/anej/","section":"authors","summary":"Anej is a second year Data Science MSc student with Bachelor\u0026rsquo;s in CS and Math from University in Ljubljana, Slovenia. He is fascinated by the complexity and diversity of human language and the (computational) challenges these bring.","tags":null,"title":"Anej Svete","type":"authors"},{"authors":["anton"],"categories":null,"content":"Anton received his BSc at Ecole Polytechnique (France). He is currently an MSc student in Data Science at ETH.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"007c0c7232240c091e8a22f5d15226f6","permalink":"https://rycolab.io/authors/anton/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/anton/","section":"authors","summary":"Anton received his BSc at Ecole Polytechnique (France). He is currently an MSc student in Data Science at ETH.","tags":null,"title":"Anton Raël","type":"authors"},{"authors":["ayca"],"categories":null,"content":"Ayça is a computer vision research engineer at the ETH Media Technology Center. She completed her MSc at ETH in Electrical Engineering and Information Technology.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7e74f14ffb3b1f0dd0bf0ecc442ec976","permalink":"https://rycolab.io/authors/ayca/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ayca/","section":"authors","summary":"Ayça is a computer vision research engineer at the ETH Media Technology Center. She completed her MSc at ETH in Electrical Engineering and Information Technology.","tags":null,"title":"Ayça Takmaz","type":"authors"},{"authors":["carlos"],"categories":null,"content":"Carlos is a lecturer at ETH Zürich. He is interested in applications of ML into medicine and information security.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8d0972dd4ff800cd6d19f264079261ef","permalink":"https://rycolab.io/authors/carlos/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/carlos/","section":"authors","summary":"Carlos is a lecturer at ETH Zürich. He is interested in applications of ML into medicine and information security.","tags":null,"title":"Carlos Cotrini","type":"authors"},{"authors":["clara"],"categories":null,"content":"Clara is a second year PhD with Ryan at ETH Zürich. She received her B.S. and M.S. from Stanford University in Computational and Mathematical Engineering. Her research interests include neural machine translation and robustness in AI-based systems. In her free time, she likes to rock climb, trail run, and make snarky comments (directed predominantly at Ryan).\nNative Language: English\nAnimal Form: Grumpy Cat\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3de996f9586f710ca1ed8c97792251c","permalink":"https://rycolab.io/authors/clara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/clara/","section":"authors","summary":"Clara is a second year PhD with Ryan at ETH Zürich. She received her B.S. and M.S. from Stanford University in Computational and Mathematical Engineering. Her research interests include neural machine translation and robustness in AI-based systems.","tags":null,"title":"Clara Meister","type":"authors"},{"authors":["damian"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"915cffd0a6d0547272831667bc5c6bb1","permalink":"https://rycolab.io/authors/damian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/damian/","section":"authors","summary":"","tags":null,"title":"Damián Blasi","type":"authors"},{"authors":["daniel"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"13fb944df2e35b4baa2c6664c8ff164a","permalink":"https://rycolab.io/authors/daniel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/daniel/","section":"authors","summary":"","tags":null,"title":"Daniel Vera","type":"authors"},{"authors":["edo"],"categories":null,"content":"Edoardo is a prospective postdoctoral fellow at Mila in Montreal, Canada. He is expected to complete his Ph.D. at the University of Cambridge in September 2020. In Cambridge, he was affiliated with St John\u0026rsquo;s College and supervised by Prof. Anna Korhonen and Ivan Vulić. As an undergrad, he studied modern literature at the University of Pavia, Italy. He works closely with Ryan. He previously interned as an AI/ML researcher at Apple in Cupertino. His research has been supported by a Google Research Faculty Award and an ERC PoC grant for projects co-written with his supervisors. His research focuses on few-shot multilingual learning and on text-based commonsense reasoning.\nNative Language: Italian\nAnimal Form: Water bear (Tardigrade)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0d9d7c8635f56e85ab0ca3b09a1442fb","permalink":"https://rycolab.io/authors/edo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/edo/","section":"authors","summary":"Edoardo is a prospective postdoctoral fellow at Mila in Montreal, Canada. He is expected to complete his Ph.D. at the University of Cambridge in September 2020. In Cambridge, he was affiliated with St John\u0026rsquo;s College and supervised by Prof.","tags":null,"title":"Edoardo M. Ponti","type":"authors"},{"authors":["kat"],"categories":null,"content":"Ekaterina Vylomova is a Postdoctoral Fellow at the University of Melbourne. She holds a PhD in Computer Science obtained from the University of Melbourne. Her research is focused on compositionality modelling for morphology, models for derivational morphology, neural machine translation, and diachronic language modeling.\nNative Language: Russian\nAnimal Form: Mimus polyglottos\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"77b0a8f9ddff4ab3e5c7552c687132ed","permalink":"https://rycolab.io/authors/kat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kat/","section":"authors","summary":"Ekaterina Vylomova is a Postdoctoral Fellow at the University of Melbourne. She holds a PhD in Computer Science obtained from the University of Melbourne. Her research is focused on compositionality modelling for morphology, models for derivational morphology, neural machine translation, and diachronic language modeling.","tags":null,"title":"Ekaterina Vylomova","type":"authors"},{"authors":["eleftheria"],"categories":null,"content":"Eleftheria is a second-year PhD student in Computer Science at ETH Zurich. She is advised by Ryan. Previously, she was a Research Engineer at Disney Research where she worked on natural language understanding and emotion classification. Eleftheria received an MSc in Artificial Intelligence with specialization in NLP from the University of Edinburgh where her thesis was on machine translation. Before that, she received a BA in English Language and Literature with specialization in Linguistics from the University of Athens. At the moment, her research focuses on dependency parsing. Research-wise, she is also interested in computational narratology and cultural economics. Outside of research, Eleftheria likes tea, TvTropes, wholesome memes, and playing soundtracks by ear on the piano.\nNative Language: Greek\nAnimal Form: House cat\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ca5fef1fb2e39756e74f4ff22d43656f","permalink":"https://rycolab.io/authors/eleftheria/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/eleftheria/","section":"authors","summary":"Eleftheria is a second-year PhD student in Computer Science at ETH Zurich. She is advised by Ryan. Previously, she was a Research Engineer at Disney Research where she worked on natural language understanding and emotion classification.","tags":null,"title":"Eleftheria Tsipidi","type":"authors"},{"authors":["liz"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fc7f296399b51eca9f04506227a81c4e","permalink":"https://rycolab.io/authors/liz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liz/","section":"authors","summary":"","tags":null,"title":"Elizabeth Salesky","type":"authors"},{"authors":["ema"],"categories":null,"content":"Emanuele is a first-year PhD student at the University of Copenhagen, supervised by Desmond Elliott and closely collaborating with Ryan. His research is funded through a Marie Skłodowska-Curie TALENT fellowship and focuses on language grounding from multimodal and multilingual contexts. He also likes neural machine translation, typology and ramen.\nBefore his PhD, Emanuele received his master’s degree from the School of Computer and Communication Sciences at EPFL, where he was a member of the Data Science Lab led by Robert West. He also holds two bachelor’s degrees in Information Engineering from Tongji University and in Telecommunications Engineering from Politecnico di Torino.\nNative Language: Italian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bbe9321070c19534ca5cfe036a68e756","permalink":"https://rycolab.io/authors/ema/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ema/","section":"authors","summary":"Emanuele is a first-year PhD student at the University of Copenhagen, supervised by Desmond Elliott and closely collaborating with Ryan. His research is funded through a Marie Skłodowska-Curie TALENT fellowship and focuses on language grounding from multimodal and multilingual contexts.","tags":null,"title":"Emanuele Bugliarello","type":"authors"},{"authors":["florian"],"categories":null,"content":"Florian is in the second year of the M.Sc. Statistics at ETH. He is interested in Bayesian methods for NLP, low-resource NLP and interpretability. At the moment, he is writing his master thesis about reranking dependency parse trees of low-resource languages using Gaussian processes.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"a4e78ba938554b62894080fdbf54d211","permalink":"https://rycolab.io/authors/florian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/florian/","section":"authors","summary":"Florian is in the second year of the M.Sc. Statistics at ETH. He is interested in Bayesian methods for NLP, low-resource NLP and interpretability. At the moment, he is writing his master thesis about reranking dependency parse trees of low-resource languages using Gaussian processes.","tags":null,"title":"Florian Schottmann","type":"authors"},{"authors":["gian"],"categories":null,"content":"Gian is a second year Data Science Student at ETH Zürich. Prior to that, he obtained a BSc in mathematics from ETHZ. His research interests include Natural Language Generation, with a focus on decoding schemes. In his freetime, he likes to mountain bike, play videogames and ride his motorbike around the alps.\nNative Language: Swiss German\nAnimal Form: Jellyfish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6a25e7f4891be9b1b8ffac4d9553ae26","permalink":"https://rycolab.io/authors/gian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gian/","section":"authors","summary":"Gian is a second year Data Science Student at ETH Zürich. Prior to that, he obtained a BSc in mathematics from ETHZ. His research interests include Natural Language Generation, with a focus on decoding schemes.","tags":null,"title":"Gian Wiher","type":"authors"},{"authors":["irene"],"categories":null,"content":"Irene is an MPhil student at the University of Cambridge, where she is supervised by Ryan. She received her BSc in Computer Science with a minor in mathematics and statistics from the University of Helsinki. Her primary interests are machine learning and anything in the intersection of computer science and statistics. During her free time she likes running (away from responsibilities) and correcting people who say that Finland is part of Scandinavia.\nNative Language: Finnish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89af9b35ecd8250fa5b24fe3dfe7d186","permalink":"https://rycolab.io/authors/irene/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/irene/","section":"authors","summary":"Irene is an MPhil student at the University of Cambridge, where she is supervised by Ryan. She received her BSc in Computer Science with a minor in mathematics and statistics from the University of Helsinki.","tags":null,"title":"Irene Nikkarinen","type":"authors"},{"authors":["isabelle"],"categories":null,"content":"Isabelle is an associate professor at the University of Copenhagen, Department of Computer Science and works in the general areas of Statistical Natural Language Processing and Machine Learning\nAnimal Form: Owl\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"76b480c4a09bc036d7176e3210f31a66","permalink":"https://rycolab.io/authors/isabelle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/isabelle/","section":"authors","summary":"Isabelle is an associate professor at the University of Copenhagen, Department of Computer Science and works in the general areas of Statistical Natural Language Processing and Machine Learning\nAnimal Form: Owl","tags":null,"title":"Isabelle Augenstein","type":"authors"},{"authors":["jen"],"categories":null,"content":"Jennifer is a first-year PhD student at the University of Cambridge, supervised by Ryan. She earned an MMathPhys in Mathematics and Physics at the University of Warwick and an MPhil in Advanced Computer Science at the University of Cambridge where she was also supervised by Ryan. She is interested in techniques for low-resource NLP, gender bias in NLP and annotation of linguistic data, among other things. Her background is in Algebraic Geometry, so she is always happy to get a chance to put her mathematical skills to good use. In her spare time she enjoys learning languages, reading, eating chicken katsu curry and travelling as much as possible.\nNative Language: English\nAnimal Form: Otter\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5d240d57e5fd9bed1d26eb181ebe2842","permalink":"https://rycolab.io/authors/jen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jen/","section":"authors","summary":"Jennifer is a first-year PhD student at the University of Cambridge, supervised by Ryan. She earned an MMathPhys in Mathematics and Physics at the University of Warwick and an MPhil in Advanced Computer Science at the University of Cambridge where she was also supervised by Ryan.","tags":null,"title":"Jennifer C. White","type":"authors"},{"authors":["jiaoda"],"categories":null,"content":"Jiaoda Li is an ETH AI Center Doctoral Fellow. He is part of the groups of Ryan Cotterell and Stefan Feuerriegel. He also works closely with Mrinmaya Sachan and Rico Sennrich. He received his Master degree in Data Science from ETH Zürich in 2021. Prior to that, he was an undergraduate student in City University of Hong Kong, majoring in Electronic and Communication Engineering.\nNative Language: Mandarin\nAnimal Form: Mouse\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2f5d73b305f66523f6084d93add0fbdf","permalink":"https://rycolab.io/authors/jiaoda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiaoda/","section":"authors","summary":"Jiaoda Li is an ETH AI Center Doctoral Fellow. He is part of the groups of Ryan Cotterell and Stefan Feuerriegel. He also works closely with Mrinmaya Sachan and Rico Sennrich.","tags":null,"title":"Jiaoda Li","type":"authors"},{"authors":["tiago"],"categories":null,"content":"Josef is a second-year PhD student at the University of Cambridge supervised by Simone Teufel and Ryan. Before joining Rycolab, he completed the MPhil in Advanced Computer Science at the University of Cambridge. Before that, he obtained a Bachelor of Law at the University of Exeter. He is interested in legal document information retrieval. In his spare time he likes to boulder.\nNative Language: Czech\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9d3fdfe8719fded59b20776fb11d2964","permalink":"https://rycolab.io/authors/josef/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/josef/","section":"authors","summary":"Josef is a second-year PhD student at the University of Cambridge supervised by Simone Teufel and Ryan. Before joining Rycolab, he completed the MPhil in Advanced Computer Science at the University of Cambridge.","tags":null,"title":"Josef Valvoda","type":"authors"},{"authors":["gianni"],"categories":null,"content":"Juan Luis Gastaldi (Gianni) is currently a Marie Skłodowska-Curie Fellow at the chair of History and Philosophy of Mathematical Sciences (ETH D-GESS). He is a philosopher and historian of science, specialized in philosophy and history of formal knowledge (mathematics, logic and computer science), from the beginning of the 19th century to our days. He is also the managing director of the ETH Turing Centre Zürich.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"164bfd3f5eea775c75dbd4bce7a05bff","permalink":"https://rycolab.io/authors/gianni/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/gianni/","section":"authors","summary":"Juan Luis Gastaldi (Gianni) is currently a Marie Skłodowska-Curie Fellow at the chair of History and Philosophy of Mathematical Sciences (ETH D-GESS). He is a philosopher and historian of science, specialized in philosophy and history of formal knowledge (mathematics, logic and computer science), from the beginning of the 19th century to our days.","tags":null,"title":"Juan Luis Gastaldi","type":"authors"},{"authors":["jun"],"categories":null,"content":"Jun is a video game programmer at Electric Square. They are also interested in computational linguistics and completed their MPhil in Advanced Computer Science at the University of Cambridge in June 2020. Prior to this, they studied Computer Science at the University of Southern California. In another life, Jun would like to be either a musician or a whale.\nNative Languages: English and Cantonese\nAnimal Form: Orca\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7d339d8f30d803efeaeb243a5fe6af73","permalink":"https://rycolab.io/authors/jun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jun/","section":"authors","summary":"Jun is a video game programmer at Electric Square. They are also interested in computational linguistics and completed their MPhil in Advanced Computer Science at the University of Cambridge in June 2020.","tags":null,"title":"Jun Yen Leung","type":"authors"},{"authors":["karolina"],"categories":null,"content":"Karolina is a first-year PhD student at the University of Copenhagen, co-advised by Isabelle Augenstein and Ryan. Previously, she received a BSc in Economics with a major in Statistics and Econometrics and completed an MSc in Statistics, both from the Humboldt University of Berlin. Besides, prior to starting her PhD she has worked as a data science consultant. Her primary research interests are bias and fairness in NLP, interpretability and statistical methods. In her spare time she enjoys knitting warm socks and sweaters, and learning languages, both very useful for her Ph.D.-related move to Denmark.\nNative Language: Polish\nAnimal Form: Hummingbird\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89a3e957bf68ea773109c28337cfefd1","permalink":"https://rycolab.io/authors/karolina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/karolina/","section":"authors","summary":"Karolina is a first-year PhD student at the University of Copenhagen, co-advised by Isabelle Augenstein and Ryan. Previously, she received a BSc in Economics with a major in Statistics and Econometrics and completed an MSc in Statistics, both from the Humboldt University of Berlin.","tags":null,"title":"Karolina Stańczak","type":"authors"},{"authors":["luca"],"categories":null,"content":"Luca is a second-year CS MSc. student. He received his BSc in CSEng from PoliMi, Italy. The focus of his studies are machine learning and NLP (currently, text summarization)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0274d5dfc9b30c878e9ba77604bc74cf","permalink":"https://rycolab.io/authors/luca/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/luca/","section":"authors","summary":"Luca is a second-year CS MSc. student. He received his BSc in CSEng from PoliMi, Italy. The focus of his studies are machine learning and NLP (currently, text summarization)","tags":null,"title":"Luca Malagutti","type":"authors"},{"authors":["lucas"],"categories":null,"content":"Lucas is a PhD student at MIT supervised by Yoon Kim. Previously, he completed the MPhil in Advanced Computer Science at the University of Cambridge. He wrote his MPhil thesis with Ryan on probing pre-trained neural language models for morpho-syntax. Before then he was a BSc Artificial Intelligence and Computer Science student at the University of Edinburgh.\nNative Languages: Spanish and Portuguese\nAnimal Form: Llama\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f4eeb2051886e69b374435203391df51","permalink":"https://rycolab.io/authors/lucas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lucas/","section":"authors","summary":"Lucas is a PhD student at MIT supervised by Yoon Kim. Previously, he completed the MPhil in Advanced Computer Science at the University of Cambridge. He wrote his MPhil thesis with Ryan on probing pre-trained neural language models for morpho-syntax.","tags":null,"title":"Lucas Torroba Hennigen","type":"authors"},{"authors":["mans"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bcd9a2ac5f3d13f165b3f46243c9f942","permalink":"https://rycolab.io/authors/mans/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mans/","section":"authors","summary":"","tags":null,"title":"Mans Hulden","type":"authors"},{"authors":["marinela"],"categories":null,"content":"Marinela started her PhD in October 2019 in the Language Technology Lab at the University of Cambridge under the supervision of Prof. Anna Korhonen and Ryan. She is a member of Trinity College and is funded by Trinity Overseas Bursary.\nBefore starting her PhD, Marinela obtained MSc and BSc degrees in Mathematics, both from the University of Belgrade in Serbia.\nNative Language: Serbian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7cbd738aeab27bddcfebd1c698fee92d","permalink":"https://rycolab.io/authors/marinela/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/marinela/","section":"authors","summary":"Marinela started her PhD in October 2019 in the Language Technology Lab at the University of Cambridge under the supervision of Prof. Anna Korhonen and Ryan. She is a member of Trinity College and is funded by Trinity Overseas Bursary.","tags":null,"title":"Marinela Parović","type":"authors"},{"authors":["martina"],"categories":null,"content":"Martina is writing her Master\u0026rsquo;s thesis at ETH Zürich, supervised by Clara and Ryan. Her academic interests are neural machine translation, morphology and cross-lingual learning. Also, she likes learning new languages and eating lots of chocolate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b08b9d0bebfd9788e31d72c0fa38ce6f","permalink":"https://rycolab.io/authors/martina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/martina/","section":"authors","summary":"Martina is writing her Master\u0026rsquo;s thesis at ETH Zürich, supervised by Clara and Ryan. Her academic interests are neural machine translation, morphology and cross-lingual learning. Also, she likes learning new languages and eating lots of chocolate.","tags":null,"title":"Martina Forster","type":"authors"},{"authors":["marvin"],"categories":null,"content":"Marvin is a third-year undergraduate student in Computer Science at ETH Zürich. In his free time he enjoys studying languages.\nNative Languages: German and English\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"56cfa73f08ee4a1e0b15d0ce5dbbbf59","permalink":"https://rycolab.io/authors/marvin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/marvin/","section":"authors","summary":"Marvin is a third-year undergraduate student in Computer Science at ETH Zürich. In his free time he enjoys studying languages.\nNative Languages: German and English","tags":null,"title":"Marvin Jarju","type":"authors"},{"authors":["mian"],"categories":null,"content":"Mian studies MSc Data Science at ETH and graduated from Applied Mathematics at UC Berkeley. She is very grateful for being part of DAAD RiSE, USC ISI and UCSF, where she gets to know what interdisciplinary research is like. Her current research interests are open-domain question answering, bias \u0026amp; fairness, and NLP applications with real-world data. Now She is working on her master thesis with Niklas and Shehzaad, and seeking for PhD positions. While many people loathe, she could spend many hours on reading, collecting and visualizing data. Then, if not browsing data, she is likely sitting there to listen/watch new podcasts, film trailers, bibliographies or TvN shows. Meanwhile, her mind is drifted away with making (4-song) Spotify playlists, reading\u0026amp;re-starting blogs, and taking street photos.\nAnimal Form: Octopus\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6cac6af3d7ef86ad99b6c1fca6c2e3fa","permalink":"https://rycolab.io/authors/mian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mian/","section":"authors","summary":"Mian studies MSc Data Science at ETH and graduated from Applied Mathematics at UC Berkeley. She is very grateful for being part of DAAD RiSE, USC ISI and UCSF, where she gets to know what interdisciplinary research is like.","tags":null,"title":"Mian Zhong","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rycolab.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":["niklas"],"categories":null,"content":"Niklas is a doctoral student interested in latent intensity scales. Combining methods from Natural Language Processing, Computational Social Science and Network Science, his interests are particularly centred around numbers in text, conflict intensity and sentiment analysis. After completing a MSc in Data Science at UCL, he worked at the interface of these fields at the IBM AI Core Team, Microsoft Research Cambridge and the German Federal Foreign Office in Shanghai. During his BSc in Information Management Systems at TU Berlin, Niklas spent 6 months at the University of Oxford and 12 months at Tsinghua University in Beijing funded by the German Academic Scholarship Foundation / Studienstiftung.\nNative Language: German\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3980a18dc1184eb009e3646396b5d2a3","permalink":"https://rycolab.io/authors/niklas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/niklas/","section":"authors","summary":"Niklas is a doctoral student interested in latent intensity scales. Combining methods from Natural Language Processing, Computational Social Science and Network Science, his interests are particularly centred around numbers in text, conflict intensity and sentiment analysis.","tags":null,"title":"Niklas Stoehr","type":"authors"},{"authors":["paula"],"categories":null,"content":"Paula is a third year PhD student at the University of Cambridge, where she is advised by Prof Ann Copestake and Ryan. She is supported by the Vice-Chancellor\u0026rsquo;s and Selwyn College Scholarship. Prior to starting her PhD she did the BSc in Computer Science at the University of St Andrews and completed the MPhil in Advanced Computer Science at Cambridge. During those degrees she was generously supported by the G. D. Fahrenheit Scholarship, awarded by the City Council of Gdańsk.\nHer main research interests include cross-lingual learning and computational approaches to linguistic typology and morphology. She is fascinated by how world’s languages differ in their means of encoding meaning and hopes that investigating those differences, alongside language similarities can facilitate building better, less biased NLP systems.\nNative Language: Polish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"74eae12507749d073766d39a5b14ab4e","permalink":"https://rycolab.io/authors/paula/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/paula/","section":"authors","summary":"Paula is a third year PhD student at the University of Cambridge, where she is advised by Prof Ann Copestake and Ryan. She is supported by the Vice-Chancellor\u0026rsquo;s and Selwyn College Scholarship.","tags":null,"title":"Paula Czarnowska","type":"authors"},{"authors":["philippe"],"categories":null,"content":"Philippe is an NLP research engineer at the ETH Media Technology Center (currently working on automatic post-editing of MT). He completed MSc in Computer Science at ETH.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"8bc0fd568ea288f7cbed9e16f11af304","permalink":"https://rycolab.io/authors/philippe/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/philippe/","section":"authors","summary":"Philippe is an NLP research engineer at the ETH Media Technology Center (currently working on automatic post-editing of MT). He completed MSc in Computer Science at ETH.","tags":null,"title":"Philippe Schlattner","type":"authors"},{"authors":["pouya"],"categories":null,"content":"Pouya is a Master\u0026rsquo;s student in Computer Science at ETH. He is interested in a variety of topics related to Machine Learning and NLP including Parsing, Information Extraction, and Multilingual NLP. In his free time, he likes to play Basketball, read about psychotherapy, get some REM sleep, and dream about how he can apply NLP to those areas.\nNative Language: Persian\nAnimal Form: Panthera tigris virgata\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ca209308c144c9f381a15bc865a34a02","permalink":"https://rycolab.io/authors/pouya/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/pouya/","section":"authors","summary":"Pouya is a Master\u0026rsquo;s student in Computer Science at ETH. He is interested in a variety of topics related to Machine Learning and NLP including Parsing, Information Extraction, and Multilingual NLP.","tags":null,"title":"Pouya Pourjafar","type":"authors"},{"authors":["ran"],"categories":null,"content":"Ran is a second-year PhD student at the University of Cambridge advised by Ryan. Previously, Ran completed the Computer Science Tripos at the University of Cambridge and continued on to Part III where he was also advised by Ryan. He is primarily interested in research regarding algorithms in NLP and parsing, and is picking up new interests as he goes along. While he waits for his systems to train he likes to play tennis, lacrosse and do stand-up comedy.\nNative Language: Hebrew\nAnimal Form: Honey badger\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6eba7e88ca7bbc6ce8119664e70023ed","permalink":"https://rycolab.io/authors/ran/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ran/","section":"authors","summary":"Ran is a second-year PhD student at the University of Cambridge advised by Ryan. Previously, Ran completed the Computer Science Tripos at the University of Cambridge and continued on to Part III where he was also advised by Ryan.","tags":null,"title":"Ran Zmigrod","type":"authors"},{"authors":["richard"],"categories":null,"content":"Richard Futrell is an Assistant Professor in the Department of Language Science at the University of California, Irvine. His research focuses on language processing in humans and machines.\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ed7eae801e1f7fcbe6a9d5e7cc088611","permalink":"https://rycolab.io/authors/richard/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/richard/","section":"authors","summary":"Richard Futrell is an Assistant Professor in the Department of Language Science at the University of California, Irvine. His research focuses on language processing in humans and machines.\n-- ","tags":null,"title":"Richard Futrell","type":"authors"},{"authors":["rowan"],"categories":null,"content":"Please refer to his bio on the University of Cambridge website.\nNative Language: English\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d3750635a3b8746ef003d5679b039abb","permalink":"https://rycolab.io/authors/rowan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rowan/","section":"authors","summary":"Please refer to his bio on the University of Cambridge website.\nNative Language: English","tags":null,"title":"Rowan Hall Maudslay","type":"authors"},{"authors":["ryan"],"categories":null,"content":"I was born and raised in the city of Baltimore, Maryland—the greatest city in America. But you don’t have to take my word for it, it’s spray-painted on the city’s benches: I completed my undergraduate degree at Johns Hopkins University (also in Baltimore, Maryland) in Cognitive Science (with focal areas in Linguistics and Computational Methods) under the tutelage of Colin Wilson. I was then recruited by Jason Eisner to do a Ph.D. in Computer Science at Johns Hopkins University where I was a member of the Center for Language and Speech Processing; I also took quite a few courses in mathematics, statistics and linguistics along the way. I am currently a tenure-track assistant professor at ETH Zürich in the Department of Computer Science where I am a member of the Institut für maschinelles Lernen. I was previously a Lecturer (that’s British for tenure-track assistant professor) at the Computer Laboratory (the oldest computer science department in the world) at the University of Cambridge in the United Kingdom where I am still affiliated. I have also done research stints at Google AI, Facebook AI Research and the Ludwig Maximilian University of Munich.\nI am curious about most things: Simone Teufel calls me a “research hippie”—I’ve never been quite sure what that means, though. It is my greatest dream to achieve near-native competence in an agglutinative language, but here’s hoping. Recent scholarly work that bears my name may be perused on my lab’s publication list.\nNote: I am not taking students at the moment—my lab has reached critical mass. I spend roughly 3–4 hours a week with my primary (and secondary) advisees and a similar amount of time with many of my close collaborators as well. I am fresh out of cycles 😞\nNote Note: Despite the above note, please feel free to email anyway—about anything. But, if you do email me, please address me by my first name: Ryan. I strongly disprefer academic titles.\nFamily on the internet: My uncle John Cotterell is a painter; check him out here. My cousin Jake Velker does amazing work at the One Acre Fund. My parents (Thomas Cotterell and Gina Scarinzi) are not on the internet, but here’s a recent picture of us in London, England: Animal Form: Wug\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"65d4d588cef9fb8886b394cbe1e95b85","permalink":"https://rycolab.io/authors/ryan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ryan/","section":"authors","summary":"I was born and raised in the city of Baltimore, Maryland—the greatest city in America. But you don’t have to take my word for it, it’s spray-painted on the city’s benches: I completed my undergraduate degree at Johns Hopkins University (also in Baltimore, Maryland) in Cognitive Science (with focal areas in Linguistics and Computational Methods) under the tutelage of Colin Wilson.","tags":null,"title":"Ryan Cotterell","type":"authors"},{"authors":["samuel"],"categories":null,"content":"Samuel is a Ph.D. student at Roy Wagner\u0026rsquo;s Chair for the History and Philosophy of Mathematics. He works on the history of type theory and its relation to programming errors.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"c0b58ee0cb59fefd52b1283d9f3b41db","permalink":"https://rycolab.io/authors/samuel/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/samuel/","section":"authors","summary":"Samuel is a Ph.D. student at Roy Wagner\u0026rsquo;s Chair for the History and Philosophy of Mathematics. He works on the history of type theory and its relation to programming errors.","tags":null,"title":"Samuel Hunziker","type":"authors"},{"authors":["selena"],"categories":null,"content":"Selena is a second -year MSc student in Computer Science at ETH Zurich. Previously, she received her BSc from University of Novi Sad in Serbia. Currently, she is working on a research project with Ryan on cognitively plausible morphological inflection models. Apart from that, she is interested in theoretical computer science and in her spare time enjoys playing bridge.\nNative Language: Serbian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"19bd63ab7c994c5a2aea5fd2b3cecab4","permalink":"https://rycolab.io/authors/selena/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/selena/","section":"authors","summary":"Selena is a second -year MSc student in Computer Science at ETH Zurich. Previously, she received her BSc from University of Novi Sad in Serbia. Currently, she is working on a research project with Ryan on cognitively plausible morphological inflection models.","tags":null,"title":"Selena Pepić","type":"authors"},{"authors":["shijie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"be2d1967e64e4ed029b3e9f673940c75","permalink":"https://rycolab.io/authors/shijie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shijie/","section":"authors","summary":"","tags":null,"title":"Shijie Wu","type":"authors"},{"authors":["simone"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"561fdecbc989c73616aba426ed25861c","permalink":"https://rycolab.io/authors/simone/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/simone/","section":"authors","summary":"","tags":null,"title":"Simone Teufel","type":"authors"},{"authors":["stefan"],"categories":null,"content":"Stefan holds an MPhil degree in Advanced Computer Science from the University of Cambridge and BA degrees in Mathematics and Computer Science from the American University in Bulgaria. His interests include explainability of deep learning models, neural machine translation, and natural language generation. He likes spending his free time with friends and whenever he does not have free time, he likes working with friends.\nNative Language: Bulgarian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1fee0630ebf691a0b4eb76303184b35c","permalink":"https://rycolab.io/authors/stefan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/stefan/","section":"authors","summary":"Stefan holds an MPhil degree in Advanced Computer Science from the University of Cambridge and BA degrees in Mathematics and Computer Science from the American University in Bulgaria. His interests include explainability of deep learning models, neural machine translation, and natural language generation.","tags":null,"title":"Stefan Lazov","type":"authors"},{"authors":["tiago"],"categories":null,"content":"Tiago is a third-year PhD student in Computer Science at the University of Cambridge, where he is supervised by Ryan. The supervision mostly consists of arguing with Ryan over research ideas, and Tiago insisting he is right. At the moment, Tiago is mainly interested in information theory and its application to the study of machine learning models and linguistics. To this end, he has recently been dabbling in information-theoretic linguistics and probing. Tiago enjoys the few sunny days he gets in the UK and is always looking forward to summer. Before joining Cambridge for his PhD, he did his undergraduate studies in Automation and Control Engineering at the University of Brasilia\u0026mdash;a very dry city with a lovely sky\u0026mdash;and a masters in computer science at the Federal University of Minas Gerais, located in Belo Horizonte, the city with (arguably) the best food and (potentially) the most friendly people in the world. (Tiago himself is not from Belo Horizonte, though, so he is not that friendly). Tiago takes great pleasure in correcting Ryan’s grammatical mistakes when Ryan speaks to him in Portuguese.\nNative Language: Portuguese\nAnimal Form: Sea turtle\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fdbf9f3b62c6aa81ec87c8656829ba69","permalink":"https://rycolab.io/authors/tiago/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tiago/","section":"authors","summary":"Tiago is a third-year PhD student in Computer Science at the University of Cambridge, where he is supervised by Ryan. The supervision mostly consists of arguing with Ryan over research ideas, and Tiago insisting he is right.","tags":null,"title":"Tiago Pimentel","type":"authors"},{"authors":["tianyu"],"categories":null,"content":"Tianyu is a first-year PhD student at ETH Zurich. He is advised by Ryan and Mrinmaya Sachan. He received his BSc in computer science from Peking University. He is currently interested in structured prediction, parsing, and natural language generation.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"24aa23770a64f329ed6e3676915264ca","permalink":"https://rycolab.io/authors/tianyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianyu/","section":"authors","summary":"Tianyu is a first-year PhD student at ETH Zurich. He is advised by Ryan and Mrinmaya Sachan. He received his BSc in computer science from Peking University. He is currently interested in structured prediction, parsing, and natural language generation.","tags":null,"title":"Tianyu Liu","type":"authors"},{"authors":["tim"],"categories":null,"content":"Tim develops machine learning algorithms for tough problems—tending toward applications in natural language processing and programming languages. When he\u0026rsquo;s not in front of a whiteboard or computer, he\u0026rsquo;s probably climbing things, walking around on his hands, or hanging out with Hanna Wallach and @maia.the.pomsky.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"07d172c41c903101879fc5085cda4837","permalink":"https://rycolab.io/authors/tim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tim/","section":"authors","summary":"Tim develops machine learning algorithms for tough problems—tending toward applications in natural language processing and programming languages. When he\u0026rsquo;s not in front of a whiteboard or computer, he\u0026rsquo;s probably climbing things, walking around on his hands, or hanging out with Hanna Wallach and @maia.","tags":null,"title":"Tim Vieira","type":"authors"},{"authors":["vesteinn"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7223a1d37b0c7cad06358ef2294ac4a9","permalink":"https://rycolab.io/authors/vesteinn/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/vesteinn/","section":"authors","summary":"","tags":null,"title":"Vésteinn Snæbjarnarson","type":"authors"},{"authors":["wangchunshu"],"categories":null,"content":"Chunshu is a first-year Ph.D. in the Institute of Machine Learning at ETH Zürich, where he is supervised by Ryan Cotterell and Mrinmaya Sachan. He obtained his undergraduate and master’s degree at Beihang University, supervised by Ke Xu. He spent one year as a Predoctoral Young Investigator at AI2, working with Ronan Le Bras and Yejin Choi. Previously, he interned at Microsoft Research Asia and Bytedance AI Lab, and also spent some time working with Xiang Ren at USC and with Jian Tang at MILA. Currently, his research is focused on (1) efficient method for NLP, (2) natural language generation, and (3) trustworthy NLP\nAnimal Form: Dolphin\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"777d45be7dccbe316e94ecc533ec90cf","permalink":"https://rycolab.io/authors/wangchunshu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/wangchunshu/","section":"authors","summary":"Chunshu is a first-year Ph.D. in the Institute of Machine Learning at ETH Zürich, where he is supervised by Ryan Cotterell and Mrinmaya Sachan. He obtained his undergraduate and master’s degree at Beihang University, supervised by Ke Xu.","tags":null,"title":"Wangchunshu Zhou","type":"authors"},{"authors":["martina"],"categories":null,"content":"Eleanor is a second-year Ph.D. in the Institute of Machine Learning at ETH Zürich, where she is supervised by Ryan Cotterell and Mrinmaya Sachan. She also obtained her Master’s in Computer Science at ETH Zürich. Previously, she did her undergraduate studies at Zhejiang University and also spent some time at UCLA, supervised by Kai-Wei Chang. Her research is focused on generating and understanding long texts. To this end, she has recently been dabbling into document-level machine translation, coreference resolution, and coherence modeling. She also likes fun applications of natural language generation (news generation, lyrics generation, etc.). ​In her spare time, Eleanor enjoys learning languages, skiing and hiking. She also plays the piano. It is her greatest dream to travel to all 195 countries in the world.\nNative Language: Mandarin\nAnimal Form: Penguin\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"52d7ad1914cb73f076ef2e373686b91d","permalink":"https://rycolab.io/authors/yuchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuchen/","section":"authors","summary":"Eleanor is a second-year Ph.D. in the Institute of Machine Learning at ETH Zürich, where she is supervised by Ryan Cotterell and Mrinmaya Sachan. She also obtained her Master’s in Computer Science at ETH Zürich.","tags":null,"title":"Yuchen Jiang","type":"authors"},{"authors":["yufei"],"categories":null,"content":"Yufei is a third-year undergraduate student in the École Polytechnique, France, double-majoring in mathematics and computer science. She is supervised by Ryan for the ETH Zürich Summer Student Research Fellowship. She is currently working on removing gender bias from contextual word representations.\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"32f3ce7af08bb902238d53f01fad01cd","permalink":"https://rycolab.io/authors/yufei/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yufei/","section":"authors","summary":"Yufei is a third-year undergraduate student in the École Polytechnique, France, double-majoring in mathematics and computer science. She is supervised by Ryan for the ETH Zürich Summer Student Research Fellowship. She is currently working on removing gender bias from contextual word representations.","tags":null,"title":"Yufei Liu","type":"authors"},{"authors":["zhijing"],"categories":null,"content":"Zhijing Jin (she/her) is a Ph.D. at Max Planck Institute \u0026amp; ETH Zürich. Her research goals are two-fold: (1) to expand the impact of NLP by promoting NLP for social good, and (2) to improve NLP models by connecting NLP with causal inference. She is also co-supervised by Prof Bernhard Schoelkopf. Previously, she was a research intern at Amazon AI (2019-2020) supervised by Prof Zheng Zhang, David Wipf, and Prof Xipeng Qiu. She has published at many NLP and AI venues (e.g., AAAI, ACL, EMNLP, NAACL, COLING, AISTATS), and NLP for healthcare venues (e.g., AAHPM, JPSM). Her work has been cited in MIT News, ACM TechNews, WeVolver, VentureBeat, and Synced. She is actively involved in AI for social good, as the organizer of NLP for Positive Impact Workshop at ACL 2021, and RobustML workshop at ICLR 2021. To support the NLP research community, she organizes the ACL Year-Round Mentorship Program. To foster the causality research community, she is the Publications Chair for the 1st conference on Causal Learning and Reasoning (CLeaR).\nNative language: Mandarin Chinese, Shanghainese\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"e1535db07c72c6729cb46dc38e37c50c","permalink":"https://rycolab.io/authors/zhijing/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/zhijing/","section":"authors","summary":"Zhijing Jin (she/her) is a Ph.D. at Max Planck Institute \u0026amp; ETH Zürich. Her research goals are two-fold: (1) to expand the impact of NLP by promoting NLP for social good, and (2) to improve NLP models by connecting NLP with causal inference.","tags":null,"title":"Zhijing Jin","type":"authors"},{"authors":["Tuhin Chakrabarty"],"categories":null,"content":" Bio Tuhin Chakrabarty is a PhD student in Computer Science at Columbia University. Within the department he is a part of the Natural Language Processing group , where he is advised by Smaranda Muresan. His research is supported by the Columbia Center of Artificial Intelligence \u0026amp; Technology (CAIT) \u0026amp; Amazon Science Ph.D. Fellowship. His research interests are broadly in Natural Language Processing and Machine Learning, with special focus in Language Generation. His overarching research question centers around how we can control large language models to understand , interpret or generate creative text.Recently he is also working in Continual Learning of Large language models. https://tuhinjubcse.github.io/\n","date":1659526200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1659526200,"objectID":"d81f659cdda526ee2fc3ad5c91f63fa0","permalink":"https://rycolab.io/talk/chakrabarty-aug-22/","publishdate":"2022-07-29T00:16:50+02:00","relpermalink":"/talk/chakrabarty-aug-22/","section":"talk","summary":"Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions and that models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models, even though impressive, still perform poorly on a wide range of tasks outside of their respective training and evaluation sets.","tags":[],"title":"Continual-T0: Fine-tuned Language Models are Continual Learners","type":"talk"},{"authors":["David Mortensen"],"categories":null,"content":" Bio David Mortensen is a computational linguist interested in phonology, morphology, language change, linguistic typology, and human-in-the-loop computation. He is currently a Systems Scientist (a non-tenure track research faculty member at the Assistant Professor level) in the Language Technologies Institute, which is part of Carnegie Mellon University’s School of Computer Science. Before coming to CMU, he was an Assistant Professor in the Department of Linguistics at the University of Pittsburgh.\n","date":1658230200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1658230200,"objectID":"e2b495f7c1f6cff27e0cabf44aa8bde2","permalink":"https://rycolab.io/talk/mortensen-jul-19-22/","publishdate":"2022-07-19T23:46:48+02:00","relpermalink":"/talk/mortensen-jul-19-22/","section":"talk","summary":"There is wide debate about the degree to which the properties of human cognition affect how languages are structured and how they change over time. This controversy extends to the lexicon. We show that the decline of lexical items can be partially accounted for by biases that have been demonstrated in the cognitive science literature. ","tags":[],"title":"How Cognitive Biases Do (and Do Not) Shape Lexicons: Two Computational Studies","type":"talk"},{"authors":["Noga Zaslavsky"],"categories":null,"content":" Bio Noga Zaslavsky is a postdoc at MIT. Her research aims to understand language, learning, and reasoning from first principles, building on ideas and methods from machine learning and information theory.\n","date":1656936000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656936000,"objectID":"b5c0197ec506185743d9a146420704f4","permalink":"https://rycolab.io/talk/zaslavsky-jul-4-22/","publishdate":"2022-07-19T23:46:48+02:00","relpermalink":"/talk/zaslavsky-jul-4-22/","section":"talk","summary":"Our world is extremely complex, and yet we are able to exchange our thoughts and beliefs about it using a relatively small number of words. What computational principles can explain this extraordinary ability?","tags":[],"title":"Losing bits and finding meaning: Efficient compression shapes meaning in language","type":"talk"},{"authors":["Tianyu Liu","Yuchen Eleanor Jiang","Ryan Cotterell","Mrinmaya Sachan"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"eb842e2e80b4b6fad1ce09bac8158318","permalink":"https://rycolab.io/publication/liual-naacl-22/","publishdate":"2022-05-23T14:37:54.232763Z","relpermalink":"/publication/liual-naacl-22/","section":"publication","summary":"","tags":null,"title":"A Structured Span Selector","type":"publication"},{"authors":["Zeerak Talat","Hagen Blix","Josef Valvoda","Maya Indira Ganesh","Ryan Cotterell","Adina Williams"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"b8155b530bd7681a2c466b8463a4440f","permalink":"https://rycolab.io/publication/talatal-naacl-22/","publishdate":"2022-05-23T14:37:55.39086Z","relpermalink":"/publication/talatal-naacl-22/","section":"publication","summary":"","tags":null,"title":"A Word on Machine Ethics: A Response to Jiang et al. (2021)","type":"publication"},{"authors":["Yuchen Eleanor Jiang","Tianyu Liu","Shuming Ma","Dongdong Zhang","Jian Yang","Haoyang Huang","Rico Sennrich","Ryan Cotterell","Mrinmaya Sachan","Ming Zhou"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"e2287db463ef53d795fdd6d6bfefb874","permalink":"https://rycolab.io/publication/jiangal-naacl-22/","publishdate":"2022-05-23T14:37:54.883375Z","relpermalink":"/publication/jiangal-naacl-22/","section":"publication","summary":"","tags":null,"title":"BlonDe: An Automatic Evaluation Metric for Document-level Machine Translation","type":"publication"},{"authors":["Jiaoda Li","Ryan Cotterell","Mrinmaya Sachan"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"f2332cd84dad7d54ff6145b247fcf423","permalink":"https://rycolab.io/publication/lial-naacl-22/","publishdate":"2022-05-23T14:37:55.223503Z","relpermalink":"/publication/lial-naacl-22/","section":"publication","summary":"","tags":null,"title":"Probing via Prompting","type":"publication"},{"authors":["Karolina Stańczak","Edoardo Ponti","Lucas Torroba Hennigen","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1656633600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1656633600,"objectID":"ecaee66061825591c4310b2e456508d2","permalink":"https://rycolab.io/publication/stanczakal-naacl-22/","publishdate":"2022-05-23T14:37:54.674825Z","relpermalink":"/publication/stanczakal-naacl-22/","section":"publication","summary":"","tags":null,"title":"Same Neurons, Different Languages: Probing Morphosyntax in Multilingual Pre-trained Models","type":"publication"},{"authors":["Aaron Schein"],"categories":null,"content":" Bio Aaron Schein is a postdoc at Columbia University and will start his professorship at the University of Chicago in fall. He develops statistical models to analyse large-scale data in political science, economics, and genetics, among other fields.\n","date":1654869600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1654869600,"objectID":"777f579ae83f140242f77c68c2a622fb","permalink":"https://rycolab.io/talk/schein-jun-10-22/","publishdate":"2022-07-19T23:46:48+02:00","relpermalink":"/talk/schein-jun-10-22/","section":"talk","summary":"Aaron Schein is a postdoc at Columbia University and will start his professorship at the University of Chicago in fall. He develops statistical models to analyse large-scale data in political science, economics, and genetics, among other fields.","tags":[],"title":"Talk by Aaron","type":"talk"},{"authors":["Clara Meister","Tiago Pimentel","Thomas Hikaru Clark","Ryan Cotterell","Roger P. Levy"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"d4a47aa546801b1900049bdbba27569c","permalink":"https://rycolab.io/publication/meisteral-acl-22-b/","publishdate":"2022-05-23T14:37:53.504503Z","relpermalink":"/publication/meisteral-acl-22-b/","section":"publication","summary":"","tags":null,"title":"Analyzing Wrap-Up Effects through an Information-Theoretic Lens","type":"publication"},{"authors":["Karim Lasri","Tiago Pimentel","Alessandro Lenci","Thierry Poibeau","Ryan Cotterell"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"e117f7367d44d174ecb858dd249d98cb","permalink":"https://rycolab.io/publication/lasrial-acl-22/","publishdate":"2022-05-23T14:37:54.067574Z","relpermalink":"/publication/lasrial-acl-22/","section":"publication","summary":"","tags":null,"title":"Causal Probing for Grammatical Number: From Encoding to Usage","type":"publication"},{"authors":["Aryaman Arora","Clara Isabel Meister","Ryan Cotterell"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"acee213decb79795ab9d2e135dbc8655","permalink":"https://rycolab.io/publication/aroraal-acl-22/","publishdate":"2022-05-23T14:37:53.901302Z","relpermalink":"/publication/aroraal-acl-22/","section":"publication","summary":"","tags":null,"title":"Estimating the Entropy of Linguistic Distributions","type":"publication"},{"authors":["Clara Isabel Meister","Gian Wiher","Tiago Pimentel","Ryan Cotterell"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"ba3944c5206a4072eb9370c945ec44ee","permalink":"https://rycolab.io/publication/meisteral-acl-22-a/","publishdate":"2022-05-23T14:37:53.315448Z","relpermalink":"/publication/meisteral-acl-22-a/","section":"publication","summary":"","tags":null,"title":"On the probability–quality paradox in language generation generation","type":"publication"},{"authors":["Alexander Immer","Lucas Torroba Hennigen","Vincent Fortuin","Ryan Cotterell"],"categories":null,"content":"","date":1651363200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651363200,"objectID":"cc1283c731525434a2bf5d5dfebe761b","permalink":"https://rycolab.io/publication/immeral-acl-22/","publishdate":"2022-05-23T14:37:53.724158Z","relpermalink":"/publication/immeral-acl-22/","section":"publication","summary":"","tags":null,"title":"Probing as Quantifying the Inductive Bias of Pre-trained Representations","type":"publication"},{"authors":["Shauli Ravfogel","Michael Twiton","Yoav Goldberg","Ryan Cotterell"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"1aa500ff5a6f4b378b7677d0a5f41eb0","permalink":"https://rycolab.io/publication/ravfogelal-icml-22/","publishdate":"2022-05-23T14:37:55.597443Z","relpermalink":"/publication/ravfogelal-icml-22/","section":"publication","summary":"","tags":null,"title":"Linear Relaxed Adversarial Concept Erasure","type":"publication"},{"authors":["Afra Amini","Tiago Pimentel","Clara Meister","Ryan Cotterell"],"categories":null,"content":"","date":1640995200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1640995200,"objectID":"1144d4708c8baf1d9ecceb86ae209e7f","permalink":"https://rycolab.io/publication/aminial-tacl-22/","publishdate":"2022-05-23T14:37:54.411418Z","relpermalink":"/publication/aminial-tacl-22/","section":"publication","summary":"","tags":null,"title":"Naturalistic Causal Probing for Morpho-Syntax","type":"publication"},{"authors":["Zeerak Talat$^*$","Hagen Blix$^*$","Josef Valvoda","Maya Indira Ganesh","Ryan Cotterell","Adina Williams"],"categories":null,"content":"","date":1636156800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636156800,"objectID":"8f95a5a044a8db45399f3fc462e193f8","permalink":"https://rycolab.io/publication/talatal-local-21/","publishdate":"2021-11-06T14:17:51.046327Z","relpermalink":"/publication/talatal-local-21/","section":"publication","summary":"Ethics is one of the longest standing intellectual endeavors of humanity. In recent years, the fields of AI and NLP have attempted to wrangle with how learning systems that interact with humans should be constrained to behave ethically. One proposal in this vein is the construction of morality models that can take in arbitrary text and output a moral judgment about the situation described. In this work, we focus on a single case study of the recently proposed Delphi model and offer a critique of the project’s proposed method of automating morality judgments. Through an analysis of Delphi, we examine broader issues that would be applicable to any similar attempt. We conclude with a discussion of how machine ethics could usefully proceed, by focusing on current and near-future uses of technology, in a way that centers around transparency, democratic values, and allows for straightforward accountability.","tags":null,"title":"A Word on Machine Ethics: A Response to Jiang et al. (2021)","type":"publication"},{"authors":["Tiago Pimentel","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"479d27b86e873c02ae67666bc5bb6ad4","permalink":"https://rycolab.io/emnlp21/pimentelcotterell-emnlp-21/","publishdate":"2021-10-08T07:56:32.687045Z","relpermalink":"/emnlp21/pimentelcotterell-emnlp-21/","section":"emnlp21","summary":"Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents -- allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.","tags":null,"title":"A Bayesian Framework for Information-Theoretic Probing","type":"emnlp21"},{"authors":["Tiago Pimentel","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"f6a55062f00899750cd965563afa6509","permalink":"https://rycolab.io/publication/pimentelcotterell-emnlp-21/","publishdate":"2021-10-08T07:56:32.687045Z","relpermalink":"/publication/pimentelcotterell-emnlp-21/","section":"publication","summary":"Pimentel et al. (2020) recently analysed probing from an information-theoretic perspective. They argue that probing should be seen as approximating a mutual information. This led to the rather unintuitive conclusion that representations encode exactly the same information about a target task as the original sentences. The mutual information, however, assumes the true probability distribution of a pair of random variables is known, leading to unintuitive results in settings where it is not. This paper proposes a new framework to measure what we term Bayesian mutual information, which analyses information from the perspective of Bayesian agents -- allowing for more intuitive findings in scenarios with finite data. For instance, under Bayesian MI we have that data can add information, processing can help, and information can hurt, which makes it more intuitive for machine learning applications. Finally, we apply our framework to probing where we believe Bayesian mutual information naturally operationalises ease of extraction by explicitly limiting the available background knowledge to solve a task.","tags":null,"title":"A Bayesian Framework for Information-Theoretic Probing","type":"publication"},{"authors":["Damian Pascual","Beni Egressy","Clara Meister","Ryan Cotterell","Roger Wattenhofer"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"ce746d39e90b836a5d22cf20db08abd5","permalink":"https://rycolab.io/emnlp21/pascualal-emnlp-21/","publishdate":"2021-10-08T07:56:33.802722Z","relpermalink":"/emnlp21/pascualal-emnlp-21/","section":"emnlp21","summary":"Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.","tags":null,"title":"A Plug-and-Play Method for Controlled Text Generation","type":"emnlp21"},{"authors":["Damian Pascual","Beni Egressy","Clara Meister","Ryan Cotterell","Roger Wattenhofer"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fd0ba2dbbd1cb1c255b6ca5407598f13","permalink":"https://rycolab.io/publication/pascualal-emnlp-21/","publishdate":"2021-10-08T07:56:33.802722Z","relpermalink":"/publication/pascualal-emnlp-21/","section":"publication","summary":"Large pre-trained language models have repeatedly shown their ability to produce fluent text. Yet even when starting from a prompt, generation can continue in many plausible directions. Current decoding methods with the goal of controlling generation, e.g., to ensure specific words are included, either require additional models or fine-tuning, or work poorly when the task at hand is semantically unconstrained, e.g., story generation. In this work, we present a plug-and-play decoding method for controlled language generation that is so simple and intuitive, it can be described in a single sentence: given a topic or keyword, we add a shift to the probability distribution over our vocabulary towards semantically similar words. We show how annealing this distribution can be used to impose hard constraints on language generation, something no other plug-and-play method is currently able to do with SOTA language generators. Despite the simplicity of this approach, we see it works incredibly well in practice: decoding from GPT-2 leads to diverse and fluent sentences while guaranteeing the appearance of given guide words. We perform two user studies, revealing that (1) our method outperforms competing methods in human evaluations; and (2) forcing the guide words to appear in the generated text has no impact on the fluency of the generated text.","tags":null,"title":"A Plug-and-Play Method for Controlled Text Generation","type":"publication"},{"authors":["Tiago Pimentel","Clara Meister","Elizabeth Salesky","Simone Teufel","Damián Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"afa54473bcc2caa6c10c931c60a30bd2","permalink":"https://rycolab.io/emnlp21/pimentelal-emnlp-21-a/","publishdate":"2021-10-08T07:56:33.303646Z","relpermalink":"/emnlp21/pimentelal-emnlp-21-a/","section":"emnlp21","summary":"While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal--duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal--duration trade-off in operation, both across and within the world's languages.","tags":null,"title":"A surprisal--duration trade-off across and within the world's languages","type":"emnlp21"},{"authors":["Tiago Pimentel","Clara Meister","Elizabeth Salesky","Simone Teufel","Damián Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"0cbed81dcb7da785c5fb9d75fa491e3b","permalink":"https://rycolab.io/publication/pimentelal-emnlp-21-a/","publishdate":"2021-10-08T07:56:33.303646Z","relpermalink":"/publication/pimentelal-emnlp-21-a/","section":"publication","summary":"While there exist scores of natural languages, each with its unique features and idiosyncrasies, they all share a unifying theme: enabling human communication. We may thus reasonably predict that human cognition shapes how these languages evolve and are used. Assuming that the capacity to process information is roughly constant across human populations, we expect a surprisal--duration trade-off to arise both across and within languages. We analyse this trade-off using a corpus of 600 languages and, after controlling for several potential confounds, we find strong supporting evidence in both settings. Specifically, we find that, on average, phones are produced faster in languages where they are less surprising, and vice versa. Further, we confirm that more surprising phones are longer, on average, in 319 languages out of the 600. We thus conclude that there is strong evidence of a surprisal--duration trade-off in operation, both across and within the world's languages.","tags":null,"title":"A surprisal--duration trade-off across and within the world's languages","type":"publication"},{"authors":["Niklas Stoehr","Lucas Torroba Hennigen","Samin Ahbab","Robert West","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"af8cb472fb23d34debca785461ad7618","permalink":"https://rycolab.io/emnlp21/stoehral-emnlp-21-a/","publishdate":"2021-10-08T07:56:32.537611Z","relpermalink":"/emnlp21/stoehral-emnlp-21-a/","section":"emnlp21","summary":"Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies.","tags":null,"title":"Classifying Dyads for Militarized Conflict Analysis","type":"emnlp21"},{"authors":["Niklas Stoehr","Lucas Torroba Hennigen","Samin Ahbab","Robert West","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"0e5634a3b5257a5e160254873d2874d3","permalink":"https://rycolab.io/publication/stoehral-emnlp-21-a/","publishdate":"2021-10-08T07:56:32.537611Z","relpermalink":"/publication/stoehral-emnlp-21-a/","section":"publication","summary":"Understanding the origins of militarized conflict is a complex, yet important undertaking. Existing research seeks to build this understanding by considering bi-lateral relationships between entity pairs (dyadic causes) and multi-lateral relationships among multiple entities (systemic causes). The aim of this work is to compare these two causes in terms of how they correlate with conflict between two entities. We do this by devising a set of textual and graph-based features which represent each of the causes. The features are extracted from Wikipedia and modeled as a large graph. Nodes in this graph represent entities connected by labeled edges representing ally or enemy-relationships. This allows casting the problem as an edge classification task, which we term dyad classification. We propose and evaluate classifiers to determine if a particular pair of entities are allies or enemies. Our results suggest that our systemic features might be slightly better correlates of conflict. Further, we find that Wikipedia articles of allies are semantically more similar than enemies.","tags":null,"title":"Classifying Dyads for Militarized Conflict Analysis","type":"publication"},{"authors":["Clara Meister","Afra Amini","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fed5f9462fb373890e86362d23dde319","permalink":"https://rycolab.io/emnlp21/meisteral-emnlp-21-b/","publishdate":"2021-10-08T07:56:32.842369Z","relpermalink":"/emnlp21/meisteral-emnlp-21-b/","section":"emnlp21","summary":"Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et. al. 2019's stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.","tags":null,"title":"Conditional Poisson Stochastic Beam Search","type":"emnlp21"},{"authors":["Clara Meister","Afra Amini","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"7820403512dddfef6a03747644239b2a","permalink":"https://rycolab.io/publication/meisteral-emnlp-21-b/","publishdate":"2021-10-08T07:56:32.842369Z","relpermalink":"/publication/meisteral-emnlp-21-b/","section":"publication","summary":"Beam search is the default decoding strategy for many sequence generation tasks in NLP. The set of approximate K-best items returned by the algorithm is a useful summary of the distribution for many applications; however, the candidates typically exhibit high overlap and may give a highly biased estimate for expectations under our model. These problems can be addressed by instead using stochastic decoding strategies. In this work, we propose a new method for turning beam search into a stochastic process: Conditional Poisson stochastic beam search. Rather than taking the maximizing set at each iteration, we sample K candidates without replacement according to the conditional Poisson sampling design. We view this as a more natural alternative to Kool et. al. 2019's stochastic beam search (SBS). Furthermore, we show how samples generated under the CPSBS design can be used to build consistent estimators and sample diverse sets from sequence models. In our experiments, we observe CPSBS produces lower variance and more efficient estimators than SBS, even showing improvements in high entropy settings.","tags":null,"title":"Conditional Poisson Stochastic Beam Search","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fb6b8389daf62efc56af42fce7d915f5","permalink":"https://rycolab.io/emnlp21/zmigrodal-emnlp-21/","publishdate":"2021-10-08T07:56:33.451426Z","relpermalink":"/emnlp21/zmigrodal-emnlp-21/","section":"emnlp21","summary":"Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no sampling algorithm has been presented in the literature to account for this additional constraint. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a graph subject to the root constraint. Wilson (1996)'s sampling algorithm has a running time of O(H) where H is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm has a running time of O(N^3), which is often greater than the mean hitting time of a directed graph. Additionally, we build upon Colbourn's algorithm and present a novel extension that can sample K trees without replacement in O(K N^3 + K^2 N) time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph.","tags":null,"title":"Efficient Sampling of Dependency Structure","type":"emnlp21"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"f702d57dd5fd3519dcb9aeca11d1b269","permalink":"https://rycolab.io/publication/zmigrodal-emnlp-21/","publishdate":"2021-10-08T07:56:33.451426Z","relpermalink":"/publication/zmigrodal-emnlp-21/","section":"publication","summary":"Probabilistic distributions over spanning trees in directed graphs are a fundamental model of dependency structure in natural language processing, syntactic dependency trees. In NLP, dependency trees often have an additional root constraint: only one edge may emanate from the root. However, no sampling algorithm has been presented in the literature to account for this additional constraint. In this paper, we adapt two spanning tree sampling algorithms to faithfully sample dependency trees from a graph subject to the root constraint. Wilson (1996)'s sampling algorithm has a running time of O(H) where H is the mean hitting time of the graph. Colbourn (1996)'s sampling algorithm has a running time of O(N^3), which is often greater than the mean hitting time of a directed graph. Additionally, we build upon Colbourn's algorithm and present a novel extension that can sample K trees without replacement in O(K N^3 + K^2 N) time. To the best of our knowledge, no algorithm has been given for sampling spanning trees without replacement from a directed graph.","tags":null,"title":"Efficient Sampling of Dependency Structure","type":"publication"},{"authors":["Tiago Pimentel","Clara Meister","Simone Teufel","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"9afa98fb3de5a92d0ac02c3d3aaa4621","permalink":"https://rycolab.io/emnlp21/pimentelal-emnlp-21-b/","publishdate":"2021-10-08T07:56:32.997021Z","relpermalink":"/emnlp21/pimentelal-emnlp-21-b/","section":"emnlp21","summary":"Homophony's widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time; e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen (2020), who posit that good wordforms are more often homophonous simply because they are more phonotactically probable. In this paper, we join in on the debate. We first propose a new information-theoretic quantification of a language's homophony: the sample Rényi entropy. Then, we use this quantification to revisit Trott and Bergen's claims. While their point is theoretically sound, a specific methodological issue in their experiments raises doubts about their results. After addressing this issue, we find no clear pressure either towards or against homophony -- a much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's findings.","tags":null,"title":"On Homophony and Rényi Entropy","type":"emnlp21"},{"authors":["Tiago Pimentel","Clara Meister","Simone Teufel","Ryan Cotterell"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"fe0d7f9fdc7d8fc35a396de8a6fc232c","permalink":"https://rycolab.io/publication/pimentelal-emnlp-21-b/","publishdate":"2021-10-08T07:56:32.997021Z","relpermalink":"/publication/pimentelal-emnlp-21-b/","section":"publication","summary":"Homophony's widespread presence in natural languages is a controversial topic. Recent theories of language optimality have tried to justify its prevalence, despite its negative effects on cognitive processing time; e.g., Piantadosi et al. (2012) argued homophony enables the reuse of efficient wordforms and is thus beneficial for languages. This hypothesis has recently been challenged by Trott and Bergen (2020), who posit that good wordforms are more often homophonous simply because they are more phonotactically probable. In this paper, we join in on the debate. We first propose a new information-theoretic quantification of a language's homophony: the sample Rényi entropy. Then, we use this quantification to revisit Trott and Bergen's claims. While their point is theoretically sound, a specific methodological issue in their experiments raises doubts about their results. After addressing this issue, we find no clear pressure either towards or against homophony -- a much more nuanced result than either Piantadosi et al.'s or Trott and Bergen's findings.","tags":null,"title":"On Homophony and Rényi Entropy","type":"publication"},{"authors":["Clara Meister","Tiago Pimentel","Patrick Haller","Lena Jäger","Ryan Cotterell","Roger Levy"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"dd1ebbc4204a38377dbb1ac4707f3eae","permalink":"https://rycolab.io/emnlp21/meisteral-emnlp-21-a/","publishdate":"2021-10-08T07:56:33.149254Z","relpermalink":"/emnlp21/meisteral-emnlp-21-a/","section":"emnlp21","summary":"The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal -- or lack thereof -- should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID's predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document -- a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.","tags":null,"title":"Revisiting the Uniform Information Density Hypothesis","type":"emnlp21"},{"authors":["Clara Meister","Tiago Pimentel","Patrick Haller","Lena Jäger","Ryan Cotterell","Roger Levy"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"cbdbaa011ed25665186348089c920601","permalink":"https://rycolab.io/publication/meisteral-emnlp-21-a/","publishdate":"2021-10-08T07:56:33.149254Z","relpermalink":"/publication/meisteral-emnlp-21-a/","section":"publication","summary":"The uniform information density (UID) hypothesis posits a preference among language users for utterances structured such that information is distributed uniformly across a signal. While its implications on language production have been well explored, the hypothesis potentially makes predictions about language comprehension and linguistic acceptability as well. Further, it is unclear how uniformity in a linguistic signal -- or lack thereof -- should be measured, and over which linguistic unit, e.g., the sentence or language level, this uniformity should hold. Here we investigate these facets of the UID hypothesis using reading time and acceptability data. While our reading time results are generally consistent with previous work, they are also consistent with a weakly super-linear effect of surprisal, which would be compatible with UID's predictions. For acceptability judgments, we find clearer evidence that non-uniformity in information density is predictive of lower acceptability. We then explore multiple operationalizations of UID, motivated by different interpretations of the original hypothesis, and analyze the scope over which the pressure towards uniformity is exerted. The explanatory power of a subset of the proposed operationalizations suggests that the strongest trend may be a regression towards a mean surprisal across the language, rather than the phrase, sentence, or document -- a finding that supports a typical interpretation of UID, namely that it is the byproduct of language users maximizing the use of a (hypothetical) communication channel.","tags":null,"title":"Revisiting the Uniform Information Density Hypothesis","type":"publication"},{"authors":["Tim Vieira","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"14986e40efe72717e8149162d1beece2","permalink":"https://rycolab.io/emnlp21/vieiraal-emnlp-21/","publishdate":"2021-10-08T07:56:33.962958Z","relpermalink":"/emnlp21/vieiraal-emnlp-21/","section":"emnlp21","summary":"Computational models of human language often involve combinatorial problems. For instance, a probabilistic parser may marginalize over exponentially many trees to make predictions.  Algorithms for such problems often employ dynamic programming and are not always unique. Finding one with optimal asymptotic runtime can be unintuitive, time consuming, and error-prone. Our work aims to automate this laborious process.  Given an initial correct declarative program, we search for a sequence of semantics-preserving transformations to improve its running time as much as possible. To this end, we describe a set of program transformations, a simple metric for assessing the efficiency of a transformed program, and a heuristic search procedure to improve this metric. We show that in practice, automated search---like the mental search performed by human programmers---can find substantial improvements to the initial program. Empirically, we show that many speed-ups described in the NLP literature could have been discovered automatically by our system.","tags":null,"title":"Searching for More Efficient Dynamic Programs","type":"emnlp21"},{"authors":["Tim Vieira","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1635724800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1635724800,"objectID":"5d9d6b71ef6bfbb3800200b582025f9b","permalink":"https://rycolab.io/publication/vieiraal-emnlp-21/","publishdate":"2021-10-08T07:56:33.962958Z","relpermalink":"/publication/vieiraal-emnlp-21/","section":"publication","summary":"Computational models of human language often involve combinatorial problems. For instance, a probabilistic parser may marginalize over exponentially many trees to make predictions.  Algorithms for such problems often employ dynamic programming and are not always unique. Finding one with optimal asymptotic runtime can be unintuitive, time consuming, and error-prone. Our work aims to automate this laborious process.  Given an initial correct declarative program, we search for a sequence of semantics-preserving transformations to improve its running time as much as possible. To this end, we describe a set of program transformations, a simple metric for assessing the efficiency of a transformed program, and a heuristic search procedure to improve this metric. We show that in practice, automated search---like the mental search performed by human programmers---can find substantial improvements to the initial program. Empirically, we show that many speed-ups described in the NLP literature could have been discovered automatically by our system.","tags":null,"title":"Searching for More Efficient Dynamic Programs","type":"publication"},{"authors":["Jason Wei","Clara Meister","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"292e718bc257aa4df3d0f87d572098e5","permalink":"https://rycolab.io/publication/weial-acl-21/","publishdate":"2021-10-08T07:56:31.656032Z","relpermalink":"/publication/weial-acl-21/","section":"publication","summary":"The uniform information density (UID) hypothesis, which posits that speakers prefer utterances that distribute information uniformly across the signal, has gained substantial traction in psycholinguistics as an explanation for certain syntactic, morphological, and prosodic choices. Could we operationalize uniform information density as an inductive bias for statistical language modeling? In this paper, we augment the canonical MLE objective for training language models by encoding UID as regularization. In experiments on ten languages spanning five language families, we find that using UID regularization consistently improves perplexity in language models, having a larger effect when training data is limited. Moreover, via analysis of generated sequences, we find that UID-regularized language models are higher-entropy and produce text that is longer and more lexically diverse. Our results not only suggest that UID is a reasonable inductive bias for language modeling, but also provide an alternative validation of the UID hypothesis using modern-day NLP tools.","tags":null,"title":"A cognitive regularizer for language modeling","type":"publication"},{"authors":["Clara Meister","Martina Forster","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"db5df325a5307e8a413dd6099f75a0ff","permalink":"https://rycolab.io/publication/meisteral-acl-21-a/","publishdate":"2021-10-08T07:56:31.35307Z","relpermalink":"/publication/meisteral-acl-21-a/","section":"publication","summary":"Beam search is a go-to strategy for decoding neural sequence models. The algorithm can naturally be viewed as a subset optimization problem, albeit one where the corresponding set function does not reflect interactions between candidates. Empirically, this leads to sets often exhibiting high overlap, e.g., strings may differ by only a single word. Yet in use-cases that call for multiple solutions, a diverse or representative set is often desired. To address this issue, we propose a reformulation of beam search, which we call determinantal beam search. Determinantal beam search has a natural relationship to determinantal point processes (DPPs), models over sets that inherently encode intra-set interactions. By posing iterations in beam search as a series of subdeterminant maximization problems, we can turn the algorithm into a diverse subset selection process. In a case study, we use the string subsequence kernel to explicitly encourage n-gram coverage in text generated from a sequence model. We observe that our algorithm offers competitive performance against other diverse set generation strategies in the context of language generation, while providing a more general approach to optimizing for diversity.","tags":null,"title":"Determinantal Beam Search","type":"publication"},{"authors":["Jennifer C. White","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"63ccda2611a09489178b0aead7d48be0","permalink":"https://rycolab.io/publication/whitecotterell-acl-21/","publishdate":"2021-10-08T07:56:31.063834Z","relpermalink":"/publication/whitecotterell-acl-21/","section":"publication","summary":"Since language models are used to model a wide variety of languages, it is natural to ask whether the neural architectures used for the task have inductive biases towards modeling particular types of languages. Investigation of these biases has proved complicated due to the many variables that appear in the experimental setup. Languages vary in many typological dimensions, and it is difficult to single out one or two to investigate without the others acting as confounders. We propose a novel method for investigating the inductive biases of language models using artificial languages. These languages are constructed to allow us to create parallel corpora across languages that differ only in the typological feature being investigated, such as word order. We then use them to train and test language models. This constitutes a fully controlled causal framework, and demonstrates how grammar engineering can serve as a useful tool for analyzing neural models. Using this method, we find that commonly used neural architectures exhibit different inductive biases: LSTMs display little preference with respect to word ordering, while transformers display a clear preference for some orderings over others. Further, we find that neither the inductive bias of the LSTM nor that of the transformer appear to reflect any tendencies that we see in attested natural languages","tags":null,"title":"Examining the Inductive Bias of Neural Language Models with Artificial Languages","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"e6b72c9eb5a3b6ca73d12736d3e80524","permalink":"https://rycolab.io/publication/zmigrodal-acl-21-b/","publishdate":"2021-10-08T07:56:31.505921Z","relpermalink":"/publication/zmigrodal-acl-21-b/","section":"publication","summary":"Weighted finite-state machines are a fundamental building block of NLP systems. They have withstood the test of time—from their early use in noisy channel models in the 1990s up to modern-day neurally parameterized conditional random fields. This work examines the computation of higher-order derivatives with respect to the normalization constant for weighted finite-state machines. We provide a general algorithm for evaluating derivatives of all orders, which has not been previously described in the literature. In the case of second-order derivatives, our scheme runs in the optimal O(Aˆ2 Nˆ4) time where A is the alphabet size and N is the number of states. Our algorithm is significantly faster than prior algorithms. Additionally, our approach leads to a significantly faster algorithm for computing second-order expectations, such as covariance matrices and gradients of first-order expectations.","tags":null,"title":"Higher-order Derivatives of Weighted Finite-state Machines","type":"publication"},{"authors":["Clara Meister","Stefan Lazov","Isabelle Augenstein","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"4cd4d9cdf21d7aa06d76609bef78973d","permalink":"https://rycolab.io/publication/meisteral-acl-21-b/","publishdate":"2021-10-08T07:56:31.800317Z","relpermalink":"/publication/meisteral-acl-21-b/","section":"publication","summary":"Sparse attention has been claimed to increase model interpretability under the assumption that it highlights influential inputs. Yet the attention distribution is typically over representations internal to the model rather than the inputs themselves, suggesting this assumption may not have merit. We build on the recent work exploring the interpretability of attention; we design a set of experiments to help us understand how sparsity affects our ability to use attention as an explainability tool. On three text classification tasks, we verify that only a weak relationship between inputs and co-indexed intermediate representations exists -- under sparse attention and otherwise. Further, we do not find any plausible mappings from sparse attention distributions to a sparse set of influential inputs through other avenues. Rather, we observe in this setting that inducing sparsity may make it less plausible that attention can be used as a tool for understanding model behavior.","tags":null,"title":"Is Sparse Attention more Interpretable?","type":"publication"},{"authors":["Clara Meister","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"2c6d336cdab04b35f064739fc275d5f5","permalink":"https://rycolab.io/publication/meisteral-acl-21-c/","publishdate":"2021-10-08T07:56:31.945458Z","relpermalink":"/publication/meisteral-acl-21-c/","section":"publication","summary":"We propose an alternate approach to quantifying how well language models learn natural language: we ask how well they match the statistical tendencies of natural language. To answer this question, we analyze whether text generated from language models exhibits the statistical tendencies present in the human-generated text on which they were trained. We provide a framework--paired with significance tests--for evaluating the fit of language models to these trends. We find that neural language models appear to learn only a subset of the tendencies considered, but align much more closely with empirical trends than proposed theoretical distributions (when present). Further, the fit to different distributions is highly-dependent on both model architecture and generation strategy. As concrete examples, text generated under the nucleus sampling scheme adheres more closely to the type--token relationship of natural language than text produced using standard ancestral sampling; text from LSTMs reflects the natural language distributions over length, stopwords, and symbols surprisingly well.","tags":null,"title":"Language Model Evaluation Beyond Perplexity","type":"publication"},{"authors":["Irene Nikkarinen*","Tiago Pimentel*","Damián Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"f5be89770e418009c5b19229481c04f1","permalink":"https://rycolab.io/publication/nikkarinenal-acl-findings-21/","publishdate":"2021-10-08T07:56:33.632859Z","relpermalink":"/publication/nikkarinenal-acl-findings-21/","section":"publication","summary":"The unigram distribution is the non-contextual probability of finding a specific word form in a corpus. While of central importance to the study of language, it is commonly approximated by each word's sample frequency in the corpus. This approach, being highly dependent on sample size, assigns zero probability to any out-of-vocabulary (oov) word form. As a result, it produces negatively biased probabilities for any oov word form, while positively biased probabilities to in-corpus words. In this work, we argue in favor of properly modeling the unigram distribution -- claiming it should be a central task in natural language processing. With this in mind, we present a novel model for estimating it in a language (a neuralization of Goldwater et al.'s (2011) model) and show it produces much better estimates across a diverse set of 7 languages than the naïve use of neural character-level language models.","tags":null,"title":"Modeling the Unigram Distribution","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1627776000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627776000,"objectID":"268774ed56f1d854d9db0799af7fb100","permalink":"https://rycolab.io/publication/zmigrodal-acl-21-a/","publishdate":"2021-10-08T07:56:31.209651Z","relpermalink":"/publication/zmigrodal-acl-21-a/","section":"publication","summary":"The connection between the maximum spanning tree in a directed graph and the best dependency tree of a sentence has been exploited by the NLP community. However, for many dependency parsing schemes, an important detail of this approach is that the spanning tree must have exactly one edge emanating from the root. While work has been done to efficiently solve this problem for finding the one-best dependency tree, no research has attempted to extend this solution to finding the K-best dependency trees. This is arguably a more important extension as a larger proportion of decoded trees will not be subject to the root constraint of dependency trees. Indeed, we show that the rate of root constraint violations increases by an average of 13 times when decoding with K=50 as opposed to K=1. In this paper, we provide a simplification of the K-best spanning tree algorithm of Camerini et al. (1980). Our simplification allows us to obtain a constant time speed-up over the original algorithm. Furthermore, we present a novel extension of the algorithm for decoding the K-best dependency trees of a graph which are subject to a root constraint.","tags":null,"title":"On Finding the $K$-best Non-projective Dependency Trees","type":"publication"},{"authors":["Jennifer C. White","Tiago Pimentel","Naomi Saphra","Ryan Cotterell"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"81064504d2ba8dca773aebd9360dd37e","permalink":"https://rycolab.io/publication/whiteal-naacl-21/","publishdate":"2021-10-05T16:25:27.329214Z","relpermalink":"/publication/whiteal-naacl-21/","section":"publication","summary":"Probes are models devised to investigate the encoding of knowledge—e.g. syntactic structure—in contextual representations. Probes are often designed for simplicity, which has led to restrictions on probe design that may not allow for the full exploitation of the structure of encoded information; one such restriction is linearity. We examine the case of a structural probe (Hewitt and Manning, 2019), which aims to investigate the encoding of syntactic structure in contextual representations through learning only linear transformations. By observing that the structural probe learns a metric, we are able to kernelize it and develop a novel non-linear variant with an identical number of parameters. We test on 6 languages and find that the radial-basis function (RBF) kernel, in conjunction with regularization, achieves a statistically significant improvement over the baseline in all languages—implying that at least part of the syntactic knowledge is encoded non-linearly. We conclude by discussing how the RBF kernel resembles BERT’s self-attention layers and speculate that this resemblance leads to the RBF-based probe’s stronger performance.","tags":null,"title":"A Non-Linear Structural Probe","type":"publication"},{"authors":["Rowan Hall Mauslay","Ryan Cotterell"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"15d2c4f4ebbb7bfb3c485294d0006231","permalink":"https://rycolab.io/publication/hall-mauslayal-naacl-21/","publishdate":"2021-10-08T07:56:34.118132Z","relpermalink":"/publication/hall-mauslayal-naacl-21/","section":"publication","summary":"Analysing whether neural language models encode linguistic information has become popular in NLP. One method of doing so, which is frequently cited to support the claim that models like BERT encode syntax, is called probing; probes are small supervised models trained to extract linguistic information from another model's output. If a probe is able to predict a particular structure, it is argued that the model whose output it is trained on must have implicitly learnt to encode it. However, drawing a generalisation about a model's linguistic knowledge about a specific phenomena based on what a probe is able to learn may be problematic: in this work, we show that semantic cues in training data means that syntactic probes do not properly isolate syntax. We generate a new corpus of semantically nonsensical but syntactically well-formed Jabberwocky sentences, which we use to evaluate two probes trained on normal data. We train the probes on several popular language models (BERT, GPT-2, and RoBERTa), and find that in all settings they perform worse when evaluated on these data, for one probe by an average of 15.4 UUAS points absolute. Although in most cases they still outperform the baselines, their lead is reduced substantially, e.g. by 53% in the case of BERT for one probe. This begs the question: what empirical scores constitute knowing syntax?","tags":null,"title":"Do Syntactic Probes Probe Syntax? Experiments with Jabberwocky Probing","type":"publication"},{"authors":["Tiago Pimentel","Brian Roark","Søren Wichmann","Ryan Cotterell","Damián Blasi"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"f4dd65625d1c8a5c507a4a6aee443f2b","permalink":"https://rycolab.io/publication/pimentelal-naacl-21-a/","publishdate":"2021-10-08T07:56:34.267506Z","relpermalink":"/publication/pimentelal-naacl-21-a/","section":"publication","summary":"This work presents an information-theoretic operationalisation of cross-linguistic non-arbitrariness. It is not a new idea that there are small, cross-linguistic associations between the forms and meanings of words. For instance, it has been claimed (Blasi et al., 2016) that the word for \"tongue\" is more likely than chance to contain the phone [l]. By controlling for the influence of language family and geographic proximity within a very large concept-aligned cross-lingual lexicon, we extend methods previously used to detect within language non-arbitrariness (Pimentel et al., 2019) to measure cross-linguistic associations. We find that there is a significant effect of non-arbitrariness, but it is unsurprisingly small (less than 0.5% on average according to our information-theoretic estimate). We also provide a concept-level analysis which shows that a quarter of the concepts considered in our work exhibit a significant level of cross-linguistic non-arbitrariness. In sum, the paper provides new methods to detect cross-linguistic associations at scale.","tags":null,"title":"Finding Concept-specific Biases in Form--Meaning Associations","type":"publication"},{"authors":["Tiago Pimentel*","Irene Nikkarinen*","Kyle Mahowald","Ryan Cotterell","Damián Blasi"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"678503316636b9c24f84634d0d977867","permalink":"https://rycolab.io/publication/pimentelal-naacl-21-b/","publishdate":"2021-10-08T07:56:34.855969Z","relpermalink":"/publication/pimentelal-naacl-21-b/","section":"publication","summary":"The mapping of lexical meanings to wordforms is a major feature of natural languages. While usage pressures might assign short words to frequent meanings (Zipf's law of abbreviation), the need for a productive and open-ended vocabulary, local constraints on sequences of symbols, and various other factors all shape the lexicons of the world's languages. Despite their importance in shaping lexical structure, the relative contributions of these factors have not been fully quantified. Taking a coding-theoretic view of the lexicon and making use of a novel generative statistical model, we define upper bounds for the compressibility of the lexicon under various constraints. Examining corpora from 7 typologically diverse languages, we use those upper bounds to quantify the lexicon's optimality and to explore the relative costs of major constraints on natural codes. We find that (compositional) morphology and graphotactics can sufficiently account for most of the complexity of natural codes -- as measured by code length.","tags":null,"title":"How (Non-)Optimal is the Lexicon?","type":"publication"},{"authors":["Josef Valvoda","Tiago Pimentel","Niklas Stoehr","Ryan Cotterell","Simone Teufel"],"categories":null,"content":"","date":1622505600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622505600,"objectID":"8d79bfd3c3c02b61d3ff68ec34c707e9","permalink":"https://rycolab.io/publication/valvodaal-naacl-21/","publishdate":"2021-10-08T07:56:34.43756Z","relpermalink":"/publication/valvodaal-naacl-21/","section":"publication","summary":"In common law, the outcome of a new case is determined mostly by precedent cases, rather than by existing statutes. However, how exactly does the precedent influence the outcome of a new case? Answering this question is crucial for guaranteeing fair and consistent judicial decision-making. We are the first to approach this question computationally by comparing two longstanding jurisprudential views; Halsbury's, who believes that the arguments of the precedent are the main determinant of the outcome, and Goodhart's, who believes that what matters most is the precedent's facts. We base our study on the corpus of legal cases from the European Court of Human Rights (ECtHR), which allows us to access not only the case itself, but also cases cited in the judges' arguments (i.e. the precedent cases).  Taking an information-theoretic view, and modelling the question as a case outcome classification task, we find that the precedent's arguments share 0.38 nats of information with the case's outcome, whereas precedent's facts only share 0.18 nats of information (i.e., 58% less); suggesting Halsbury's view may be more accurate in this specific court.  We found however in a qualitative analysis that there are specific statues where Goodhart's view dominates, and present some evidence these are the ones where the legal concept at hand is less straightforward.","tags":null,"title":"What About the Precedent: An Information-Theoretic Analysis of Common Law","type":"publication"},{"authors":["Shijie Wu","Mans Hulden","Ryan Cotterell"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"a69473188b6d75bd8a5e12db43511a77","permalink":"https://rycolab.io/publication/wual-eacl-21/","publishdate":"2021-10-08T07:56:32.243756Z","relpermalink":"/publication/wual-eacl-21/","section":"publication","summary":"The transformer has been shown to outperform recurrent neural network-based sequence-to-sequence models in various word-level NLP tasks. The model offers other benefits as well: It trains faster and has fewer parameters. Yet for character-level transduction tasks, eg morphological inflection generation and historical text normalization, few shows success on outperforming recurrent models with the transformer. In an empirical study, we uncover that, in contrast to recurrent sequence-to-sequence models, the batch size plays a crucial role in the performance of the transformer on character-level tasks, and we show that with a large enough batch size, the transformer does indeed outperform recurrent models. We also introduce a simple technique to handle feature-guided character-level transduction that further improves performance. With these insights, we achieve state-of-the-art performance on morphological inflection and historical text normalization. We also show that the transformer outperforms a strong baseline on two other character-level transduction tasks: grapheme-to-phoneme conversion and transliteration.","tags":null,"title":"Applying the Transformer to Character-level Transduction","type":"publication"},{"authors":["Tiago Pimentel","Ryan Cotterell","Brian Roark"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"acb5f67a068d4761ecd5c5bf41860404","permalink":"https://rycolab.io/publication/pimentelal-eacl-21/","publishdate":"2021-10-08T07:56:32.389245Z","relpermalink":"/publication/pimentelal-eacl-21/","section":"publication","summary":"Psycholinguistic studies of human word processing and lexical access provide ample evidence of the preferred nature of word-initial versus word-final segments, e.g., in terms of attention paid by listeners (greater) or the likelihood of reduction by speakers (lower). This has led to the conjecture -- as in Wedel et al. (2019b), but common elsewhere -- that languages have evolved to provide more information earlier in words than later. Information-theoretic methods to establish such tendencies in lexicons have suffered from several methodological shortcomings that leave open the question of whether this high word-initial informativeness is actually a property of the lexicon or simply an artefact of the incremental nature of recognition. In this paper, we point out the confounds in existing methods for comparing the informativeness of segments early in the word versus later in the word, and present several new measures that avoid these confounds. When controlling for these confounds, we still find evidence across hundreds of languages that indeed there is a cross-linguistic tendency to front-load information in words.","tags":null,"title":"Disambiguatory signals are stronger in word initial positions","type":"publication"},{"authors":["Martina Forster","Clara Meister","Ryan Cotterell"],"categories":null,"content":"","date":1617235200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1617235200,"objectID":"ec9df7e414d8bdebbf0c0fa12fec564a","permalink":"https://rycolab.io/publication/forsteral-eacl-21/","publishdate":"2021-10-08T07:56:32.095925Z","relpermalink":"/publication/forsteral-eacl-21/","section":"publication","summary":"Neural sequence-to-sequence models are currently the predominant choice for language generation tasks. Yet, on word-level tasks, exact inference of these models reveals the empty string is often the global optimum. Prior works have speculated this phenomenon is a result of the inadequacy of neural models for language generation. However, in the case of morphological inflection, we find that the empty string is almost never the most probable solution under the model. Further, greedy search often finds the global optimum. These observations suggest that the poor calibration of many neural models may stem from characteristics of a specific subset of tasks rather than general ill-suitedness of such models for language generation.","tags":null,"title":"Searching for Search Errors in Neural Morphological Inflection","type":"publication"},{"authors":["Jiaoda Li","Ryan Cotterell","Mrinmaya Sachan"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"d2bf60ebd17f59db73387e848801b747","permalink":"https://rycolab.io/publication/lial-tacl-21/","publishdate":"2021-10-08T07:56:35.297259Z","relpermalink":"/publication/lial-tacl-21/","section":"publication","summary":"","tags":null,"title":"Differentiable Subset Pruning of Transformer Heads","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"bd618c3c802e3eefc5f846e2e6fa5fa3","permalink":"https://rycolab.io/publication/zmigrodal-tacl-21/","publishdate":"2021-10-08T07:56:35.450783Z","relpermalink":"/publication/zmigrodal-tacl-21/","section":"publication","summary":"We give a general framework for inference in spanning tree models. We propose unified algorithms for the important cases of first-order expectations and second-order expectations in edge-factored, non-projective spanning-tree models. Our algorithms exploit a fundamental connection between gradients and expectations, which allows us to derive efficient algorithms. These algorithms are easy to implement with or without automatic differentiation software. We motivate the development of our framework with several cautionary tales of previous research, which has developed numerous inefficient algorithms for computing expectations and their gradients. We demonstrate how our framework efficiently computes several quantities with known algorithms, including the expected attachment score, entropy, and generalized expectation criteria. As a bonus, we give algorithms for quantities that are missing in the literature, including the KL divergence. In all cases, our approach matches the efficiency of existing algorithms and, in several cases, reduces the runtime complexity by a factor of the sentence length. We validate the implementation of our framework through runtime experiments. We find our algorithms are up to 15 and 9 times faster than previous algorithms for computing the Shannon entropy and the gradient of the generalized expectation objective, respectively.","tags":null,"title":"Efficient Computation of Expectations under Spanning Tree Distributions","type":"publication"},{"authors":["Emanuele Bugliarello","Ryan Cotterell","Naoaki Okazaki","Desmond Elliott"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"52c57b1e23ec68c83e2b0a32be10dde4","permalink":"https://rycolab.io/publication/bugliarelloal-tacl-21/","publishdate":"2021-10-08T07:56:35.154464Z","relpermalink":"/publication/bugliarelloal-tacl-21/","section":"publication","summary":"Large-scale pretraining and task-specific fine-tuning is now the standard methodology for many tasks in computer vision and natural language processing. Recently, a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI. These models can be categorized into either single-stream or dual-stream encoders. We study the differences between these two categories, and show how they can be unified under a single theoretical framework. We then conduct controlled experiments to discern the empirical differences between five vision and language BERTs. Our experiments show that training data and hyperparameters are responsible for most of the differences between the reported results, but they also reveal that the embedding layer plays a crucial role in these massive models.","tags":null,"title":"Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs","type":"publication"},{"authors":["Adina Williams","Ryan Cotterell","Lawrence Wolf-Sonkin","Damian Blasi","Hanna Wallach"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"647f8242e2378189a60913210c69e277","permalink":"https://rycolab.io/publication/williamsal-tacl-21/","publishdate":"2021-10-08T07:56:35.617089Z","relpermalink":"/publication/williamsal-tacl-21/","section":"publication","summary":"We use large-scale corpora in six different gendered languages, along with tools from NLP and information theory, to test whether there is a relationship between the grammatical genders of inanimate nouns and the adjectives used to describe those nouns. For all six languages, we find that there is a statistically significant relationship. We also find that there are statistically significant relation- ships between the grammatical genders of inanimate nouns and the verbs that take those nouns as direct objects, as indirect objects, and as subjects. We defer a deeper investiga- tion of these relationships for future work.","tags":null,"title":"On the Relationships Between the Grammatical Genders of Inanimate Nouns and Their Co-Occurring Adjectives and Verbs","type":"publication"},{"authors":["Edoardo M. Ponti","Ivan Vulić","Ryan Cotterell","Marinela Parović","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1609459200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1609459200,"objectID":"3b871b2a579c4fffa97444fa9ce329f7","permalink":"https://rycolab.io/publication/pontial-tacl-21/","publishdate":"2021-10-08T07:56:35.000948Z","relpermalink":"/publication/pontial-tacl-21/","section":"publication","summary":"Most combinations of NLP tasks and language varieties lack in-domain examples for supervised training because of the paucity of annotated data. How can neural models make sample-efficient generalizations from task-language combinations with available data to low-resource ones? In this work, we propose a Bayesian generative model for the space of neural parameters. We assume that this space can be factorized into latent variables for each language and each task. We infer the posteriors over such latent variables based on data from seen task-language combinations through variational inference. This enables zero-shot classification on unseen combinations at prediction time. For instance, given training data for named entity recognition (NER) in Vietnamese and for part-of-speech (POS) tagging in Wolof, our model can perform accurate predictions for NER in Wolof. In particular, we experiment with a typologically diverse sample of 33 languages from 4 continents and 11 families, and show that our model yields comparable or better results than state-of-the-art, zero-shot cross-lingual transfer methods. Our code is available at https://github.com/cambridgeltl/parameter-factorization.","tags":null,"title":"Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages","type":"publication"},{"authors":["Paula Czarnowska","Sebastian Ruder","Ryan Cotterell","Ann Copestake"],"categories":null,"content":"","date":1606780800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606780800,"objectID":"a76d128f3056daaec67202402873bea1","permalink":"https://rycolab.io/publication/czarnowskaal-coling-20/","publishdate":"2021-10-08T07:56:43.006344Z","relpermalink":"/publication/czarnowskaal-coling-20/","section":"publication","summary":"We propose a novel morphologically aware probability model for bilingual lexicon induction, which jointly models lexeme translation and inflectional morphology in a structured way. Our model exploits the basic linguistic intuition that the lexeme is the key lexical unit of meaning, while inflectional morphology provides additional syntactic information. This approach leads to substantial performance improvements—19% average improvement in accuracy across 6 language pairs over the state of the art in the supervised setting and 16% in the weakly supervised setting. As another contribution, we highlight issues associated with modern BLI that stem from ignoring inflectional morphology, and propose three suggestions for improving the task.","tags":null,"title":"Morphologically Aware Word-Level Translation","type":"publication"},{"authors":["Francisco Vargas","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"7d4a36bf3e5fcd094021aaacb9bdb562","permalink":"https://rycolab.io/publication/vargascotterell-emnlp-20/","publishdate":"2021-10-08T07:56:40.688452Z","relpermalink":"/publication/vargascotterell-emnlp-20/","section":"publication","summary":"Bolukbasi et al. (2016) presents one of the first gender bias mitigation techniques for word embeddings. Their method takes pre-trained word embeddings as input and attempts to isolate a linear subspace that captures most of the gender bias in the embeddings. As judged by an analogical evaluation task, their method virtually eliminates gender bias in the embeddings. However, an implicit and untested assumption of their method is that the bias subspace is actually linear. In this work, we generalize their method to a kernelized, non-linear version. We take inspiration from kernel principal component analysis and derive a non-linear bias isolation technique. We discuss and overcome some of the practical drawbacks of our method for non-linear gender bias mitigation in word embeddings and analyze empirically whether the bias subspace is actually linear. Our analysis shows that gender bias is in fact well captured by a linear subspace, justifying the assumption of Bolukbasi et al. (2016).","tags":null,"title":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation","type":"publication"},{"authors":["Clara Meister","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"00b67b122cbde7f1fd036ec9b7fcfec9","permalink":"https://rycolab.io/publication/meisteral-emnlp-20/","publishdate":"2021-10-08T07:56:41.87351Z","relpermalink":"/publication/meisteral-emnlp-20/","section":"publication","summary":"Quite surprisingly, exact maximum a posteriori (MAP) decoding of neural language generators frequently leads to low-quality results. Rather, most state-of-the-art results on language generation tasks are attained using beam search despite its overwhelmingly high search error rate. This implies that the MAP objective alone does not express the properties we desire in text, which merits the question: if beam search is the answer, what was the question? We frame beam search as the exact solution to a different decoding objective in order to gain insights into why high probability under a model alone may not indicate adequacy. We find that beam search enforces uniform information density in text, a property motivated by cognitive science. We suggest a set of decoding objectives that explicitly enforce this property and find that exact decoding with these objectives alleviates the problems encountered when decoding poorly calibrated language generation models. Additionally, we analyze the text produced using various decoding strategies and see that, in our neural machine translation experiments, the extent to which this property is adhered to strongly correlates with BLEU.","tags":null,"title":"If Beam Search is the Answer, What was the Question?","type":"publication"},{"authors":["Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"e0cf51125dafd47351851f86c166e2a8","permalink":"https://rycolab.io/publication/hennigenal-emnlp-20/","publishdate":"2021-08-20T18:07:37.764937Z","relpermalink":"/publication/hennigenal-emnlp-20/","section":"publication","summary":"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.","tags":null,"title":"Intrinsic Probing through Dimension Selection","type":"publication"},{"authors":["Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"06d17cff0b9422a1d5e37b5bb285eae6","permalink":"https://rycolab.io/publication/torroba-hennigenal-emnlp-20/","publishdate":"2021-10-08T07:56:41.418752Z","relpermalink":"/publication/torroba-hennigenal-emnlp-20/","section":"publication","summary":"Most modern NLP systems make use of pre-trained contextual representations that attain astonishingly high performance on a variety of tasks. Such high performance should not be possible unless some form of linguistic structure inheres in these representations, and a wealth of research has sprung up on probing for it. In this paper, we draw a distinction between intrinsic probing, which examines how linguistic information is structured within a representation, and the extrinsic probing popular in prior work, which only argues for the presence of such information by showing that it can be successfully extracted. To enable intrinsic probing, we propose a novel framework based on a decomposable multivariate Gaussian probe that allows us to determine whether the linguistic information in word embeddings is dispersed or focal. We then probe fastText and BERT for various morphosyntactic attributes across 36 languages. We find that most attributes are reliably encoded by only a few neurons, with fastText concentrating its linguistic structure more than BERT.","tags":null,"title":"Intrinsic Probing through Dimension Selection","type":"publication"},{"authors":["Jun Yen Leung","Guy Emerson","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"e215838e46b34bea56143b0e9d9d9ff7","permalink":"https://rycolab.io/publication/leungal-emnlp-20/","publishdate":"2021-10-08T07:56:43.154121Z","relpermalink":"/publication/leungal-emnlp-20/","section":"publication","summary":"Across languages, multiple consecutive adjectives modifying a noun (e.g. ``the big red dog″) follow certain unmarked ordering rules. While explanatory accounts have been put forward, much of the work done in this area has relied primarily on the intuitive judgment of native speakers, rather than on corpus data. We present the first purely corpus-driven model of multi-lingual adjective ordering in the form of a latent-variable model that can accurately order adjectives across 24 different languages, even when the training and testing languages are different. We utilize this novel statistical model to provide strong converging evidence for the existence of universal, cross-linguistic, hierarchical adjective ordering tendencies.","tags":null,"title":"Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model","type":"publication"},{"authors":["Arya D. McCarthy","Adina Williams","Shijia Liu","David Yarowsky","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"7ca684e99b099ca2409ebd8614e13a38","permalink":"https://rycolab.io/publication/mccarthyal-emnlp-20/","publishdate":"2021-10-08T07:56:43.342264Z","relpermalink":"/publication/mccarthyal-emnlp-20/","section":"publication","summary":"A grammatical gender system divides a lexicon into a small number of relatively fixed grammatical categories. How similar are these gender systems across languages? To quantify the similarity, we define gender systems extensionally, thereby reducing the problem of comparisons between languages’ gender systems to cluster evaluation. We borrow a rich inventory of statistical tools for cluster evaluation from the field of community detection (Driver and Kroeber, 1932; Cattell, 1945), that enable us to craft novel information-theoretic metrics for measuring similarity between gender systems. We first validate our metrics, then use them to measure gender system similarity in 20 languages. Finally, we ask whether our gender system similarities alone are sufficient to reconstruct historical relationships between languages. Towards this end, we make phylogenetic predictions on the popular, but thorny, problem from historical linguistics of inducing a phylogenetic tree over extant Indo-European languages. Languages on the same branch of our phylogenetic tree are notably similar, whereas languages from separate branches are no more similar than chance.","tags":null,"title":"Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions","type":"publication"},{"authors":["Tiago Pimentel","Naomi Saphra","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"ccd37a475ea91c7644701517d10adf20","permalink":"https://rycolab.io/publication/pimentelal-emnlp-20/","publishdate":"2021-10-08T07:56:41.577101Z","relpermalink":"/publication/pimentelal-emnlp-20/","section":"publication","summary":"The question of how to probe contextual word representations for linguistic structure in a way that is both principled and useful has seen significant attention recently in the NLP literature. In our contribution to this discussion, we argue for a probe metric that reflects the fundamental trade-off between probe complexity and performance: the Pareto hypervolume. To measure complexity, we present a number of parametric and non-parametric metrics. Our experiments using Pareto hypervolume as an evaluation metric show that probes often do not conform to our expectations---e.g., why should the non-contextual fastText representations encode more morpho-syntactic information than the contextual BERT representations? These results suggest that common, simplistic probing tasks, such as part-of-speech labeling and dependency arc labeling, are inadequate to evaluate the linguistic structure encoded in contextual word representations. This leads us to propose full dependency parsing as a probing task. In support of our suggestion that harder probing tasks are necessary, our experiments with dependency parsing reveal a wide gap in syntactic knowledge between contextual and non-contextual representations.","tags":null,"title":"Pareto Probing: Trading Off Accuracy for Simplicity","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"ef7a081c8b13acb03471ad23994af5e7","permalink":"https://rycolab.io/publication/zmigrodal-emnlp-20/","publishdate":"2021-10-08T07:56:41.728423Z","relpermalink":"/publication/zmigrodal-emnlp-20/","section":"publication","summary":"The connection between dependency trees and spanning trees is exploited by the NLP community to train and to decode graph-based dependency parsers. However, the NLP literature has missed an important difference between the two structures: only one edge may emanate from the root in a dependency tree. We analyzed the output of state-of-the-art parsers on many languages from the Universal Dependency Treebank: although these parsers are often able to learn that trees which violate the constraint should be assigned lower probabilities, their ability to do so unsurprisingly de-grades as the size of the training set decreases.In fact, the worst constraint-violation rate we observe is 24%. Prior work has proposed an inefficient algorithm to enforce the constraint, which adds a factor of n to the decoding runtime. We adapt an algorithm due to Gabow and Tarjan (1984) to dependency parsing, which satisfies the constraint without compromising the original runtime.","tags":null,"title":"Please Mind the Root: Decoding Arborescences for Dependency Parsing","type":"publication"},{"authors":["Johannes Bjerva","Elizabeth Salesky","Sabrina J. Mielke","Aditi Chaudhary","Giuseppe G. A. Celano","Edoardo M. Ponti","Ekaterina Vylomova","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"adc24c0cb72c2c158a274d0ca277e2a9","permalink":"https://rycolab.io/publication/bjervaal-sigtyp-20/","publishdate":"2021-10-08T07:56:42.192518Z","relpermalink":"/publication/bjervaal-sigtyp-20/","section":"publication","summary":"Typological knowledge bases (KBs) such as WALS (Dryer and Haspelmath, 2013) contain information about linguistic properties of the world’s languages. They have been shown to be useful for downstream applications, including cross-lingual transfer learning and linguistic probing. A major drawback hampering broader adoption of typological KBs is that they are sparsely populated, in the sense that most languages only have annotations for some features, and skewed, in that few features have wide coverage. As typological features often correlate with one another, it is possible to predict them and thus automatically populate typological KBs, which is also the focus of this shared task. Overall, the task attracted 8 submissions from 5 teams, out of which the most successful methods make use of such feature correlations. However, our error analysis reveals that even the strongest submitted systems struggle with predicting feature values for languages where few features are known.","tags":null,"title":"SIGTYP 2020 Shared Task: Prediction of Typological Features","type":"publication"},{"authors":["Tiago Pimentel","Rowan Hall Maudslay","Damián Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"1dea2434a38fd242a587b610d625d717","permalink":"https://rycolab.io/publication/pimentel-2-al-emnlp-20/","publishdate":"2021-10-08T07:56:42.36775Z","relpermalink":"/publication/pimentel-2-al-emnlp-20/","section":"publication","summary":"Lexical ambiguity is widespread in language, allowing for the reuse of economical word forms and therefore making language more efficient. If ambiguous words cannot be disambiguated from context, however, this gain in efficiency might make language less clear---resulting in frequent miscommunication. For a language to be clear and efficiently encoded, we posit that the lexical ambiguity of a word type should correlate with how much information context provides about it, on average. To investigate whether this is the case, we operationalise the lexical ambiguity of a word as the entropy of meanings it can take, and provide two ways to estimate this---one which requires human annotation (using WordNet), and one which does not (using BERT), making it readily applicable to a large number of languages. We validate these measures by showing that, on six high-resource languages, there are significant Pearson correlations between our BERT-based estimate of ambiguity and the number of synonyms a word has in WordNet (e.g. ρ=0.40 in English). We then test our main hypothesis---that a word's lexical ambiguity should negatively correlate with its contextual uncertainty---and find significant correlations on all 18 typologically diverse languages we analyse. This suggests that, in the presence of ambiguity, speakers compensate by making contexts more informative.","tags":null,"title":"Speakers Fill Semantic Gaps with Context","type":"publication"},{"authors":["Elizabeth Salesky","Eleanor Chodroff","Tiago Pimentel","Matthew Wiesner","Ryan Cotterell","Alan W Black","Jason Eisner"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"630bf245688581fe745a2fc35fde7c29","permalink":"https://rycolab.io/publication/saleskyal-acl-20/","publishdate":"2021-10-08T07:56:42.707781Z","relpermalink":"/publication/saleskyal-acl-20/","section":"publication","summary":"A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.","tags":null,"title":"A Corpus for Large-Scale Phonetic Typology","type":"publication"},{"authors":["Rowan Hall Maudslay","Josef Valvoda","Tiago Pimentel","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"553515b076cfd79cf6ac3caab7b3fc42","permalink":"https://rycolab.io/publication/hall-maudslayal-acl-20/","publishdate":"2021-10-08T07:56:42.854322Z","relpermalink":"/publication/hall-maudslayal-acl-20/","section":"publication","summary":"Measuring what linguistic information is encoded in continuous representations of language has become a popular area of research. To do this, researchers train \"probes\"— supervised models designed to extract linguistic structure from embeddings. The line between what constitutes a probe and a model designed to achieve a particular task is often blurred. To fully understand what we are learning about the target language representation—or the instrument with which we performing measurement with for that matter—we would do well to compare probes to classic parsers. As a case study, we consider the structural probe (Hewitt and Manning, 2019), designed to quantify the presence of syntactic information. We create a simple parser that improves upon the performance of the structural probe by 11.4% on UUAS, despite having an identical lightweight parameterization. Under a second less common metric, however, the structural probe outperforms traditional parsers. This begs the question: why should some metrics be preferred for probing and others for parsing?","tags":null,"title":"A Tale of a Probe and a Parser","type":"publication"},{"authors":["Clara Meister","Elizabeth Salesky","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"65df8ea6f2757b720fbb48bafcdc38a4","permalink":"https://rycolab.io/publication/meisteral-acl-20/","publishdate":"2021-10-08T07:56:41.128518Z","relpermalink":"/publication/meisteral-acl-20/","section":"publication","summary":"Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. We also find that variance in model performance can be explained largely by the resulting entropy of the model. Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place. Our code is available online at https://github.com/rycolab/entropyRegularization.","tags":null,"title":"Generalized Entropy Regularization or: There's Nothing Special about Label Smoothing","type":"publication"},{"authors":["Tiago Pimentel","Josef Valvoda","Rowan Hall Maudslay","Ran Zmigrod","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"96c6d62c85e714712e9a3cd7dfcf94c8","permalink":"https://rycolab.io/publication/pimentelal-acl-20/","publishdate":"2021-10-08T07:56:41.274372Z","relpermalink":"/publication/pimentelal-acl-20/","section":"publication","summary":"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually know about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task.  A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic formalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inhering in the contextualized representation. The empirical portion of our paper focuses on obtaining tight estimates for how much information BERT knows about both parts of speech and dependency labels, evaluating it in a set of ten typologically diverse languages often under-represented in parsing research, plus English, totalling eleven languages.  We find BERT only accounts for more information about parts of speech than a traditional type-based word embedding in five of the eleven analysed languages. When we look at dependency labels, BERT does improve upon type-based embeddings in all analysed languages, but accounting for at most 12% more information.","tags":null,"title":"Information-Theoretic Probing for Linguistic Structure","type":"publication"},{"authors":["Emanuele Bugliarello","Sabrina J. Mielke","Antonios Anastasopoulos","Ryan Cotterell","Naoaki Okazaki"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"10a46a0454ae8759bd3c1cf7f3c4f89a","permalink":"https://rycolab.io/publication/bugliarelloal-acl-20/","publishdate":"2021-10-08T07:56:40.544221Z","relpermalink":"/publication/bugliarelloal-acl-20/","section":"publication","summary":"The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.","tags":null,"title":"It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information","type":"publication"},{"authors":["Rowan Hall Maudslay","Tiago Pimentel","Ryan Cotterell","Simone Teufel"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"8364344a4166a666333052c13e0e17e6","permalink":"https://rycolab.io/publication/hall-maudslayal-wflp-20/","publishdate":"2021-10-08T07:56:43.731741Z","relpermalink":"/publication/hall-maudslayal-wflp-20/","section":"publication","summary":"We report the results of our system on the Metaphor Detection Shared Task at the Second Workshop on Figurative Language Processing 2020. Our model is an ensemble, utilising contextualised and static distributional semantic representations, along with word-type concreteness ratings. Using these features, it predicts word metaphoricity with a deep multi-layer perceptron. We are able to best the state-of-the-art from the 2018 Shared Task by an average of 8.0% F1, and finish fourth in both sub-tasks in which we participate.","tags":null,"title":"Metaphor Detection Using Context and Concreteness","type":"publication"},{"authors":["Adina Williams","Tiago Pimentel","Arya McCarthy","Hagen Blix","Eleanor Chodroff","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"628b1f074b060a453a0afee5af15adce","permalink":"https://rycolab.io/publication/williamsal-acl-20/","publishdate":"2021-10-08T07:56:40.978533Z","relpermalink":"/publication/williamsal-acl-20/","section":"publication","summary":"The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties. Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues. Here, we investigate the strength of those clues. More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns. We know that form and meaning are often also indicative of grammatical gender \u0026mdash; which, as we quantitatively verify, can itself share information with declension class \u0026mdash; so we also control for gender. We find for two Indo-European languages (Czech and German) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender). The three-way interaction between class, form, and meaning (given gender) is also significant. Our study is important for two reasons: First, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions. Secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages. The code is publicly available at https://github.com/rycolab/declension-mi.","tags":null,"title":"Predicting Declension Class from Form and Meaning","type":"publication"},{"authors":["Ekaterina Vylomova","Jennifer White","Elizabeth Salesky","Sabrina J. Mielke","Shijie Wu","Edoardo Maria Ponti","Rowan Hall Maudslay","Ran Zmigrod","Josef Valvoda","Svetlana Toldova","Francis Tyers","Elena Klyachko","Ilya Yegorov","Natalia Krizhanovsky","Paula Czarnowska","Irene Nikkarinen","Andrew Krizhanovsky","Tiago Pimentel","Lucas Torroba Hennigen","Christo Kirov","Garrett Nicolai","Adina Williams","Antonios Anastasopoulos","Hilaria Cruz","Eleanor Chodroff","Ryan Cotterell","Miikka Silfverberg","Mans Hulden"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"5ce06cc7342c98b30fcdfe8e326b1930","permalink":"https://rycolab.io/publication/vylomovaal-sigm-20/","publishdate":"2021-10-08T07:56:42.023336Z","relpermalink":"/publication/vylomovaal-sigm-20/","section":"publication","summary":"A broad goal in natural language processing (NLP) is to develop a system that has the capacity to process any natural language. Most systems, however, are developed using data from just one language such as English. The SIGMORPHON 2020 shared task on morphological reinflection aims to investigate systems’ ability to generalize across typologically distinct languages, many of which are low resource. Systems were developed using data from 45 languages and just 5 language families, fine-tuned with data from an additional 45 languages and 10 language families (13 in total), and evaluated on all 90 languages. A total of 22 systems (19 neural) from 10 teams were submitted to the task. All four winning systems were neural (two monolingual transformers and two massively multilingual RNN-based models with gated attention). Most teams demonstrate utility of data hallucination and augmentation, ensembles, and multilingual training for low-resource languages. Non-neural learners and manually designed grammars showed competitive and even superior performance on some languages (such as Ingrian, Tajik, Tagalog, Zarma, Lingala), especially with very limited data. Some language families (Afro-Asiatic, Niger-Congo, Turkic) were relatively easy for most systems and achieved over 90% mean accuracy while others were more challenging.","tags":null,"title":"SIGMORPHON 2020 Shared Task 0: Typologically Diverse Morphological Inflection","type":"publication"},{"authors":["Alexander Erdmann","Micha Elsner","Shijie Wu","Ryan Cotterell","Nizar Habash"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"05b7c75176d48aa04a24594196d68eb3","permalink":"https://rycolab.io/publication/erdmannal-acl-20/","publishdate":"2021-10-08T07:56:40.391816Z","relpermalink":"/publication/erdmannal-acl-20/","section":"publication","summary":"This work treats the paradigm discovery problem (PDP)—the task of learning an inflectional morphological system from unannotated sentences. We formalize the PDP and develop evaluation metrics for judging systems. Using currently available resources, we construct datasets for the task. We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages. Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm. Then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots. An error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work. Our code and data are available at https://github.com/alexerdmann/ParadigmDiscovery.","tags":null,"title":"The Paradigm Discovery Problem","type":"publication"},{"authors":["Clara Meister","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a583d975602b3880816be059c3edceb7","permalink":"https://rycolab.io/publication/meisteral-tacl-20/","publishdate":"2021-10-08T07:56:40.833956Z","relpermalink":"/publication/meisteral-tacl-20/","section":"publication","summary":"Decoding for many NLP tasks requires a heuristic algorithm for approximating exact search since the full search space is often intractable if not simply too large to traverse efficiently. The default algorithm for this job is beam search---a pruned version of breadth-first search---which in practice, returns better results than exact inference due to beneficial search bias. In this work, we show that standard beam search is a computationally inefficient choice for many decoding tasks; specifically, when the scoring function is a monotonic function in sequence length, other search algorithms can be used to reduce the number of calls to the scoring function (e.g., a neural network), which is often the bottleneck computation. We propose best-first beam search, an algorithm that provably returns the same set of results as standard beam search, albeit in the minimum number of scoring function calls to guarantee optimality (modulo beam size).  We show that best-first beam search can be used with length normalization and mutual information decoding, among other rescoring functions.  Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar search bias in terms of downstream performance, but runs in a fraction of the time.","tags":null,"title":"Best-First Beam Search","type":"publication"},{"authors":["Tiago Pimentel","Brian Roark","Ryan Cotterell"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fc0a72e5e26865b4a0609143cb55a02c","permalink":"https://rycolab.io/publication/pimentelal-tacl-20/","publishdate":"2021-10-08T07:56:42.528191Z","relpermalink":"/publication/pimentelal-tacl-20/","section":"publication","summary":"We present methods for calculating a measure of phonotactic complexity—bits per phoneme—that permits a straightforward cross-linguistic comparison. When given a word, represented as a sequence of phonemic segments such as symbols in the international phonetic alphabet, and a statistical model trained on a sample of word types from the language, we can approximately measure bits per phoneme using the negative log-probability of that word under the model. This simple measure allows us to compare the entropy across languages, giving insight into how complex a language’s phonotactics are. Using a collection of 1016 basic concept words across 106 languages, we demonstrate a very strong negative correlation of −0.74 between bits per phoneme and the average length of words.","tags":null,"title":"Phonotactic Complexity and its Trade-offs","type":"publication"},{"authors":["Martina Forster","Clara Meister"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"2aab8232131f5db34d63f575d7a3e8ce","permalink":"https://rycolab.io/publication/forstermeister-sigm-20/","publishdate":"2021-10-08T07:56:43.877166Z","relpermalink":"/publication/forstermeister-sigm-20/","section":"publication","summary":"This paper presents our system for the SIGMORPHON 2020 Shared Task. We build off of the baseline systems, performing exact inference on models trained on language family data. Our systems return the globally best solution under these models. Our two systems achieve 80.9% and 75.6% accuracy on the test set. We ultimately find that, in this setting, exact inference does not seem to help or hinder the performance of morphological inflection generators, which stands in contrast to its affect on Neural Machine Translation (NMT) models.","tags":null,"title":"SIGMORPHON 2020 Task 0 System Description: ETH Zürich Team","type":"publication"},{"authors":["Arya D. McCarthy","Christo Kirov","Matteo Grella","Amrit Nidhi","Patrick Xia","Kyle Gorman","Ekaterina Vylomova","Sabrina J. Mielke","Garrett Nicolai","Miikka Silfverberg","Timofey Arkhangelskiy","Nataly Krizhanovsky","Andrew Krizhanovsky","Elena Klyachko","Alexey Sorokin","John Mansfield","Valts Ernštreits","Yuval Pinter","Cassandra L. Jacobs","Ryan Cotterell","Mans Hulden","David Yarowsky"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"c645aa83fdb3089bec99c60b4d983b61","permalink":"https://rycolab.io/publication/mccarthyal-lrec-20/","publishdate":"2021-10-08T07:56:43.577111Z","relpermalink":"/publication/mccarthyal-lrec-20/","section":"publication","summary":"The Universal Morphology (UniMorph) project is a collaborative effort providing broad-coverage instantiated normalized morphological paradigms for hundreds of diverse world languages. The project comprises two major thrusts: a language-independent feature schema for rich morphological annotation and a type-level resource of annotated data in diverse languages realizing that schema. We have implemented several improvements to the extraction pipeline which creates most of our data, so that it is both more complete and more correct. We have added 66 new languages, as well as new parts of speech for 12 languages. We have also amended the schema in several ways. Finally, we present three new community tools: two to validate data for resource creators, and one to make morphological data available from the command line. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the schema, tooling, and dissemination of project resources since the UniMorph 2.0 release described at LREC 2018.","tags":null,"title":"UniMorph 3.0: Universal Morphology","type":"publication"},{"authors":["Paula Czarnowska","Sebastian Ruder","Edouard Grave","Ryan Cotterell","Ann Copestake"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"296b08393c2f8c8422cb74edca0e8730","permalink":"https://rycolab.io/publication/czarnowskaal-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:29.66145Z","relpermalink":"/publication/czarnowskaal-emnlp-ijcnlp-19/","section":"publication","summary":"Human translators routinely have to translate rare inflections of words--due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habláramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the best performing models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.","tags":null,"title":"Don’t Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction","type":"publication"},{"authors":["Pei Zhou","Weijia Shi","Jieyu Zhao","Kuan-Hao Huang","Muhao Chen","Ryan Cotterell","Kai-Wei Chang"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"25f9a7f3afd4937aa991707ae9d06c12","permalink":"https://rycolab.io/publication/zhoual-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:31.734774Z","relpermalink":"/publication/zhoual-emnlp-ijcnlp-19/","section":"publication","summary":"Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding of these languages under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.","tags":null,"title":"Examining Gender Bias in Languages with Grammatical Gender","type":"publication"},{"authors":["Rowan Hall Maudslay","Hila Gonen","Ryan Cotterell","Simone Teufel"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"74d11d07da4859c3b2be113fbd5adbda","permalink":"https://rycolab.io/publication/hall-maudslayal-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:29.815618Z","relpermalink":"/publication/hall-maudslayal-emnlp-ijcnlp-19/","section":"publication","summary":"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.","tags":null,"title":"It’s All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution","type":"publication"},{"authors":["Adina Williams","Ryan Cotterell","Lawrence Wolf-Sonkin","Damian Blasi","Hanna Wallach"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"b7f65d5b01c4b86c378d10100edac08d","permalink":"https://rycolab.io/publication/williamsal-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:31.289191Z","relpermalink":"/publication/williamsal-emnlp-ijcnlp-19/","section":"publication","summary":"Many of the world's languages employ grammatical gender on the lexeme. For instance, in Spanish, house ''casa'' is feminine, whereas the word for paper ''papel'' is masculine. To a speaker of a genderless language, this categorization seems to exist with neither rhyme nor reason. But, is the association of nouns to gender classes truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.","tags":null,"title":"Quantifying the Semantic Core of Gender Systems","type":"publication"},{"authors":["Edoardo Maria Ponti","Ivan Vulić","Ryan Cotterell","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"bd8665dc8c25e8141fa3a624c5fd0955","permalink":"https://rycolab.io/publication/pontial-emnlp-ijcnlp-19/","publishdate":"2021-08-20T18:07:30.986851Z","relpermalink":"/publication/pontial-emnlp-ijcnlp-19/","section":"publication","summary":"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace's method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world's languages, we hope that these insights will broaden the scope of applications for language technology.","tags":null,"title":"Towards Zero-Shot Language Modeling","type":"publication"},{"authors":["Arya D. McCarthy","Ekaterina Vylomova","Shijie Wu","Chaitanya Malaviya","Lawrence Wolf-Sonkin","Garrett Nicolai","Christo Kirov","Miikka Silfverberg","Sabrina Mielke","Jeffrey Heinz","Ryan Cotterell","Mans Hulden"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"da6c1c8cf0e58894242c01b498a5fdd8","permalink":"https://rycolab.io/publication/mccarthyal-tacl-19/","publishdate":"2021-08-20T18:07:30.548684Z","relpermalink":"/publication/mccarthyal-tacl-19/","section":"publication","summary":"The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years' inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year's strong baselines or highly ranked systems from previous years' shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.","tags":null,"title":"The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection","type":"publication"},{"authors":["Ran Zmigrod","Sabrina Mielke","Hanna Wallach","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"02c577382eddda3a8c17afd5facc6144","permalink":"https://rycolab.io/publication/zmigrodal-acl-19/","publishdate":"2021-08-20T18:07:32.031802Z","relpermalink":"/publication/zmigrodal-acl-19/","section":"publication","summary":"Gender stereotypes are manifest in most of the world's languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.","tags":null,"title":"Counterfactual Data Augmentation for Mitigating Gender Bias in Languages with Rich Morphology","type":"publication"},{"authors":["Shijie Wu","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"7547a9341238e368f994d0782bb943a4","permalink":"https://rycolab.io/publication/wucotterell-acl-19/","publishdate":"2021-08-20T18:07:31.581363Z","relpermalink":"/publication/wucotterell-acl-19/","section":"publication","summary":"Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.","tags":null,"title":"Exact Hard Monotonic Attention for Character-Level Transduction","type":"publication"},{"authors":["Tiago Pimentel","Arya McCarthy","Damian Blasi","Brian Roark","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2aec702aa08c71f0b3cfeb2cc63e90ba","permalink":"https://rycolab.io/publication/pimentelal-acl-19/","publishdate":"2021-08-20T18:07:30.842823Z","relpermalink":"/publication/pimentelal-acl-19/","section":"publication","summary":"A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram `gl′ have any systematic relationship to the meaning of words like `glisten′, `gleam′ and `glow′? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small---despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.","tags":null,"title":"Meaning to Form: Measuring Systematicity as Information","type":"publication"},{"authors":["Shijie Wu","Ryan Cotterell","Timothy J. O'Donnell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"19ada103f8247e8e1a50fb490f672367","permalink":"https://rycolab.io/publication/wual-acl-19/","publishdate":"2021-08-20T18:07:31.437492Z","relpermalink":"/publication/wual-acl-19/","section":"publication","summary":"We present a study of morphological irregularity. Following recent work, we define an information-theoretic measure of irregularity based on the predictability of forms in a language. Using a neural transduction model, we estimate this quantity for the forms in 28 languages. We first present several validatory and exploratory analyses of irregularity. We then show that our analyses provide evidence for a correlation between irregularity and frequency: higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent. To our knowledge, this result is the first of its breadth and confirms longstanding proposals from the linguistics literature. The correlation is more robust when aggregated at the level of whole paradigms---providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes.","tags":null,"title":"Measuring Morphological Irregularity","type":"publication"},{"authors":["Damian Blasi","Ryan Cotterell","Lawrence Wolf-Sonkin","Sabine Stoll","Balthasar Bickel","Marco Baroni"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"93639342f841bc5e61c57825210d6019","permalink":"https://rycolab.io/publication/blasial-acl-19/","publishdate":"2021-08-20T18:07:29.368073Z","relpermalink":"/publication/blasial-acl-19/","section":"publication","summary":"Embedding a clause inside another (``the girl [who likes cars [that run fast]] has arrived″) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics.","tags":null,"title":"On the distribution of deep clausal embeddings: A large cross-linguistic study","type":"publication"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"84cc0a9e2fb24ebde73ba952790fd562","permalink":"https://rycolab.io/publication/bjervaal-acl-19/","publishdate":"2021-08-20T18:07:29.066953Z","relpermalink":"/publication/bjervaal-acl-19/","section":"publication","summary":"The study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. In this paper, we present a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. Our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population.","tags":null,"title":"Uncovering Typological Implications with Belief Nets","type":"publication"},{"authors":["Alexander M. Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Isabelle Augenstein","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"88feb9579b10f90d0805e8ee27e849a8","permalink":"https://rycolab.io/publication/hoyleal-acl-19/","publishdate":"2021-08-20T18:07:29.95873Z","relpermalink":"/publication/hoyleal-acl-19/","section":"publication","summary":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","tags":null,"title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","type":"publication"},{"authors":["Sabrina Mielke","Ryan Cotterell","Kyle Gorman","Brian Roark","Jason Eisner"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"f4b1b6ea089e83dd43526718672dc3d0","permalink":"https://rycolab.io/publication/mielkeal-acl-19/","publishdate":"2021-08-20T18:07:30.693441Z","relpermalink":"/publication/mielkeal-acl-19/","section":"publication","summary":"How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that ``translationese″ is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.","tags":null,"title":"What Kind of Language Is Hard to Language-Model?","type":"publication"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"5d53e7002cc0533fd0478a4bc7673df1","permalink":"https://rycolab.io/publication/bjervaal-naacl-19/","publishdate":"2021-08-20T18:07:29.222293Z","relpermalink":"/publication/bjervaal-naacl-19/","section":"publication","summary":"In the principles-and-parameters framework, the structural features of languages depend on parameters that may be toggled on or off, with a single parameter often dictating the status of multiple features. The implied covariance between features inspires our probabilisation of this line of linguistic inquiry---we develop a generative model of language based on exponential-family matrix factorisation. By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, outperforming several baselines on the task of predicting held-out features. Furthermore, we show that language embeddings pre-trained on monolingual text allow for generalisation to unobserved languages. This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typological features and languages.","tags":null,"title":"A Probabilistic Generative Model of Linguistic Typology","type":"publication"},{"authors":["Chaitanya Malaviya","Shijie Wu","Ryan Cotterell"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"7cfbe19e49c0f05cd5a7b3beaa3f894e","permalink":"https://rycolab.io/publication/malaviyanaal-acl-19/","publishdate":"2021-08-20T18:07:30.405013Z","relpermalink":"/publication/malaviyanaal-acl-19/","section":"publication","summary":"English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of lemmatization seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for lemmatization and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the model in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and lemmatization is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.","tags":null,"title":"A Simple Joint Model for Improved Contextual Neural Lemmatization","type":"publication"},{"authors":["Alexander Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"dc6d51404dc2effc57d9764c923031ec","permalink":"https://rycolab.io/publication/hoyleal-naacl-19/","publishdate":"2021-08-20T18:07:30.108211Z","relpermalink":"/publication/hoyleal-naacl-19/","section":"publication","summary":"When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.","tags":null,"title":"Combining Sentiment Lexica with a Multi-View Variational Autoencoder","type":"publication"},{"authors":["Ekaterina Vylomova","Ryan Cotterell","Timothy Baldwin","Trevor Cohn","Jason Eisner"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"3f8612ffd7dc7fc048ff998b6a81e076","permalink":"https://rycolab.io/publication/vylomovaal-naacl-19/","publishdate":"2021-08-20T18:07:31.134744Z","relpermalink":"/publication/vylomovaal-naacl-19/","section":"publication","summary":"Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide ''gold'' tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.","tags":null,"title":"Contextualization of Morphological Inflection","type":"publication"},{"authors":["Jieyu Zhao","Tianlu Wang","Mark Yatskar","Ryan Cotterell","Vicente Ordonez","Kai-Wei Chang"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"07cd7213afdbf397a75bca00c039c3ee","permalink":"https://rycolab.io/publication/zhoual-naacl-19/","publishdate":"2021-08-20T18:07:31.879823Z","relpermalink":"/publication/zhoual-naacl-19/","section":"publication","summary":"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","tags":null,"title":"Gender Bias in Contextualized Word Embeddings","type":"publication"},{"authors":["Shijia Liu","Adina Williams","Hongyuan Mei","Ryan Cotterell"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"06e41617c9ae911725b3fc26bd63b47d","permalink":"https://rycolab.io/publication/liual-naacl-19/","publishdate":"2021-08-20T18:07:30.261308Z","relpermalink":"/publication/liual-naacl-19/","section":"publication","summary":"While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the classifiers modify. Using the empirical distribution of classifiers from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the mutual information (in bits) between the distribution over classifiers and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.","tags":null,"title":"On the Idiosyncrasies of the Mandarin Chinese Classifier System","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Mans Hulden","Jason Eisner"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"4df7e71c0378509ff7f0bb1850a5990f","permalink":"https://rycolab.io/publication/cotterellal-tacl-19/","publishdate":"2021-08-20T18:07:29.514482Z","relpermalink":"/publication/cotterellal-tacl-19/","section":"publication","summary":"We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language's inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm---how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.","tags":null,"title":"On the Complexity and Typology of Inflectional Morphological Systems","type":"publication"},{"authors":["Sebastian Ruder$^*$","Ryan Cotterell$^*$","Yova Kementchedjhieva","Anders Søgaard"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"2f067d72799894553dc0f72584bb1252","permalink":"https://rycolab.io/publication/ruderal-emnlp-18/","publishdate":"2021-08-20T18:07:25.332333Z","relpermalink":"/publication/ruderal-emnlp-18/","section":"publication","summary":"We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.","tags":null,"title":"A Discriminative Latent-Variable Model for Bilingual Lexicon Induction","type":"publication"},{"authors":["Yova Kementchedjhieva","Sebastian Ruder","Ryan Cotterell","Anders Søgaard"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"12558fbbd9611f5da65ff93738691b22","permalink":"https://rycolab.io/publication/kementchedjhievaal-conll-18/","publishdate":"2021-08-20T18:07:24.739743Z","relpermalink":"/publication/kementchedjhievaal-conll-18/","section":"publication","summary":"Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings.","tags":null,"title":"Generalizing Procrustes Analysis for Better Bilingual Dictionary Induction","type":"publication"},{"authors":["Shijie Wu","Pamela Shapiro","Ryan Cotterell"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"898bd87faa75ca4cc9c323a70eb39ebf","permalink":"https://rycolab.io/publication/wual-emnlp-18/","publishdate":"2021-08-20T18:07:25.627764Z","relpermalink":"/publication/wual-emnlp-18/","section":"publication","summary":"Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.","tags":null,"title":"Hard Non-Monotonic Attention for Character-Level Transduction","type":"publication"},{"authors":["Arya D. McCarthy","Miikka Silfverberg","Ryan Cotterell","Mans Hulden","David Yarowsky"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"a150607e03d1cc2b9cc4a3a8da3fad7a","permalink":"https://rycolab.io/publication/mccarthyal-udw-18/","publishdate":"2021-08-20T18:07:25.172465Z","relpermalink":"/publication/mccarthyal-udw-18/","section":"publication","summary":"The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languages---UD at the token level and UniMorph at the type level. As each corpus is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project's annotations could be used to validate the other's. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as parsing and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13% recall. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.","tags":null,"title":"Marrying Universal Dependencies and Universal Morphology","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Arya D. McCarthy","Katharina Kann","Sabrina Mielke","Garrett Nicolai","Miikka Silfverberg","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"3af314661f7a50d23092c996ac3531de","permalink":"https://rycolab.io/publication/cotterellal-conll-18/","publishdate":"2021-08-20T18:07:23.865116Z","relpermalink":"/publication/cotterellal-conll-18/","section":"publication","summary":"The CoNLL-SIGMORPHON 2018 shared task on supervised learning of morphological generation featured data sets from 103 typologically diverse languages. Apart from extending the number of languages involved in earlier supervised tasks of generating inflected forms, this year the shared task also featured a new second task which asked participants to inflect words in sentential context, similar to a cloze task. This second task featured seven languages. Task 1 received 27 submissions and task 2 received 6 submissions. Both tasks featured a low, medium, and high data condition. Nearly all submissions featured a neural component and built on highly-ranked systems from the earlier 2017 shared task. In the inflection task (task 1), 41 of the 52 languages present in last year’s inflection task showed improvement by the best systems in the low-resource setting. The cloze task (task 2) proved to be difficult, and few submissions managed to consistently improve upon both a simple neural baseline system and a lemmarepeating baseline.","tags":null,"title":"The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection","type":"publication"},{"authors":["Lawrence Wolf-Sonkin$^*$","Jason Naradowsky$^*$","Sabrina J. Mielke$^*$","Ryan Cotterell$^*$"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"81c65001211ead5c4bae2ff60e6a56c7","permalink":"https://rycolab.io/publication/wolf-sonkin-acl-18/","publishdate":"2021-08-20T18:07:25.484794Z","relpermalink":"/publication/wolf-sonkin-acl-18/","section":"publication","summary":"Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.","tags":null,"title":"A Structured Variational Autoencoder for Contextual Morphological Inflection","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"3ae1326e83220b4c8f5d6afadaa7e6a8","permalink":"https://rycolab.io/publication/cotterelleisner-naacl-18/","publishdate":"2021-08-20T18:07:24.307085Z","relpermalink":"/publication/cotterelleisner-naacl-18/","section":"publication","summary":"What makes some types of languages more probable than others? For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be? The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language. In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains. In contrast to previous work, we work directly with the acoustic information---the first two formant values---rather than modeling discrete sets of symbols from the international phonetic alphabet. We develop a novel generative probability model and report results on over 200 languages.","tags":null,"title":"A Deep Generative Model of Vowel Formant Typology","type":"publication"},{"authors":["Ryan Cotterell","Sabrina J. Mielke","Jason Eisner","Brian Roark"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"d436f65260058640e7930be367a0e8cb","permalink":"https://rycolab.io/publication/cotterellal-naacl-18-a/","publishdate":"2021-08-20T18:07:24.012768Z","relpermalink":"/publication/cotterellal-naacl-18-a/","section":"publication","summary":"For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.","tags":null,"title":"Are All Languages Equally Hard to Language-Model?","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Sabrina J. Mielke","Jason Eisner"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9ee0d90db50ad67fb3060518130a3c42","permalink":"https://rycolab.io/publication/cotterellal-naacl-18-b/","publishdate":"2021-08-20T18:07:24.162106Z","relpermalink":"/publication/cotterellal-naacl-18-b/","section":"publication","summary":"Lexical ambiguity makes it difficult to compute useful statistics of a corpus. A given word form might represent any of several morphological feature bundles. One can, however, use unsupervised learning (as in EM) to fit a model that probabilistically disambiguates word forms. We present such an approach, which employs a neural network to smoothly model a prior distribution over feature bundles (even rare ones). Although this basic model does not consider a token's context, that very property allows it to operate on a simple list of unigram type counts, partitioning each count among different analyses of that unigram. We discuss evaluation metrics for this novel task and report results on 5 languages.","tags":null,"title":"Unsupervised Disambiguation of Syncretism in Inflected Lexicons","type":"publication"},{"authors":["Christo Kirov","Ryan Cotterell","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Patrick Xia","Manaal Faruqui","Sabrina Mielke","Arya McCarthy","Sandra Kübler","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"89cfa7b59e28d9a972a4e49489e18e08","permalink":"https://rycolab.io/publication/kiroval-lrec-18/","publishdate":"2021-08-20T18:07:24.884129Z","relpermalink":"/publication/kiroval-lrec-18/","section":"publication","summary":"The Universal Morphology (UniMorph) project is a collaborative effort to improve how NLP handles complex morphology across the world’s languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the collection, annotation, and dissemination of project resources since the initial UniMorph release described at LREC 2016.","tags":null,"title":"UniMorph 2.0: Universal Morphology","type":"publication"},{"authors":["Ryan Cotterell","Julia Kreutzer"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"8f9f94018a73e2ff526a65aeedc1d3ec","permalink":"https://rycolab.io/publication/cotterellkreutzer-arxiv-18/","publishdate":"2021-08-20T18:07:24.448949Z","relpermalink":"/publication/cotterellkreutzer-arxiv-18/","section":"publication","summary":"","tags":null,"title":"Explaining and Generalizing Back-Translation through Wake-Sleep","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1030c6c628ef205a3f67724e7d809180","permalink":"https://rycolab.io/publication/cotterellschuetze-tacl-18/","publishdate":"2021-08-20T18:07:24.593742Z","relpermalink":"/publication/cotterellschuetze-tacl-18/","section":"publication","summary":"Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word′s meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituent segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DErivBase (Zeller et al., 2013) data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3% and 5%. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist′s notion of morphological productivity.","tags":null,"title":"Joint Semantic Synthesis and Morphological Analysis of the Derived Word","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Mans Hulden","Jason Eisner"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1fa421ac33a261ae3d663c5a02cdb76a","permalink":"https://rycolab.io/publication/cotterellal-arxiv-18/","publishdate":"2021-08-20T18:07:23.719115Z","relpermalink":"/publication/cotterellal-arxiv-18/","section":"publication","summary":"","tags":null,"title":"On the Diachronic Stability of Irregularity in Inflectional Morphology","type":"publication"},{"authors":["Christo Kirov","Ryan Cotterell"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"15717ae85e8cb20882e5431a78d9e060","permalink":"https://rycolab.io/publication/kirovcotterell-tacl-18/","publishdate":"2021-08-20T18:07:25.02946Z","relpermalink":"/publication/kirovcotterell-tacl-18/","section":"publication","summary":"Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter in 1988, Pinker and Prince presented a comprehensive rebuttal of many of Rumelhart and McClelland's claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince's criticisms without requiring any simplification of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a reexamination of their utility in linguistic and cognitive modeling.","tags":null,"title":"Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate","type":"publication"},{"authors":["Ryan Cotterell","Kevin Duh."],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"1f118c7c6d7e9f331838935ea2fed573","permalink":"https://rycolab.io/publication/cotterellduh-ijcnlp-17/","publishdate":"2021-08-20T18:07:19.338782Z","relpermalink":"/publication/cotterellduh-ijcnlp-17/","section":"publication","summary":"Low-resource named entity recognition is still an open problem in NLP. Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world's languages it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows knowledge transfer from the high-resource languages to the low-resource ones, improving F1 by up to 9.8 points.","tags":null,"title":"Low-Resource Named Entity Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields","type":"publication"},{"authors":["Ryan Cotterell","Georg Heigold"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"4174111fd88ca9bd66c5290666730be9","permalink":"https://rycolab.io/publication/cotterellheigold-emnlp-17/","publishdate":"2021-08-20T18:07:19.625699Z","relpermalink":"/publication/cotterellheigold-emnlp-17/","section":"publication","summary":"Even for common NLP tasks, sufficient supervision is not available in many languages--morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones.","tags":null,"title":"Cross-lingual, Character-Level Neural Morphological Tagging","type":"publication"},{"authors":["Ryan Cotterell","Ekaterina Vylomova","Huda Khayrallah","Christo Kirov","David Yarowsky"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"d1c98d2aefe8457ea6296afaa5dc2204","permalink":"https://rycolab.io/publication/cotterellal-emnlp-17/","publishdate":"2021-08-20T18:07:18.883532Z","relpermalink":"/publication/cotterellal-emnlp-17/","section":"publication","summary":"The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models adapted from the inflection task are able to learn the range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems.","tags":null,"title":"Paradigm Completion for Derivational Morphology","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","Géraldine Walther","Ekaterina Vylomova","Patrick Xia","Manaal Faruqui","Sandra Kübler","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"c09bf1108b8a9f99886af0418cb4f453","permalink":"https://rycolab.io/publication/cotterellal-conll-17/","publishdate":"2021-08-20T18:07:18.740686Z","relpermalink":"/publication/cotterellal-conll-17/","section":"publication","summary":"The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation required systems to be trained and tested in each of 52 typologically diverse languages. In sub-task 1, submitted systems were asked to predict a specific inflected form of a given lemma. In sub-task 2, systems were given a lemma and some of its specific inflected forms, and asked to complete the inflectional paradigm by predicting all of the remaining inflected forms. Both sub-tasks included high, medium, and low-resource conditions. Sub-task 1 received 24 system submissions, while sub-task 2 received 3 system submissions. Following the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared task, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in non-identical sets of inflected forms being predicted correctly, suggesting that there is room for future improvement.","tags":null,"title":"CoNLL--SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection in 52 Languages","type":"publication"},{"authors":["Francis Ferraro","Adam Poliak","Ryan Cotterell","Benjamin Van Durme"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"087c51922a795fbc97864e39def05304","permalink":"https://rycolab.io/publication/ferraroal-starsem-17/","publishdate":"2021-08-20T18:07:19.782117Z","relpermalink":"/publication/ferraroal-starsem-17/","section":"publication","summary":"We study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10% gains over baselines.","tags":null,"title":"Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"bcf915476f31940a15d842647e3f1001","permalink":"https://rycolab.io/publication/kannal-acl-17/","publishdate":"2021-08-20T18:07:19.92643Z","relpermalink":"/publication/kannal-acl-17/","section":"publication","summary":"We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.","tags":null,"title":"One-Shot Neural Cross-Lingual Transfer for Paradigm Completion","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"71b13a4d0772b4d54b40f69f2e93feaf","permalink":"https://rycolab.io/publication/cotterelleisner-acl-17/","publishdate":"2021-08-20T18:07:19.482639Z","relpermalink":"/publication/cotterelleisner-acl-17/","section":"publication","summary":"Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.","tags":null,"title":"Probabilistic Typology: Deep Generative Models of Vowel Inventories","type":"publication"},{"authors":["Christo Kirov","John Sylak-Glassman","Rebecca Knowles","Ryan Cotterell","Matt Post"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"cd8133a88495b62b72d13bc2400f38a2","permalink":"https://rycolab.io/publication/kiroval-eacl-17/","publishdate":"2021-08-20T18:07:20.22186Z","relpermalink":"/publication/kiroval-eacl-17/","section":"publication","summary":"A traditional claim in linguistics is that all human languages are equally expressive---able to convey the same wide range of meanings. Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech. The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation.","tags":null,"title":"A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax","type":"publication"},{"authors":["Ekaterina Vylomova","Ryan Cotterell","Timothy Baldwin","Trevor Cohn"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"88dd0bbfa4129bc86b6c49d1d0fdf629","permalink":"https://rycolab.io/publication/vylomovaal-eacl-17/","publishdate":"2021-08-20T18:07:20.536737Z","relpermalink":"/publication/vylomovaal-eacl-17/","section":"publication","summary":"Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.","tags":null,"title":"Context-Aware Prediction of Derivational Word-forms","type":"publication"},{"authors":["Ryan Cotterell","Adam Poliak","Ben Van Durme","Jason Eisner"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"62e68751e7f01daaccf799f3031f75b2","permalink":"https://rycolab.io/publication/cotterellalb-eacl-17/","publishdate":"2021-08-20T18:07:19.174746Z","relpermalink":"/publication/cotterellalb-eacl-17/","section":"publication","summary":"The popular skip-gram model induces word embeddings by exploiting the signal from word-context coocurrence. We offer a new interpretation of skip-gram based on exponential family PCA-a form of matrix factorization to generalize the skip-gram model to tensor factorization. In turn, this lets us train embeddings through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or morphological information (to share parameters across related words). We experiment on 40 languages and show our model improves upon skip-gram.","tags":null,"title":"Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis","type":"publication"},{"authors":["Arun Kumar","Ryan Cotterell","Lluís Padró","Antoni Oliver"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"950429cf9d367338236f6c9801bbdeb1","permalink":"https://rycolab.io/publication/kumaral-eacl-17/","publishdate":"2021-08-20T18:07:20.385595Z","relpermalink":"/publication/kumaral-eacl-17/","section":"publication","summary":"The Dravidian languages are one of the most widely spoken language families in the world, yet there are very few annotated resources available to NLP researchers. To remedy this, we create DravMorph, a corpus annotated for morphological segmentation and part-of-speech. Additionally, we exploit novel features and higher-order models to set state-of-the-art results on these corpora on both tasks, beating techniques proposed in the literature by as much as 4 points in segmentation F1.","tags":null,"title":"Morphological Analysis of the Dravidian Language Family","type":"publication"},{"authors":["Ryan Cotterell","John Sylak-Glassman","Christo Kirov"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"fc54a64fe12dd9d57beea964e7ce8876","permalink":"https://rycolab.io/publication/cotterellala-eacl-17/","publishdate":"2021-08-20T18:07:19.030607Z","relpermalink":"/publication/cotterellala-eacl-17/","section":"publication","summary":"Many of the world's languages contain an abundance of inflected forms for each lexeme. A critical task in processing such languages is predicting these inflected forms. We develop a novel statistical model for the problem, drawing on graphical modeling techniques and recent advances in deep learning. We derive a Metropolis-Hastings algorithm to jointly decode the model. Our Bayesian network draws inspiration from principal parts morphological analysis. We demonstrate improvements on 5 languages.","tags":null,"title":"Neural Graphical Models over Strings for Principal Parts Morphological Paradigm Completion","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"919462c9664194568756fd3006526dba","permalink":"https://rycolab.io/publication/kannal-eacl-17/","publishdate":"2021-08-20T18:07:20.076832Z","relpermalink":"/publication/kannal-eacl-17/","section":"publication","summary":"We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.","tags":null,"title":"Neural Multi-Source Morphological Reinflection","type":"publication"},{"authors":["Ryan Cotterell","Arun Kumar","Hinrich Schütze"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"a39aaa943290d5f6face02f3d9d47983","permalink":"https://rycolab.io/publication/cotterellal-emnlp-16/","publishdate":"2021-08-20T18:07:14.345665Z","relpermalink":"/publication/cotterellal-emnlp-16/","section":"publication","summary":"Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure---especially in the case of derivational morphology. In this work, we introduce a discriminative joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.","tags":null,"title":"Morphological Segmentation Inside-Out","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"1feb9c000aedb9cbb4a319955397bc86","permalink":"https://rycolab.io/publication/kannal-emnlp-16/","publishdate":"2021-08-20T18:07:14.92185Z","relpermalink":"/publication/kannal-emnlp-16/","section":"publication","summary":"Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoderdecoder model for this task. Additionally, we extend our model to include morphemelevel and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian.","tags":null,"title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments","type":"publication"},{"authors":["Tim Vieira$^*$","Ryan Cotterell$^*$","Jason Eisner"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"1906bb6181348303c0d677575abad1ce","permalink":"https://rycolab.io/publication/vieiraal-emnlp-16/","publishdate":"2021-08-20T18:07:15.388898Z","relpermalink":"/publication/vieiraal-emnlp-16/","section":"publication","summary":"We propose a method for learning the structure of variable-order CRFs, a more flexible variant of higher-order linear-chain CRFs. Variableorder CRFs achieve faster inference by including features for only some of the tag ngrams. Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer. An active-set outer loop allows the feature set to grow as far as needed. On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2--6x speedup over a baseline, with no significant drop in accuracy.","tags":null,"title":"Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze","Jason Eisner"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"d5e258b2cfce483a0a8e30a31119cd1d","permalink":"https://rycolab.io/publication/cotterellal-acl-16/","publishdate":"2021-08-20T18:07:14.202616Z","relpermalink":"/publication/cotterellal-acl-16/","section":"publication","summary":"Languages with rich inflectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For instance, each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are unlikely to observe all inflections of a given lemma. This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information. We solve this problem by exploiting existing morphological resources that can enumerate a word’s component morphemes. We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create embeddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy, and word similarity","tags":null,"title":"Morphological Smoothing and Extrapolation of Word Embeddings","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"0dae3427f8a795d86396e714ddca0095","permalink":"https://rycolab.io/publication/cotterellal-sigmorphon-16/","publishdate":"2021-08-20T18:07:14.636438Z","relpermalink":"/publication/cotterellal-sigmorphon-16/","section":"publication","summary":"The 2016 SIGMORPHON Shared Task was devoted to the problem of morphological reinflection. It introduced morphological datasets for 10 languages with diverse typological characteristics. The shared task drew submissions from 9 teams representing 11 institutions reflecting a variety of approaches to addressing supervised learning of reinflection. For the simplest task, inflection generation from lemmas, the best system averaged 95.56% exact-match accuracy across all languages, ranging from Maltese (88.99%) to Hungarian (99.30%). With the relatively large training datasets provided, recurrent neural network architectures consistently performed best—in fact, there was a significant margin between neural and non-neural approaches. The best neural approach, averaged over all tasks and languages, outperformed the best nonneural one by 13.76% absolute; on individual tasks and languages the gap in accuracy sometimes exceeded 60%. Overall, the results show a strong state of the art, and serve as encouragement for future shared tasks that explore morphological analysis and generation with varying degrees of supervision.","tags":null,"title":"The SIGMORPHON 2016 Shared Task—Morphological Reinflection","type":"publication"},{"authors":["Ryan Cotterell","Tim Vieira","Hinrich Schütze"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"7fa0b09b73010847f89c913ed7de741a","permalink":"https://rycolab.io/publication/cotterellal-naacl-16/","publishdate":"2021-08-20T18:07:14.49089Z","relpermalink":"/publication/cotterellal-naacl-16/","section":"publication","summary":"We present a model of morphological segmentation that jointly learns to segment and restore orthographic changes, e.g., funniest 7 → fun-y-est. We term this form of analysis canonical segmentation and contrast it with the traditional surface segmentation, which segments a surface form into a sequence of substrings, e.g., funniest 7 → funn-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian.","tags":null,"title":"A Joint Model of Orthography and Morphological Segmentation","type":"publication"},{"authors":["Pushpendre Rastogi","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"bffcb6da09c3c39cf85ea1db79e3a5b8","permalink":"https://rycolab.io/publication/rastogial-naacl-16/","publishdate":"2021-08-20T18:07:15.241797Z","relpermalink":"/publication/rastogial-naacl-16/","section":"publication","summary":"How should one apply deep learning to tasks such as morphological reinflection, which stochastically edit one string to get another? A recent approach to such sequence-to-sequence tasks is to compress the input string into a vector that is then used to generate the output string, using recurrent neural networks. In contrast, we propose to keep the traditional architecture, which uses a finite-state transducer to score all possible output strings, but to augment the scoring function with the help of recurrent networks. A stack of bidirectional LSTMs reads the input string from leftto-right and right-to-left, in order to summarize the input context in which a transducer arc is applied. We combine these learned features with the transducer to define a probability distribution over aligned output strings, in the form of a weighted finite-state automaton. This reduces hand-engineering of features, allows learned features to examine unbounded context in the input string, and still permits exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization.","tags":null,"title":"Weighting Finite-State Transductions With Neural Context","type":"publication"},{"authors":["Chandler May","Ryan Cotterell","Benjamin Van Durme"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"7c53d024a335da48d7abaa47844cbe98","permalink":"https://rycolab.io/publication/mayal-arxiv-16/","publishdate":"2021-08-20T18:07:15.067282Z","relpermalink":"/publication/mayal-arxiv-16/","section":"publication","summary":"","tags":null,"title":"Analysis of Morphology in Topic Modeling","type":"publication"},{"authors":["John Sylak-Glassman","Ryan Cotterell"],"categories":null,"content":"","date":1451606400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1451606400,"objectID":"ccfb86dfac9259d2a0cd43a651f99e75","permalink":"https://rycolab.io/publication/glassmanal-cls-16/","publishdate":"2021-08-20T18:07:14.778626Z","relpermalink":"/publication/glassmanal-cls-16/","section":"publication","summary":"","tags":null,"title":"Contrastive Morphological Typology and Logical Hierarchies","type":"publication"},{"authors":["Nanyun Peng","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"27421098a0f9e3c451ea02557297cb9c","permalink":"https://rycolab.io/publication/pengal-emnlp-15/","publishdate":"2021-08-20T18:07:10.261959Z","relpermalink":"/publication/pengal-emnlp-15/","section":"publication","summary":"We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming. We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of variable-length n-gram count features.  This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.","tags":null,"title":"Dual Decomposition Inference for Graphical Models over Strings","type":"publication"},{"authors":["Thomas Müller","Ryan Cotterell","Alexander Fraser","Hinrich Schütze"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"307e76fe83844df78b8722e4c4925865","permalink":"https://rycolab.io/publication/muelleral-conll-15/","publishdate":"2021-08-20T18:07:10.405404Z","relpermalink":"/publication/muelleral-conll-15/","section":"publication","summary":"We present Lemming, a modular log-linear model that jointly models lemmatization and tagging and supports the integration of arbitrary global features. It is trainable on corpora annotated with gold standard tags and lemmata and does not rely on morphological dictionaries or analyzers. Lemming sets the new state of the art in token-based statistical lemmatization on six languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05 to 1.58. We also give empirical evidence that jointly modeling morphological tags and lemmata is mutually beneficial.","tags":null,"title":"Joint Lemmatization and Morphological Tagging with Lemming","type":"publication"},{"authors":["Ryan Cotterell","Thomas Müller","Alexander Fraser","Hinrich Schütze"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"457b1282971eef22ec6477186cc6f4c1","permalink":"https://rycolab.io/publication/cotterellal-conll-15/","publishdate":"2021-08-20T18:07:10.548594Z","relpermalink":"/publication/cotterellal-conll-15/","section":"publication","summary":"We present labeled morphological segmentation—an alternative view of morphological processing that unifies several tasks. We introduce a new hierarchy of morphotactic tagsets and Chipmunk, a discriminative morphological segmentation system that, contrary to previous work, explicitly models morphotactics. We show improved performance on three tasks for all six languages: (i) morphological segmentation, (ii) stemming and (iii)  morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline.","tags":null,"title":"Labeled Morphological Segmentation with Semi-Markov Models","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Schütze"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"58c9c1b17d1c3c1e7deb1d36d1765e88","permalink":"https://rycolab.io/publication/cotterellschuetze-naacl-15/","publishdate":"2021-08-20T18:07:10.980625Z","relpermalink":"/publication/cotterellschuetze-naacl-15/","section":"publication","summary":"Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semi-supervised learning, encouraging the vectors to encode a word’s morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study.","tags":null,"title":"Morphological Word Embeddings","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"db153a927a072c4049d322aa1f4dfc77","permalink":"https://rycolab.io/publication/cotterelleisner-naacl-15/","publishdate":"2021-08-20T18:07:10.836487Z","relpermalink":"/publication/cotterelleisner-naacl-15/","section":"publication","summary":"We present penalized expectation propagation (PEP), a novel algorithm for approximate inference in graphical models. Expectation propagation is a variant of loopy belief propagation that keeps messages tractable by projecting them back into a given family of functions. Our extension, PEP, uses a structuredsparsity penalty to encourage simple messages, thus balancing speed and accuracy. We specifically show how to instantiate PEP in the case of string-valued random variables, where we adaptively approximate finite-state distributions by variable-order n-gram models. On phonological inference problems, we obtain substantial speedup over previous related algorithms with no significant loss in accuracy.","tags":null,"title":"Penalized Expectation Propagation for Graphical Models over Strings","type":"publication"},{"authors":["Ryan Cotterell","Nanyun Peng","Jason Eisner"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"585f26f2d387f8189154c38ea4fd00a6","permalink":"https://rycolab.io/publication/cotterellal-tacl-15/","publishdate":"2021-08-20T18:07:10.693131Z","relpermalink":"/publication/cotterellal-tacl-15/","section":"publication","summary":"The observed pronunciations or spellings of words are often explained as arising from the ''underlying forms'' of their morphemes. These forms are latent strings that linguists try to reconstruct by hand. We propose to reconstruct them automatically at scale, enabling generalization to new words. Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express, we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form. Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite-state machines with trainable weights. We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages.","tags":null,"title":"Modeling Word Forms Using Latent Underlying Morphs and Phonology","type":"publication"},{"authors":["Ryan Cotterell","Nanyun Peng","Jason Eisner"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"98faa9c135408e6a0e6802b0a788931b","permalink":"https://rycolab.io/publication/cotterellal-acl-14/","publishdate":"2021-08-20T18:07:06.497496Z","relpermalink":"/publication/cotterellal-acl-14/","section":"publication","summary":"String similarity is most often measured by weighted or unweighted edit distance d(x, y). Ristad and Yianilos (1998) defined stochastic edit distance—a probability distribution p(y | x) whose parameters can be trained from data. We generalize this so that the probability of choosing each edit operation can depend on contextual features. We show how to construct and train a probabilistic finite-state transducer that computes our stochastic  ontextual edit distance. To illustrate the improvement from conditioning on context, we model typos found in social media text.","tags":null,"title":"Stochastic Contextual Edit Distance and Probabilistic FSTs","type":"publication"},{"authors":["Ryan Cotterell","Chris Callison-Burch"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"7b97832957e1076cee3b3f8e655b5dbe","permalink":"https://rycolab.io/publication/cotterellcallison-burch-lrec-14/","publishdate":"2021-08-20T18:07:06.640488Z","relpermalink":"/publication/cotterellcallison-burch-lrec-14/","section":"publication","summary":"","tags":null,"title":"A Multi-Dialect, Multi-Genre Corpus of Informal Written Arabic","type":"publication"},{"authors":["Ryan Cotterell","Adithya Renduchintala","Naomi Saphra","Chris Callison-Burch"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"4e4d9767fc5a4c61544a2c7c302d436d","permalink":"https://rycolab.io/publication/cotterellal-lrec-14/","publishdate":"2021-08-20T18:07:06.784596Z","relpermalink":"/publication/cotterellal-lrec-14/","section":"publication","summary":"","tags":null,"title":"An Algerian Arabic-French Code-Switched Corpus","type":"publication"},{"authors":["Gaurav Kumar","Yuan Cao","Ryan Cotterell","Chris Callison-Burch","Daniel Povey","Sanjeev Khudanpur"],"categories":null,"content":"","date":1388534400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1388534400,"objectID":"d05928c8ad19690bf4167afe768a1dc6","permalink":"https://rycolab.io/publication/kumaral-iwslt-14/","publishdate":"2021-08-20T18:07:06.354263Z","relpermalink":"/publication/kumaral-iwslt-14/","section":"publication","summary":"","tags":null,"title":"Translation of the CALLHOME Egyptian Arabic Corpus For Conversational Speech Translation","type":"publication"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1b5731b483ae8dd0ebcded987d15d758","permalink":"https://rycolab.io/classes/aflt-s22/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/aflt-s22/","section":"classes","summary":"This course serves as an introduction to various advanced topics in formal language theory. The primary focus of the course is on weighted formalisms, which can easily be applied in machine learning. Topics include finite-state machines as well as the algorithms that are commonly used for their manipulation. We will also cover weighted context-free grammars, weighted tree automata, and weighted mildly context-sensitive formalisms.","tags":null,"title":"Advanced Formal Language Theory, Spring 2022","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"4535e802ff19249b73859ad06460880f","permalink":"https://rycolab.io/classes/dep-parsing-sem/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/dep-parsing-sem/","section":"classes","summary":"This seminar explores a variety of algorithms for efficient dependency parsing and their derivatioin in a unified algebraic framework.","tags":null,"title":"Dependency Structures and Lexicalized Grammars","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"1c4ddc1d8e31eeb7d1abec4327e49d10","permalink":"https://rycolab.io/classes/esslli-21/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/esslli-21/","section":"classes","summary":"The *Information Theory in Linguistics* course focuses on the application of information-theoretic methods to natural language processing, emphasizing interdisciplinary connections with the field of linguistics.","tags":null,"title":"ESSLLI 2021","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"faf78b4c5b33c1d2bc0ac81d00bcaef2","permalink":"https://rycolab.io/classes/intro-nlp-f20/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp-f20/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing, Autumn 2020","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"197234ea5a7a4ba940cb8395f125159c","permalink":"https://rycolab.io/classes/intro-nlp-f21/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp-f21/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing, Fall 2021","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"37cbbccf26c1ff0827e5ddb2e1bc0186","permalink":"https://rycolab.io/classes/intro-nlp-s21/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp-s21/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing, Spring 2021","type":"widget_page"},{"authors":null,"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"436fe49b0452ad7ed91f2f83c827e732","permalink":"https://rycolab.io/classes/phil-f22/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/phil-f22/","section":"classes","summary":"This graduate class, taught like a seminar, is designed to help you understand the philosophical underpinnings of modern work in natural language processing (NLP), most of which centered around statistical machine learning applied to natural language data.","tags":null,"title":"Philosophy of Language and Computation, Autumn 2022","type":"widget_page"}]