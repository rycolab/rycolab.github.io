[{"authors":["group"],"categories":null,"content":"We are a collocation of collaborators working on a diverse range of topics in computational linguistics, natural language processing and machine learning.\nLab Motto: We put the fun in funicular! (Photo coming soon.)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3d4d5e14ee766925debc0cd6588cd11b","permalink":"https://rycolab.github.io/authors/group/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/group/","section":"authors","summary":"We are a collocation of collaborators working on a diverse range of topics in computational linguistics, natural language processing and machine learning.\nLab Motto: We put the fun in funicular! (Photo coming soon.","tags":null,"title":"","type":"authors"},{"authors":["adina"],"categories":null,"content":"Animal Form: Kinkajou\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bfc0fd43f26e67f5f46c4bc0baf09b8e","permalink":"https://rycolab.github.io/authors/adina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/adina/","section":"authors","summary":"Animal Form: Kinkajou\n-- ","tags":null,"title":"Adina Williams","type":"authors"},{"authors":["afra"],"categories":null,"content":"Afra is a second-year MSc student in computer science at ETH Z√ºrich. Before joining ETH, she received her bachelor\u0026rsquo;s in software engineering at Sharif University of Technology. She is working on a research project with Clara and Ryan. Her research interests include using machine learning for NLP tasks. She likes reading, hiking, and playing the piano in her spare time.\nNative Language: Persian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2091cc68cd468c5542742f222e5c4e2a","permalink":"https://rycolab.github.io/authors/afra/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/afra/","section":"authors","summary":"Afra is a second-year MSc student in computer science at ETH Z√ºrich. Before joining ETH, she received her bachelor\u0026rsquo;s in software engineering at Sharif University of Technology. She is working on a research project with Clara and Ryan.","tags":null,"title":"Afra Amini","type":"authors"},{"authors":["alex"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d7149a99f41440e55ea517c3fb5d3c99","permalink":"https://rycolab.github.io/authors/alex/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/alex/","section":"authors","summary":"","tags":null,"title":"Alexander Hoyle","type":"authors"},{"authors":["clara"],"categories":null,"content":"Clara recently started her PhD with Ryan at ETH Z√ºrich. She received her B.S. and M.S. from Stanford University in Computational and Mathematical Engineering. Her research interests include neural machine translation and robustness in AI-based systems. In her free time, she likes to rock climb, trail run, and search for the elusive cheap bar in Switzerland.\nNative Language: English\nAnimal Form: Grumpy Cat\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f3de996f9586f710ca1ed8c97792251c","permalink":"https://rycolab.github.io/authors/clara/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/clara/","section":"authors","summary":"Clara recently started her PhD with Ryan at ETH Z√ºrich. She received her B.S. and M.S. from Stanford University in Computational and Mathematical Engineering. Her research interests include neural machine translation and robustness in AI-based systems.","tags":null,"title":"Clara Meister","type":"authors"},{"authors":["damian"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"915cffd0a6d0547272831667bc5c6bb1","permalink":"https://rycolab.github.io/authors/damian/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/damian/","section":"authors","summary":"","tags":null,"title":"Dami√°n Blasi","type":"authors"},{"authors":["edo"],"categories":null,"content":"Edoardo is a prospective postdoctoral fellow at Mila in Montreal, Canada. He is expected to complete his Ph.D. at the University of Cambridge in September 2020. In Cambridge, he was affiliated with St John\u0026rsquo;s College and supervised by Prof. Anna Korhonen and Ivan Vuliƒá. As an undergrad, he studied modern literature at the University of Pavia, Italy. He works closely with Ryan. He previously interned as an AI/ML researcher at Apple in Cupertino. His research has been supported by a Google Research Faculty Award and an ERC PoC grant for projects co-written with his supervisors. His research focuses on few-shot multilingual learning and on text-based commonsense reasoning.\nNative Language: Italian\nAnimal Form: Water bear (Tardigrade)\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"0d9d7c8635f56e85ab0ca3b09a1442fb","permalink":"https://rycolab.github.io/authors/edo/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/edo/","section":"authors","summary":"Edoardo is a prospective postdoctoral fellow at Mila in Montreal, Canada. He is expected to complete his Ph.D. at the University of Cambridge in September 2020. In Cambridge, he was affiliated with St John\u0026rsquo;s College and supervised by Prof.","tags":null,"title":"Edoardo M. Ponti","type":"authors"},{"authors":["kat"],"categories":null,"content":"Ekaterina Vylomova is a Postdoctoral Fellow at the University of Melbourne. She holds a PhD in Computer Science obtained from the University of Melbourne. Her research is focused on compositionality modelling for morphology, models for derivational morphology, neural machine translation, and diachronic language modeling.\nNative Language: Russian\nAnimal Form: Mimus polyglottos\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"77b0a8f9ddff4ab3e5c7552c687132ed","permalink":"https://rycolab.github.io/authors/kat/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/kat/","section":"authors","summary":"Ekaterina Vylomova is a Postdoctoral Fellow at the University of Melbourne. She holds a PhD in Computer Science obtained from the University of Melbourne. Her research is focused on compositionality modelling for morphology, models for derivational morphology, neural machine translation, and diachronic language modeling.","tags":null,"title":"Ekaterina Vylomova","type":"authors"},{"authors":["eleftheria"],"categories":null,"content":"Eleftheria is a first-year PhD student in Computer Science at ETH Zurich. She will be co-supervised by Severin Klinger and Ryan and is associated with ETH Zurich‚Äôs Media Technology Center. Previously, she was a Research Engineer at Disney Research where she worked on natural language understanding and emotion classification. Eleftheria received an MSc in Artificial Intelligence with specialization in NLP from the University of Edinburgh where her thesis was on machine translation. Before that, she received a BA in English Language and Literature with specialization in Linguistics from the University of Athens. At the moment, her research focuses on multilingual text summarization. Research-wise, she is also interested in the application of NLP and ML to the study of mass media. Outside of research, Eleftheria likes tea, TvTropes, wholesome memes, and playing soundtracks by ear on the piano.\nNative Language: Greek\nAnimal Form: House cat\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"ca5fef1fb2e39756e74f4ff22d43656f","permalink":"https://rycolab.github.io/authors/eleftheria/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/eleftheria/","section":"authors","summary":"Eleftheria is a first-year PhD student in Computer Science at ETH Zurich. She will be co-supervised by Severin Klinger and Ryan and is associated with ETH Zurich‚Äôs Media Technology Center. Previously, she was a Research Engineer at Disney Research where she worked on natural language understanding and emotion classification.","tags":null,"title":"Eleftheria Tsipidi","type":"authors"},{"authors":["liz"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fc7f296399b51eca9f04506227a81c4e","permalink":"https://rycolab.github.io/authors/liz/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/liz/","section":"authors","summary":"","tags":null,"title":"Elizabeth Salesky","type":"authors"},{"authors":["ema"],"categories":null,"content":"Emanuele is a first-year PhD student at the University of Copenhagen, supervised by Desmond Elliott and closely collaborating with Ryan. His research is funded through a Marie Sk≈Çodowska-Curie TALENT fellowship and focuses on language grounding from multimodal and multilingual contexts. He also likes neural machine translation, typology and ramen.\nBefore his PhD, Emanuele received his master‚Äôs degree from the School of Computer and Communication Sciences at EPFL, where he was a member of the Data Science Lab led by Robert West. He also holds two bachelor‚Äôs degrees in Information Engineering from Tongji University and in Telecommunications Engineering from Politecnico di Torino.\nNative Language: Italian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bbe9321070c19534ca5cfe036a68e756","permalink":"https://rycolab.github.io/authors/ema/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ema/","section":"authors","summary":"Emanuele is a first-year PhD student at the University of Copenhagen, supervised by Desmond Elliott and closely collaborating with Ryan. His research is funded through a Marie Sk≈Çodowska-Curie TALENT fellowship and focuses on language grounding from multimodal and multilingual contexts.","tags":null,"title":"Emanuele Bugliarello","type":"authors"},{"authors":["irene"],"categories":null,"content":"Irene is an MPhil student at the University of Cambridge, where she is supervised by Ryan. She received her BSc in Computer Science with a minor in mathematics and statistics from the University of Helsinki. Her primary interests are machine learning and anything in the intersection of computer science and statistics. During her free time she likes running (away from responsibilities) and correcting people who say that Finland is part of Scandinavia.\nNative Language: Finnish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89af9b35ecd8250fa5b24fe3dfe7d186","permalink":"https://rycolab.github.io/authors/irene/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/irene/","section":"authors","summary":"Irene is an MPhil student at the University of Cambridge, where she is supervised by Ryan. She received her BSc in Computer Science with a minor in mathematics and statistics from the University of Helsinki.","tags":null,"title":"Irene Nikkarinen","type":"authors"},{"authors":["isabelle"],"categories":null,"content":"Isabelle is an associate professor at the University of Copenhagen, Department of Computer Science and works in the general areas of Statistical Natural Language Processing and Machine Learning\nAnimal Form: Owl\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"76b480c4a09bc036d7176e3210f31a66","permalink":"https://rycolab.github.io/authors/isabelle/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/isabelle/","section":"authors","summary":"Isabelle is an associate professor at the University of Copenhagen, Department of Computer Science and works in the general areas of Statistical Natural Language Processing and Machine Learning\nAnimal Form: Owl","tags":null,"title":"Isabelle Augenstein","type":"authors"},{"authors":["jen"],"categories":null,"content":"Jennifer is a first-year PhD student at the University of Cambridge, supervised by Ryan. She earned an MMathPhys in Mathematics and Physics at the University of Warwick and an MPhil in Advanced Computer Science at the University of Cambridge where she was also supervised by Ryan. She is interested in techniques for low-resource NLP, gender bias in NLP and annotation of linguistic data, among other things. Her background is in Algebraic Geometry, so she is always happy to get a chance to put her mathematical skills to good use. In her spare time she enjoys learning languages, reading, eating chicken katsu curry and travelling as much as possible.\nNative Language: English\nAnimal Form: Otter\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"5d240d57e5fd9bed1d26eb181ebe2842","permalink":"https://rycolab.github.io/authors/jen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jen/","section":"authors","summary":"Jennifer is a first-year PhD student at the University of Cambridge, supervised by Ryan. She earned an MMathPhys in Mathematics and Physics at the University of Warwick and an MPhil in Advanced Computer Science at the University of Cambridge where she was also supervised by Ryan.","tags":null,"title":"Jennifer C. White","type":"authors"},{"authors":["jiaoda"],"categories":null,"content":"Jiaoda is a Master student in Data Science at ETH Z√ºrich. Prior to that, he was an undergraduate student in City University of Hong Kong, majoring in Electronic and Communication Engineering. He is currently working on network pruning for his Master‚Äôs Thesis, supervised by Ryan and Mrinmaya Sachan. He is interested in natural language understanding.\nNative Language: Mandarin\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2f5d73b305f66523f6084d93add0fbdf","permalink":"https://rycolab.github.io/authors/jiaoda/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jiaoda/","section":"authors","summary":"Jiaoda is a Master student in Data Science at ETH Z√ºrich. Prior to that, he was an undergraduate student in City University of Hong Kong, majoring in Electronic and Communication Engineering.","tags":null,"title":"Jiaoda Li","type":"authors"},{"authors":["tiago"],"categories":null,"content":"Josef is a second-year PhD student at the University of Cambridge supervised by Simone Teufel and Ryan. Before joining Rycolab, he completed the MPhil in Advanced Computer Science at the University of Cambridge. Before that, he obtained a Bachelor of Law at the University of Exeter. He is interested in legal document information retrieval. In his spare time he likes to boulder.\nNative Language: Czech\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"9d3fdfe8719fded59b20776fb11d2964","permalink":"https://rycolab.github.io/authors/josef/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/josef/","section":"authors","summary":"Josef is a second-year PhD student at the University of Cambridge supervised by Simone Teufel and Ryan. Before joining Rycolab, he completed the MPhil in Advanced Computer Science at the University of Cambridge.","tags":null,"title":"Josef Valvoda","type":"authors"},{"authors":["jun"],"categories":null,"content":"Jun is a video game programmer at Electric Square. They are also interested in computational linguistics and completed their MPhil in Advanced Computer Science at the University of Cambridge in June 2020. Prior to this, they studied Computer Science at the University of Southern California. In another life, Jun would like to be either a musician or a whale.\nNative Languages: English and Cantonese\nAnimal Form: Orca\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7d339d8f30d803efeaeb243a5fe6af73","permalink":"https://rycolab.github.io/authors/jun/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/jun/","section":"authors","summary":"Jun is a video game programmer at Electric Square. They are also interested in computational linguistics and completed their MPhil in Advanced Computer Science at the University of Cambridge in June 2020.","tags":null,"title":"Jun Yen Leung","type":"authors"},{"authors":["karolina"],"categories":null,"content":"Karolina is a first-year PhD student at the University of Copenhagen, co-advised by Isabelle Augenstein and Ryan. Previously, she received a BSc in Economics with a major in Statistics and Econometrics and completed an MSc in Statistics, both from the Humboldt University of Berlin. Besides, prior to starting her PhD she has worked as a data science consultant. Her primary research interests are bias and fairness in NLP, interpretability and statistical methods. In her spare time she enjoys knitting warm socks and sweaters, and learning languages, both very useful for her Ph.D.-related move to Denmark.\nNative Language: Polish\nAnimal Form: Hummingbird\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"89a3e957bf68ea773109c28337cfefd1","permalink":"https://rycolab.github.io/authors/karolina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/karolina/","section":"authors","summary":"Karolina is a first-year PhD student at the University of Copenhagen, co-advised by Isabelle Augenstein and Ryan. Previously, she received a BSc in Economics with a major in Statistics and Econometrics and completed an MSc in Statistics, both from the Humboldt University of Berlin.","tags":null,"title":"Karolina Sta≈Ñczak","type":"authors"},{"authors":["lucas"],"categories":null,"content":"Lucas completed the MPhil in Advanced Computer Science at the University of Cambridge where he earned a distinction. He wrote his MPhil thesis with Ryan on probing pre-trained neural language models for morpho-syntax. Previously, he was a BSc Artificial Intelligence and Computer Science student at the University of Edinburgh where he worked with Shay Cohen. Currently, he is working in the intersection of NLP and education, thinking broadly about AI, and applying to graduate schools to pursue a PhD in NLP!\nNative Languages: Spanish and Portuguese\nAnimal Form: Llama\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"f4eeb2051886e69b374435203391df51","permalink":"https://rycolab.github.io/authors/lucas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/lucas/","section":"authors","summary":"Lucas completed the MPhil in Advanced Computer Science at the University of Cambridge where he earned a distinction. He wrote his MPhil thesis with Ryan on probing pre-trained neural language models for morpho-syntax.","tags":null,"title":"Lucas Torroba Hennigen","type":"authors"},{"authors":["mans"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"bcd9a2ac5f3d13f165b3f46243c9f942","permalink":"https://rycolab.github.io/authors/mans/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/mans/","section":"authors","summary":"","tags":null,"title":"Mans Hulden","type":"authors"},{"authors":["marinela"],"categories":null,"content":"Marinela started her PhD in October 2019 in the Language Technology Lab at the University of Cambridge under the supervision of Prof. Anna Korhonen and Ryan. She is a member of Trinity College and is funded by Trinity Overseas Bursary.\nBefore starting her PhD, Marinela obtained MSc and BSc degrees in Mathematics, both from the University of Belgrade in Serbia.\nNative Language: Serbian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"7cbd738aeab27bddcfebd1c698fee92d","permalink":"https://rycolab.github.io/authors/marinela/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/marinela/","section":"authors","summary":"Marinela started her PhD in October 2019 in the Language Technology Lab at the University of Cambridge under the supervision of Prof. Anna Korhonen and Ryan. She is a member of Trinity College and is funded by Trinity Overseas Bursary.","tags":null,"title":"Marinela Paroviƒá","type":"authors"},{"authors":["martina"],"categories":null,"content":"Martina is writing her Master\u0026rsquo;s thesis at ETH Z√ºrich, supervised by Clara and Ryan. Her academic interests are neural machine translation, morphology and cross-lingual learning. Also, she likes learning new languages and eating lots of chocolate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"b08b9d0bebfd9788e31d72c0fa38ce6f","permalink":"https://rycolab.github.io/authors/martina/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/martina/","section":"authors","summary":"Martina is writing her Master\u0026rsquo;s thesis at ETH Z√ºrich, supervised by Clara and Ryan. Her academic interests are neural machine translation, morphology and cross-lingual learning. Also, she likes learning new languages and eating lots of chocolate.","tags":null,"title":"Martina Forster","type":"authors"},{"authors":["marvin"],"categories":null,"content":"Marvin is a third-year undergraduate student in Computer Science at ETH Z√ºrich. In his free time he enjoys studying languages.\nNative Languages: German and English\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"56cfa73f08ee4a1e0b15d0ce5dbbbf59","permalink":"https://rycolab.github.io/authors/marvin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/marvin/","section":"authors","summary":"Marvin is a third-year undergraduate student in Computer Science at ETH Z√ºrich. In his free time he enjoys studying languages.\nNative Languages: German and English","tags":null,"title":"Marvin Jarju","type":"authors"},{"authors":["admin"],"categories":null,"content":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.\nLorem ipsum dolor sit amet, consectetur adipiscing elit. Sed neque elit, tristique placerat feugiat ac, facilisis vitae arcu. Proin eget egestas augue. Praesent ut sem nec arcu pellentesque aliquet. Duis dapibus diam vel metus tempus vulputate.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://rycolab.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Nelson Bighetti is a professor of artificial intelligence at the Stanford AI Lab. His research interests include distributed robotics, mobile computing and programmable matter. He leads the Robotic Neurobiology group, which develops self-reconfiguring robots, systems of self-organizing robots, and mobile sensor networks.","tags":null,"title":"Nelson Bighetti","type":"authors"},{"authors":["niklas"],"categories":null,"content":"Niklas is a doctoral student fascinated by interdisciplinary questions in Computational Social Science and Applied NLP. After completing a MSc in Data Science at UCL, he worked at the interface of these fields at the IBM AI Core Team, Microsoft Research Cambridge and the German Federal Foreign Office in Shanghai. During his BSc in Information Management Systems at TU Berlin, Niklas spent 6 months at the University of Oxford and 12 months at Tsinghua University in Beijing funded by the German Academic Scholarship Foundation / Studienstiftung.\nNative Language: German\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"3980a18dc1184eb009e3646396b5d2a3","permalink":"https://rycolab.github.io/authors/niklas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/niklas/","section":"authors","summary":"Niklas is a doctoral student fascinated by interdisciplinary questions in Computational Social Science and Applied NLP. After completing a MSc in Data Science at UCL, he worked at the interface of these fields at the IBM AI Core Team, Microsoft Research Cambridge and the German Federal Foreign Office in Shanghai.","tags":null,"title":"Niklas Stoehr","type":"authors"},{"authors":["paula"],"categories":null,"content":"Paula is a third year PhD student at the University of Cambridge, where she is advised by Prof Ann Copestake and Ryan. She is supported by the Vice-Chancellor\u0026rsquo;s and Selwyn College Scholarship. Prior to starting her PhD she did the BSc in Computer Science at the University of St Andrews and completed the MPhil in Advanced Computer Science at Cambridge. During those degrees she was generously supported by the G. D. Fahrenheit Scholarship, awarded by the City Council of Gda≈Ñsk.\nHer main research interests include cross-lingual learning and computational approaches to linguistic typology and morphology. She is fascinated by how world‚Äôs languages differ in their means of encoding meaning and hopes that investigating those differences, alongside language similarities can facilitate building better, less biased NLP systems.\nNative Language: Polish\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"74eae12507749d073766d39a5b14ab4e","permalink":"https://rycolab.github.io/authors/paula/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/paula/","section":"authors","summary":"Paula is a third year PhD student at the University of Cambridge, where she is advised by Prof Ann Copestake and Ryan. She is supported by the Vice-Chancellor\u0026rsquo;s and Selwyn College Scholarship.","tags":null,"title":"Paula Czarnowska","type":"authors"},{"authors":["ran"],"categories":null,"content":"Ran is a second-year PhD student at the University of Cambridge advised by Ryan. Previously, Ran completed the Computer Science Tripos at the University of Cambridge and continued on to Part III where he was also advised by Ryan. He is primarily interested in research regarding algorithms in NLP and parsing, and is picking up new interests as he goes along. While he waits for his systems to train he likes to play tennis, lacrosse and do stand-up comedy.\nNative Language: Hebrew\nAnimal Form: Honey badger\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"6eba7e88ca7bbc6ce8119664e70023ed","permalink":"https://rycolab.github.io/authors/ran/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ran/","section":"authors","summary":"Ran is a second-year PhD student at the University of Cambridge advised by Ryan. Previously, Ran completed the Computer Science Tripos at the University of Cambridge and continued on to Part III where he was also advised by Ryan.","tags":null,"title":"Ran Zmigrod","type":"authors"},{"authors":["rowan"],"categories":null,"content":"Rowan is a first-year PhD at the University of Cambridge. He previously completed his BA and MEng, both in Computer Science, jointly supervised by Ryan and Simone Teufel. He previously worked on gender-bias mitigation from machine-learnt semantic models, and is currently working on conceptual metaphor. He, too, has things he does in his free time.\nNative Language: English\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"d3750635a3b8746ef003d5679b039abb","permalink":"https://rycolab.github.io/authors/rowan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/rowan/","section":"authors","summary":"Rowan is a first-year PhD at the University of Cambridge. He previously completed his BA and MEng, both in Computer Science, jointly supervised by Ryan and Simone Teufel. He previously worked on gender-bias mitigation from machine-learnt semantic models, and is currently working on conceptual metaphor.","tags":null,"title":"Rowan Hall Maudslay","type":"authors"},{"authors":["ryan"],"categories":null,"content":"I was born and raised in the city of Baltimore, Maryland‚Äîthe greatest city in America. But you don‚Äôt have to take my word for it, it‚Äôs spray-painted on the city‚Äôs benches: I completed my undergraduate degree at Johns Hopkins University (also in Baltimore, Maryland) in Cognitive Science (with focal areas in Linguistics and Computational Methods) under the tutelage of Colin Wilson. I was then recruited by Jason Eisner to do a Ph.D. in Computer Science at Johns Hopkins University where I was a member of the Center for Language and Speech Processing; I also took quite a few courses in mathematics, statistics and linguistics along the way. I am currently a tenure-track assistant professor at ETH Z√ºrich in the Department of Computer Science where I am a member of the Institut f√ºr maschinelles Lernen. I was previously a Lecturer (that‚Äôs British for tenure-track assistant professor) at the Computer Laboratory (the oldest computer science department in the world) at the University of Cambridge in the United Kingdom where I am still affiliated. I have also done research stints at Google AI, Facebook AI Research and the Ludwig Maximilian University of Munich.\nI am curious about most things: Simone Teufel calls me a ‚Äúresearch hippie‚Äù‚ÄîI‚Äôve never been quite sure what that means, though. It is my greatest dream to achieve near-native competence in an agglutinative language, but here‚Äôs hoping. Recent scholarly work that bears my name may be perused on my lab‚Äôs publication list.\nNote: I am not taking students at the moment‚Äîmy lab has reached critical mass. I spend roughly 3‚Äì4 hours a week with my primary (and secondary) advisees and a similar amount of time with many of my close collaborators as well. I am fresh out of cycles üòû\nNote Note: Despite the above note, please feel free to email anyway‚Äîabout anything. But, if you do email me, please address me by my first name: Ryan. I strongly disprefer academic titles and find them horridly elitist‚Äîto wit, please don\u0026rsquo;t ever call me Dr. Cotterell, Professor Cotterell, or (since I am in a German-speaking land) Professor Dr. Cotterell. How you address me in your email is a great shibboleth for whether you\u0026rsquo;ve actually read my page üòâ\nFamily on the internet: My uncle John Cotterell is a painter; check him out here. My cousin Jake Velker does amazing work at the One Acre Fund. My parents (Thomas Cotterell and Gina Scarinzi) are not on the internet, but here‚Äôs a recent picture of us in London, England: Animal Form: Wug\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"65d4d588cef9fb8886b394cbe1e95b85","permalink":"https://rycolab.github.io/authors/ryan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/ryan/","section":"authors","summary":"I was born and raised in the city of Baltimore, Maryland‚Äîthe greatest city in America. But you don‚Äôt have to take my word for it, it‚Äôs spray-painted on the city‚Äôs benches: I completed my undergraduate degree at Johns Hopkins University (also in Baltimore, Maryland) in Cognitive Science (with focal areas in Linguistics and Computational Methods) under the tutelage of Colin Wilson.","tags":null,"title":"Ryan Cotterell","type":"authors"},{"authors":["selena"],"categories":null,"content":"Selena is a second -year MSc student in Computer Science at ETH Zurich. Previously, she received her BSc from University of Novi Sad in Serbia. Currently, she is working on a research project with Ryan on cognitively plausible morphological inflection models. Apart from that, she is interested in theoretical computer science and in her spare time enjoys playing bridge.\nNative Language: Serbian\nNative Language: Serbian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"19bd63ab7c994c5a2aea5fd2b3cecab4","permalink":"https://rycolab.github.io/authors/selena/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/selena/","section":"authors","summary":"Selena is a second -year MSc student in Computer Science at ETH Zurich. Previously, she received her BSc from University of Novi Sad in Serbia. Currently, she is working on a research project with Ryan on cognitively plausible morphological inflection models.","tags":null,"title":"Selena Pepiƒá","type":"authors"},{"authors":["shijie"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"be2d1967e64e4ed029b3e9f673940c75","permalink":"https://rycolab.github.io/authors/shijie/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/shijie/","section":"authors","summary":"","tags":null,"title":"Shijie Wu","type":"authors"},{"authors":["simone"],"categories":null,"content":"","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"561fdecbc989c73616aba426ed25861c","permalink":"https://rycolab.github.io/authors/simone/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/simone/","section":"authors","summary":"","tags":null,"title":"Simone Teufel","type":"authors"},{"authors":["stefan"],"categories":null,"content":"Stefan holds an MPhil degree in Advanced Computer Science from the University of Cambridge and BA degrees in Mathematics and Computer Science from the American University in Bulgaria. His interests include explainability of deep learning models, neural machine translation, and natural language generation. He likes spending his free time with friends and whenever he does not have free time, he likes working with friends.\nNative Language: Bulgarian\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"1fee0630ebf691a0b4eb76303184b35c","permalink":"https://rycolab.github.io/authors/stefan/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/stefan/","section":"authors","summary":"Stefan holds an MPhil degree in Advanced Computer Science from the University of Cambridge and BA degrees in Mathematics and Computer Science from the American University in Bulgaria. His interests include explainability of deep learning models, neural machine translation, and natural language generation.","tags":null,"title":"Stefan Lazov","type":"authors"},{"authors":["tiago"],"categories":null,"content":"Tiago is a second-year PhD student in Computer Science at the University of Cambridge, where he is supervised by Ryan. The supervision mostly consists of arguing with Ryan over research ideas, and Tiago insisting he is right. At the moment, Tiago is mainly interested in information theory and its application to the study of machine learning models and linguistics. To this end, he has recently been dabbling in information-theoretic linguistics and probing. Tiago likes when people refer to him as a ‚Äúhappy Brazilian‚Äù and he enjoys the few sunny days in the UK: He is always looking forward to summer. Before joining Cambridge for his PhD, he did his undergraduate studies in Automation and Control Engineering at the University of Brasilia\u0026mdash;a very dry city with a lovely sky\u0026mdash;and a masters in computer science at the Federal University of Minas Gerais, located in Belo Horizonte, the city with (arguably) the best food and (potentially) the most friendly people in the world. (Tiago himself is not from Belo Horizonte, though, so he is not that friendly). Tiago takes great pleasure in correcting Ryan‚Äôs grammatical mistakes when Ryan speaks to him Portuguese.\nNative Language: Portuguese\nAnimal Form: Sea turtle\n-- ","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"fdbf9f3b62c6aa81ec87c8656829ba69","permalink":"https://rycolab.github.io/authors/tiago/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tiago/","section":"authors","summary":"Tiago is a second-year PhD student in Computer Science at the University of Cambridge, where he is supervised by Ryan. The supervision mostly consists of arguing with Ryan over research ideas, and Tiago insisting he is right.","tags":null,"title":"Tiago Pimentel","type":"authors"},{"authors":["tianyu"],"categories":null,"content":"Tianyu is a first-year PhD student at ETH Zurich. He is advised by Ryan and Mrinmaya Sachan. He received his BSc in computer science from Peking University. He is currently interested in coreference resolution models. He likes reading, hiking, and playing board games in his spare time.\nNative Language: Mandarin\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"24aa23770a64f329ed6e3676915264ca","permalink":"https://rycolab.github.io/authors/tianyu/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tianyu/","section":"authors","summary":"Tianyu is a first-year PhD student at ETH Zurich. He is advised by Ryan and Mrinmaya Sachan. He received his BSc in computer science from Peking University. He is currently interested in coreference resolution models.","tags":null,"title":"Tianyu Liu","type":"authors"},{"authors":["tim"],"categories":null,"content":"Tim develops machine learning algorithms for tough problems‚Äîtending toward applications in natural language processing and programming languages. When he\u0026rsquo;s not in front of a whiteboard or computer, he\u0026rsquo;s probably climbing things, walking around on his hands, or hanging out with Hanna Wallach and @maia.the.pomsky.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"07d172c41c903101879fc5085cda4837","permalink":"https://rycolab.github.io/authors/tim/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/tim/","section":"authors","summary":"Tim develops machine learning algorithms for tough problems‚Äîtending toward applications in natural language processing and programming languages. When he\u0026rsquo;s not in front of a whiteboard or computer, he\u0026rsquo;s probably climbing things, walking around on his hands, or hanging out with Hanna Wallach and @maia.","tags":null,"title":"Tim Vieira","type":"authors"},{"authors":["martina"],"categories":null,"content":"Yuchen (Eleanor) is a first-year Ph.D. student in Computer Science at ETH Z√ºrich, where she is supervised by Ryan and Mrinmaya Sachan. Previously, she was an undergraduate student at Zhejiang University, majoring in Biomedical Engineering and minoring in Computer Science. Her research is focused on coreference resolution and linguistic theories of coherence and coreference. She also likes commonsense, neural machine translation, and hiking. It is her greatest dream to travel all 195 countries in the world.\nNative Language: Mandarin\nAnimal Form: Penguin\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"52d7ad1914cb73f076ef2e373686b91d","permalink":"https://rycolab.github.io/authors/yuchen/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/yuchen/","section":"authors","summary":"Yuchen (Eleanor) is a first-year Ph.D. student in Computer Science at ETH Z√ºrich, where she is supervised by Ryan and Mrinmaya Sachan. Previously, she was an undergraduate student at Zhejiang University, majoring in Biomedical Engineering and minoring in Computer Science.","tags":null,"title":"Yuchen Jiang","type":"authors"},{"authors":["Francisco Vargas Palomo","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"4e809dbd8e961190569218cdabca525b","permalink":"https://rycolab.github.io/publication/palomocotterell-emnlp-20/","publishdate":"2020-09-26T06:11:20.620056Z","relpermalink":"/publication/palomocotterell-emnlp-20/","section":"publication","summary":"","tags":null,"title":"Exploring the Linear Subspace Hypothesis in Gender Bias Mitigation","type":"publication"},{"authors":["Clara Meister","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"00b67b122cbde7f1fd036ec9b7fcfec9","permalink":"https://rycolab.github.io/publication/meisteral-emnlp-20/","publishdate":"2020-09-26T06:11:17.609213Z","relpermalink":"/publication/meisteral-emnlp-20/","section":"publication","summary":"","tags":null,"title":"If Beam Search is the Answer, What was the Question?","type":"publication"},{"authors":["Lucas Torroba Hennigen","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"e0cf51125dafd47351851f86c166e2a8","permalink":"https://rycolab.github.io/publication/hennigenal-emnlp-20/","publishdate":"2020-09-26T06:11:18.631682Z","relpermalink":"/publication/hennigenal-emnlp-20/","section":"publication","summary":"","tags":null,"title":"Intrinsic Probing through Dimension Selection","type":"publication"},{"authors":["Jun Yen Leung","Guy Emerson","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"e215838e46b34bea56143b0e9d9d9ff7","permalink":"https://rycolab.github.io/publication/leungal-emnlp-20/","publishdate":"2020-09-26T06:11:19.113624Z","relpermalink":"/publication/leungal-emnlp-20/","section":"publication","summary":"","tags":null,"title":"Investigating Cross-Linguistic Adjective Ordering Tendencies with a Latent-Variable Model","type":"publication"},{"authors":["Arya D. McCarthy","Adina Williams","Shijia Liu","David Yarowsky","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"7ca684e99b099ca2409ebd8614e13a38","permalink":"https://rycolab.github.io/publication/mccarthyal-emnlp-20/","publishdate":"2020-09-26T06:11:21.618727Z","relpermalink":"/publication/mccarthyal-emnlp-20/","section":"publication","summary":"","tags":null,"title":"Measuring the Similarity of Grammatical Gender Systems by Comparing Partitions","type":"publication"},{"authors":["Tiago Pimentel","Naomi Saphra","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"ccd37a475ea91c7644701517d10adf20","permalink":"https://rycolab.github.io/publication/pimentelal-emnlp-20/","publishdate":"2020-09-26T06:11:18.11097Z","relpermalink":"/publication/pimentelal-emnlp-20/","section":"publication","summary":"","tags":null,"title":"Pareto Probing: Trading Off Accuracy for Simplicity","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"ef7a081c8b13acb03471ad23994af5e7","permalink":"https://rycolab.github.io/publication/zmigrodal-emnlp-20/","publishdate":"2020-09-26T06:11:20.114842Z","relpermalink":"/publication/zmigrodal-emnlp-20/","section":"publication","summary":"","tags":null,"title":"Please Mind the Root: Decoding Arborescences for Dependency Parsing","type":"publication"},{"authors":["Tiago Pimentel","Rowan Hall Maudslay","Dami√°n Blasi","Ryan Cotterell"],"categories":null,"content":"","date":1604188800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1604188800,"objectID":"1dea2434a38fd242a587b610d625d717","permalink":"https://rycolab.github.io/publication/pimentel-2-al-emnlp-20/","publishdate":"2020-09-26T06:11:19.62574Z","relpermalink":"/publication/pimentel-2-al-emnlp-20/","section":"publication","summary":"","tags":null,"title":"Speakers Fill Semantic Gaps with Context","type":"publication"},{"authors":["Elizabeth Salesky","Eleanor Chodroff","Tiago Pimentel","Matthew Wiesner","Ryan Cotterell","Alan W Black","Jason Eisner"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"630bf245688581fe745a2fc35fde7c29","permalink":"https://rycolab.github.io/publication/saleskyal-acl-20/","publishdate":"2020-09-26T06:11:15.517102Z","relpermalink":"/publication/saleskyal-acl-20/","section":"publication","summary":"A major hurdle in data-driven research on typology is having sufficient data in many languages to draw meaningful conclusions. We present VoxClamantis v1.0, the first large-scale corpus for phonetic typology, with aligned segments and estimated phoneme-level labels in 690 readings spanning 635 languages, along with acoustic-phonetic measures of vowels and sibilants. Access to such data can greatly facilitate investigation of phonetic typology at a large scale and across many languages. However, it is non-trivial and computationally intensive to obtain such alignments for hundreds of languages, many of which have few to no resources presently available. We describe the methodology to create our corpus, discuss caveats with current methods and their impact on the utility of this data, and illustrate possible research directions through a series of case studies on the 48 highest-quality readings. Our corpus and scripts are publicly available for non-commercial use at https://voxclamantisproject.github.io.","tags":null,"title":"A Corpus for Large-Scale Phonetic Typology","type":"publication"},{"authors":["Rowan Hall Maudslay","Josef Valvoda","Tiago Pimentel","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"553515b076cfd79cf6ac3caab7b3fc42","permalink":"https://rycolab.github.io/publication/hall-maudslayal-acl-20/","publishdate":"2020-09-26T06:11:14.589848Z","relpermalink":"/publication/hall-maudslayal-acl-20/","section":"publication","summary":"Measuring what linguistic information is encoded in continuous representations of language has become a popular area of research. To do this, researchers train \"probes\"‚Äî supervised models designed to extract linguistic structure from embeddings. The line between what constitutes a probe and a model designed to achieve a particular task is often blurred. To fully understand what we are learning about the target language representation‚Äîor the instrument with which we performing measurement with for that matter‚Äîwe would do well to compare probes to classic parsers. As a case study, we consider the structural probe (Hewitt and Manning, 2019), designed to quantify the presence of syntactic information. We create a simple parser that improves upon the performance of the structural probe by 11.4% on UUAS, despite having an identical lightweight parameterization. Under a second less common metric, however, the structural probe outperforms traditional parsers. This begs the question: why should some metrics be preferred for probing and others for parsing?","tags":null,"title":"A Tale of a Probe and a Parser","type":"publication"},{"authors":["Clara Meister","Elizabeth Salesky","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"65df8ea6f2757b720fbb48bafcdc38a4","permalink":"https://rycolab.github.io/publication/meisteral-acl-20/","publishdate":"2020-09-26T06:11:17.065321Z","relpermalink":"/publication/meisteral-acl-20/","section":"publication","summary":"Prior work has explored directly regularizing the output distributions of probabilistic models to alleviate peaky (i.e. over-confident) predictions, a common sign of overfitting. This class of techniques, of which label smoothing is one, has a connection to entropy regularization. Despite the consistent success of label smoothing across architectures and data sets in language generation tasks, two problems remain open: (1) there is little understanding of the underlying effects entropy regularizers have on models, and (2) the full space of entropy regularization techniques is largely unexplored. We introduce a parametric family of entropy regularizers, which includes label smoothing as a special case, and use it to gain a better understanding of the relationship between the entropy of a model and its performance on language generation tasks. We also find that variance in model performance can be explained largely by the resulting entropy of the model. Lastly, we find that label smoothing provably does not allow for sparsity in an output distribution, an undesirable property for language generation models, and therefore advise the use of other entropy regularization methods in its place. Our code is available online at https://github.com/rycolab/entropyRegularization.","tags":null,"title":"Generalized Entropy Regularization or: There's Nothing Special about Label Smoothing","type":"publication"},{"authors":["Tiago Pimentel","Josef Valvoda","Rowan Hall Maudslay","Ran Zmigrod","Adina Williams","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"96c6d62c85e714712e9a3cd7dfcf94c8","permalink":"https://rycolab.github.io/publication/pimentelal-acl-20/","publishdate":"2020-09-26T06:11:14.100776Z","relpermalink":"/publication/pimentelal-acl-20/","section":"publication","summary":"The success of neural networks on a diverse set of NLP tasks has led researchers to question how much these networks actually know about natural language. Probes are a natural way of assessing this. When probing, a researcher chooses a linguistic task and trains a supervised model to predict annotations in that linguistic task from the network's learned representations. If the probe does well, the researcher may conclude that the representations encode knowledge related to the task.  A commonly held belief is that using simpler models as probes is better; the logic is that simpler models will identify linguistic structure, but not learn the task itself. We propose an information-theoretic formalization of probing as estimating mutual information that contradicts this received wisdom: one should always select the highest performing probe one can, even if it is more complex, since it will result in a tighter estimate, and thus reveal more of the linguistic information inhering in the contextualized representation. The empirical portion of our paper focuses on obtaining tight estimates for how much information BERT knows about both parts of speech and dependency labels, evaluating it in a set of ten typologically diverse languages often under-represented in parsing research, plus English, totalling eleven languages.  We find BERT only accounts for more information about parts of speech than a traditional type-based word embedding in five of the eleven analysed languages. When we look at dependency labels, BERT does improve upon type-based embeddings in all analysed languages, but accounting for at most 12% more information.","tags":null,"title":"Information-Theoretic Probing for Linguistic Structure","type":"publication"},{"authors":["Emanuele Bugliarello","Sabrina J. Mielke","Antonios Anastasopoulos","Ryan Cotterell","Naoaki Okazaki"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"10a46a0454ae8759bd3c1cf7f3c4f89a","permalink":"https://rycolab.github.io/publication/bugliarelloal-acl-20/","publishdate":"2020-09-26T06:11:16.019165Z","relpermalink":"/publication/bugliarelloal-acl-20/","section":"publication","summary":"The performance of neural machine translation systems is commonly evaluated in terms of BLEU. However, due to its reliance on target language properties and generation, the BLEU metric does not allow an assessment of which translation directions are more difficult to model. In this paper, we propose cross-mutual information (XMI): an asymmetric information-theoretic metric of machine translation difficulty that exploits the probabilistic nature of most neural machine translation models. XMI allows us to better evaluate the difficulty of translating text into the target language while controlling for the difficulty of the target-side generation component independent of the translation task. We then present the first systematic and controlled study of cross-lingual translation difficulties using modern neural translation systems. Code for replicating our experiments is available online at https://github.com/e-bug/nmt-difficulty.","tags":null,"title":"It's Easier to Translate out of English than into it: Measuring Neural Translation Difficulty by Cross-Mutual Information","type":"publication"},{"authors":["Adina Williams","Tiago Pimentel","Arya McCarthy","Hagen Blix","Eleanor Chodroff","Ryan Cotterell"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"628b1f074b060a453a0afee5af15adce","permalink":"https://rycolab.github.io/publication/williamsal-acl-20/","publishdate":"2020-09-26T06:11:15.048901Z","relpermalink":"/publication/williamsal-acl-20/","section":"publication","summary":"The noun lexica of many natural languages are divided into several declension classes with characteristic morphological properties. Class membership is far from deterministic, but the phonological form of a noun and/or its meaning can often provide imperfect clues. Here, we investigate the strength of those clues. More specifically, we operationalize this by measuring how much information, in bits, we can glean about declension class from knowing the form and/or meaning of nouns. We know that form and meaning are often also indicative of grammatical gender \u0026mdash; which, as we quantitatively verify, can itself share information with declension class \u0026mdash; so we also control for gender. We find for two Indo-European languages (Czech and German) that form and meaning respectively share significant amounts of information with class (and contribute additional information above and beyond gender). The three-way interaction between class, form, and meaning (given gender) is also significant. Our study is important for two reasons: First, we introduce a new method that provides additional quantitative support for a classic linguistic finding that form and meaning are relevant for the classification of nouns into declensions. Secondly, we show not only that individual declensions classes vary in the strength of their clues within a language, but also that these variations themselves vary across languages. The code is publicly available at https://github.com/rycolab/declension-mi.","tags":null,"title":"Predicting Declension Class from Form and Meaning","type":"publication"},{"authors":["Alexander Erdmann","Micha Elsner","Shijie Wu","Ryan Cotterell","Nizar Habash"],"categories":null,"content":"","date":1593561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1593561600,"objectID":"05b7c75176d48aa04a24594196d68eb3","permalink":"https://rycolab.github.io/publication/erdmannal-acl-20/","publishdate":"2020-09-26T06:11:16.563254Z","relpermalink":"/publication/erdmannal-acl-20/","section":"publication","summary":"This work treats the paradigm discovery problem (PDP)‚Äîthe task of learning an inflectional morphological system from unannotated sentences. We formalize the PDP and develop evaluation metrics for judging systems. Using currently available resources, we construct datasets for the task. We also devise a heuristic benchmark for the PDP and report empirical results on five diverse languages. Our benchmark system first makes use of word embeddings and string similarity to cluster forms by cell and by paradigm. Then, we bootstrap a neural transducer on top of the clustered data to predict words to realize the empty paradigm slots. An error analysis of our system suggests clustering by cell across different inflection classes is the most pressing challenge for future work. Our code and data are available at https://github.com/alexerdmann/ParadigmDiscovery.","tags":null,"title":"The Paradigm Discovery Problem","type":"publication"},{"authors":["Clara Meister","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"a583d975602b3880816be059c3edceb7","permalink":"https://rycolab.github.io/publication/meisteral-tacl-20/","publishdate":"2020-09-26T06:11:23.212743Z","relpermalink":"/publication/meisteral-tacl-20/","section":"publication","summary":"Decoding for many NLP tasks requires a heuristic algorithm for approximating exact search since the full search space is often intractable if not simply too large to traverse efficiently. The default algorithm for this job is beam search---a pruned version of breadth-first search---which in practice, returns better results than exact inference due to beneficial search bias. In this work, we show that standard beam search is a computationally inefficient choice for many decoding tasks; specifically, when the scoring function is a monotonic function in sequence length, other search algorithms can be used to reduce the number of calls to the scoring function (e.g., a neural network), which is often the bottleneck computation. We propose best-first beam search, an algorithm that provably returns the same set of results as standard beam search, albeit in the minimum number of scoring function calls to guarantee optimality (modulo beam size).  We show that best-first beam search can be used with length normalization and mutual information decoding, among other rescoring functions.  Lastly, we propose a memory-reduced variant of best-first beam search, which has a similar search bias in terms of downstream performance, but runs in a fraction of the time.","tags":null,"title":"Best-First Beam Search","type":"publication"},{"authors":["Ran Zmigrod","Tim Vieira","Ryan Cotterell"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"bc982261ed6c38ce3a82eb15d72e7917","permalink":"https://rycolab.github.io/publication/zmigrodal-tacl-20/","publishdate":"2020-09-26T06:11:23.743062Z","relpermalink":"/publication/zmigrodal-tacl-20/","section":"publication","summary":"","tags":null,"title":"Efficient Computation of Expectations under Spanning Tree Distributions","type":"publication"},{"authors":["Adina Williams","Ryan Cotterell","Lawrence Wolf-Sonkin","Damian Blasi","Hanna Wallach"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f81a6754c8d2a554ad1123f73967933b","permalink":"https://rycolab.github.io/publication/williamsal-tacl-20/","publishdate":"2020-09-26T06:11:22.629471Z","relpermalink":"/publication/williamsal-tacl-20/","section":"publication","summary":"We use large-scale corpora in six different gendered languages, along with tools from NLP and information theory, to test whether there is a relationship between the grammatical genders of inanimate nouns and the adjectives used to describe those nouns. For all six languages, we find that there is a statistically significant relationship. We also find that there are statistically significant relation- ships between the grammatical genders of inanimate nouns and the verbs that take those nouns as direct objects, as indirect objects, and as subjects. We defer a deeper investiga- tion of these relationships for future work.","tags":null,"title":"On the Relationships Between the Grammatical Genders of Inanimate Nouns and Their Co-Occurring Adjectives and Verbs","type":"publication"},{"authors":["Edoardo M Ponti","Ivan Vuliƒá","Ryan Cotterell","Marinela Parovic","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"f528c7fc98df3361f0980ff1d1da54e3","permalink":"https://rycolab.github.io/publication/pontial-tacl-20/","publishdate":"2020-09-26T06:11:24.530425Z","relpermalink":"/publication/pontial-tacl-20/","section":"publication","summary":"","tags":null,"title":"Parameter Space Factorization for Zero-Shot Learning across Tasks and Languages","type":"publication"},{"authors":["Tiago Pimentel","Brian Roark","Ryan Cotterell"],"categories":null,"content":"","date":1577836800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1577836800,"objectID":"fc0a72e5e26865b4a0609143cb55a02c","permalink":"https://rycolab.github.io/publication/pimentelal-tacl-20/","publishdate":"2020-09-26T06:11:22.080978Z","relpermalink":"/publication/pimentelal-tacl-20/","section":"publication","summary":"We present methods for calculating a measure of phonotactic complexity‚Äîbits per phoneme‚Äîthat permits a straightforward cross-linguistic comparison. When given a word, represented as a sequence of phonemic segments such as symbols in the international phonetic alphabet, and a statistical model trained on a sample of word types from the language, we can approximately measure bits per phoneme using the negative log-probability of that word under the model. This simple measure allows us to compare the entropy across languages, giving insight into how complex a language‚Äôs phonotactics are. Using a collection of 1016 basic concept words across 106 languages, we demonstrate a very strong negative correlation of ‚àí0.74 between bits per phoneme and the average length of words.","tags":null,"title":"Phonotactic Complexity and its Trade-offs","type":"publication"},{"authors":["Paula Czarnowska","Sebastian Ruder","Edouard Grave","Ryan Cotterell","Ann Copestake"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"296b08393c2f8c8422cb74edca0e8730","permalink":"https://rycolab.github.io/publication/czarnowskaal-emnlp-ijcnlp-19/","publishdate":"2020-09-03T06:45:35.329776Z","relpermalink":"/publication/czarnowskaal-emnlp-ijcnlp-19/","section":"publication","summary":"Human translators routinely have to translate rare inflections of words--due to the Zipfian distribution of words in a language. When translating from Spanish, a good translator would have no problem identifying the proper translation of a statistically rare inflection such as habl√°ramos. Note the lexeme itself, hablar, is relatively common. In this work, we investigate whether state-of-the-art bilingual lexicon inducers are capable of learning this kind of generalization. We introduce 40 morphologically complete dictionaries in 10 languages and evaluate three of the best performing models on the task of translation of less frequent morphological forms. We demonstrate that the performance of state-of-the-art models drops considerably when evaluated on infrequent morphological inflections and then show that adding a simple morphological constraint at training time improves the performance, proving that the bilingual lexicon inducers can benefit from better encoding of morphology.","tags":null,"title":"Don‚Äôt Forget the Long Tail! A Comprehensive Analysis of Morphological Generalization in Bilingual Lexicon Induction","type":"publication"},{"authors":["Pei Zhou","Weijia Shi","Jieyu Zhao","Kuan-Hao Huang","Muhao Chen","Ryan Cotterell","Kai-Wei Chang"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"25f9a7f3afd4937aa991707ae9d06c12","permalink":"https://rycolab.github.io/publication/zhoual-emnlp-ijcnlp-19/","publishdate":"2020-09-03T06:45:35.830208Z","relpermalink":"/publication/zhoual-emnlp-ijcnlp-19/","section":"publication","summary":"Recent studies have shown that word embeddings exhibit gender bias inherited from the training corpora. However, most studies to date have focused on quantifying and mitigating such bias only in English. These analyses cannot be directly extended to languages that exhibit morphological agreement on gender, such as Spanish and French. In this paper, we propose new metrics for evaluating gender bias in word embeddings of these languages and further demonstrate evidence of gender bias in bilingual embeddings which align these languages with English. Finally, we extend an existing approach to mitigate gender bias in word embedding of these languages under both monolingual and bilingual settings. Experiments on modified Word Embedding Association Test, word similarity, word translation, and word pair translation tasks show that the proposed approaches can effectively reduce the gender bias while preserving the utility of the original embeddings.","tags":null,"title":"Examining Gender Bias in Languages with Grammatical Gender","type":"publication"},{"authors":["Rowan Hall Maudslay","Hila Gonen","Ryan Cotterell","Simone Teufel"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"74d11d07da4859c3b2be113fbd5adbda","permalink":"https://rycolab.github.io/publication/hall-maudslayal-emnlp-ijcnlp-19/","publishdate":"2020-09-03T06:45:33.634685Z","relpermalink":"/publication/hall-maudslayal-emnlp-ijcnlp-19/","section":"publication","summary":"This paper treats gender bias latent in word embeddings. Previous mitigation attempts rely on the operationalisation of gender bias as a projection over a linear subspace. An alternative approach is Counterfactual Data Augmentation (CDA), in which a corpus is duplicated and augmented to remove bias, e.g. by swapping all inherently-gendered words in the copy. We perform an empirical comparison of these approaches on the English Gigaword and Wikipedia, and find that whilst both successfully reduce direct bias and perform well in tasks which quantify embedding quality, CDA variants outperform projection-based methods at the task of drawing non-biased gender analogies by an average of 19% across both corpora. We propose two improvements to CDA: Counterfactual Data Substitution (CDS), a variant of CDA in which potentially biased text is randomly substituted to avoid duplication, and the Names Intervention, a novel name-pairing technique that vastly increases the number of words being treated. CDA/S with the Names Intervention is the only approach which is able to mitigate indirect gender bias: following debiasing, previously biased words are significantly less clustered according to gender (cluster purity is reduced by 49%), thus improving on the state-of-the-art for bias mitigation.","tags":null,"title":"It‚Äôs All in the Name: Mitigating Gender Bias with Name-Based Counterfactual Data Substitution","type":"publication"},{"authors":["Adina Williams","Ryan Cotterell","Lawrence Wolf-Sonkin","Damian Blasi","Hanna Wallach"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"b7f65d5b01c4b86c378d10100edac08d","permalink":"https://rycolab.github.io/publication/williamsal-emnlp-ijcnlp-19/","publishdate":"2020-09-03T06:45:34.284426Z","relpermalink":"/publication/williamsal-emnlp-ijcnlp-19/","section":"publication","summary":"Many of the world's languages employ grammatical gender on the lexeme. For instance, in Spanish, house ''casa'' is feminine, whereas the word for paper ''papel'' is masculine. To a speaker of a genderless language, this categorization seems to exist with neither rhyme nor reason. But, is the association of nouns to gender classes truly arbitrary? In this work, we present the first large-scale investigation of the arbitrariness of gender assignment that uses canonical correlation analysis as a method for correlating the gender of inanimate nouns with their lexical semantic meaning. We find that the gender systems of 18 languages exhibit a significant correlation with an externally grounded definition of lexical semantics.","tags":null,"title":"Quantifying the Semantic Core of Gender Systems","type":"publication"},{"authors":["Edoardo Maria Ponti","Ivan Vuliƒá","Ryan Cotterell","Roi Reichart","Anna Korhonen"],"categories":null,"content":"","date":1572566400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572566400,"objectID":"bd8665dc8c25e8141fa3a624c5fd0955","permalink":"https://rycolab.github.io/publication/pontial-emnlp-ijcnlp-19/","publishdate":"2020-09-03T06:45:34.800843Z","relpermalink":"/publication/pontial-emnlp-ijcnlp-19/","section":"publication","summary":"Can we construct a neural language model which is inductively biased towards learning human language? Motivated by this question, we aim at constructing an informative prior for held-out languages on the task of character-level, open-vocabulary language modelling. We obtain this prior as the posterior over network weights conditioned on the data from a sample of training languages, which is approximated through Laplace's method. Based on a large and diverse sample of languages, the use of our prior outperforms baseline models with an uninformative prior in both zero-shot and few-shot settings, showing that the prior is imbued with universal linguistic knowledge. Moreover, we harness broad language-specific information available for most languages of the world, i.e., features from typological databases, as distant supervision for held-out languages. We explore several language modelling conditioning techniques, including concatenation and meta-networks for parameter generation. They appear beneficial in the few-shot setting, but ineffective in the zero-shot setting. Since the paucity of even plain digital text affects the majority of the world's languages, we hope that these insights will broaden the scope of applications for language technology.","tags":null,"title":"Towards Zero-Shot Language Modeling","type":"publication"},{"authors":["Arya D. McCarthy","Ekaterina Vylomova","Shijie Wu","Chaitanya Malaviya","Lawrence Wolf-Sonkin","Garrett Nicolai","Christo Kirov","Miikka Silfverberg","Sabrina Mielke","Jeffrey Heinz","Ryan Cotterell","Mans Hulden"],"categories":null,"content":"","date":1564617600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1564617600,"objectID":"da6c1c8cf0e58894242c01b498a5fdd8","permalink":"https://rycolab.github.io/publication/mccarthyal-tacl-19/","publishdate":"2020-09-03T06:45:33.116395Z","relpermalink":"/publication/mccarthyal-tacl-19/","section":"publication","summary":"The SIGMORPHON 2019 shared task on cross-lingual transfer and contextual analysis in morphology examined transfer learning of inflection between 100 language pairs, as well as contextual lemmatization and morphosyntactic description in 66 languages. The first task evolves past years' inflection tasks by examining transfer of morphological inflection knowledge from a high-resource language to a low-resource language. This year also presents a new second challenge on lemmatization and morphological feature analysis in context. All submissions featured a neural component and built on either this year's strong baselines or highly ranked systems from previous years' shared tasks. Every participating team improved in accuracy over the baselines for the inflection task (though not Levenshtein distance), and every team in the contextual analysis task improved on both state-of-the-art neural and non-neural baselines.","tags":null,"title":"The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection","type":"publication"},{"authors":["Ran Zmigrod","Sabrina Mielke","Hanna Wallach","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"02c577382eddda3a8c17afd5facc6144","permalink":"https://rycolab.github.io/publication/zmigrodal-acl-19/","publishdate":"2020-09-03T06:45:28.581899Z","relpermalink":"/publication/zmigrodal-acl-19/","section":"publication","summary":"Gender stereotypes are manifest in most of the world's languages and are consequently propagated or amplified by NLP systems. Although research has focused on mitigating gender stereotypes in English, the approaches that are commonly employed produce ungrammatical sentences in morphologically rich languages. We present a novel approach for converting between masculine-inflected and feminine-inflected sentences in such languages. For Spanish and Hebrew, our approach achieves F1 scores of 82% and 73% at the level of tags and accuracies of 90% and 87% at the level of forms. By evaluating our approach using four different languages, we show that, on average, it reduces gender stereotyping by a factor of 2.5 without any sacrifice to grammaticality.","tags":null,"title":"Counterfactual Data Augmentation for Mitigating Gender Bias in Languages with Rich Morphology","type":"publication"},{"authors":["Shijie Wu","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"7547a9341238e368f994d0782bb943a4","permalink":"https://rycolab.github.io/publication/wucotterell-acl-19/","publishdate":"2020-09-03T06:45:26.957926Z","relpermalink":"/publication/wucotterell-acl-19/","section":"publication","summary":"Many common character-level, string-to-string transduction tasks, e.g., grapheme-to-phoneme conversion and morphological inflection, consist almost exclusively of monotonic transduction. Neural sequence-to-sequence models with soft attention, non-monotonic models, outperform popular monotonic models. In this work, we ask the following question: Is monotonicity really a helpful inductive bias in these tasks? We develop a hard attention sequence-to-sequence model that enforces strict monotonicity and learns alignment jointly. With the help of dynamic programming, we are able to compute the exact marginalization over all alignments. Our models achieve state-of-the-art performance on morphological inflection. Furthermore, we find strong performance on two other character-level transduction tasks. Code is available at https://github.com/shijie-wu/neural-transducer.","tags":null,"title":"Exact Hard Monotonic Attention for Character-Level Transduction","type":"publication"},{"authors":["Tiago Pimentel","Arya McCarthy","Damian Blasi","Brian Roark","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"2aec702aa08c71f0b3cfeb2cc63e90ba","permalink":"https://rycolab.github.io/publication/pimentelal-acl-19/","publishdate":"2020-09-03T06:45:27.4854Z","relpermalink":"/publication/pimentelal-acl-19/","section":"publication","summary":"A longstanding debate in semiotics centers on the relationship between linguistic signs and their corresponding semantics: is there an arbitrary relationship between a word form and its meaning, or does some systematic phenomenon pervade? For instance, does the character bigram `gl‚Ä≤ have any systematic relationship to the meaning of words like `glisten‚Ä≤, `gleam‚Ä≤ and `glow‚Ä≤? In this work, we offer a holistic quantification of the systematicity of the sign using mutual information and recurrent neural networks. We employ these in a data-driven and massively multilingual approach to the question, examining 106 languages. We find a statistically significant reduction in entropy when modeling a word form conditioned on its semantic representation. Encouragingly, we also recover well-attested English examples of systematic affixes. We conclude with the meta-point: Our approximate effect size (measured in bits) is quite small---despite some amount of systematicity between form and meaning, an arbitrary relationship and its resulting benefits dominate human language.","tags":null,"title":"Meaning to Form: Measuring Systematicity as Information","type":"publication"},{"authors":["Shijie Wu","Ryan Cotterell","Timothy J. O'Donnell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"19ada103f8247e8e1a50fb490f672367","permalink":"https://rycolab.github.io/publication/wual-acl-19/","publishdate":"2020-09-03T06:45:28.026169Z","relpermalink":"/publication/wual-acl-19/","section":"publication","summary":"We present a study of morphological irregularity. Following recent work, we define an information-theoretic measure of irregularity based on the predictability of forms in a language. Using a neural transduction model, we estimate this quantity for the forms in 28 languages. We first present several validatory and exploratory analyses of irregularity. We then show that our analyses provide evidence for a correlation between irregularity and frequency: higher frequency items are more likely to be irregular and irregular items are more likely be highly frequent. To our knowledge, this result is the first of its breadth and confirms longstanding proposals from the linguistics literature. The correlation is more robust when aggregated at the level of whole paradigms---providing support for models of linguistic structure in which inflected forms are unified by abstract underlying stems or lexemes.","tags":null,"title":"Measuring Morphological Irregularity","type":"publication"},{"authors":["Damian Blasi","Ryan Cotterell","Lawrence Wolf-Sonkin","Sabine Stoll","Balthasar Bickel","Marco Baroni"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"93639342f841bc5e61c57825210d6019","permalink":"https://rycolab.github.io/publication/blasial-acl-19/","publishdate":"2020-09-03T06:45:26.474843Z","relpermalink":"/publication/blasial-acl-19/","section":"publication","summary":"Embedding a clause inside another (``the girl [who likes cars [that run fast]] has arrived‚Ä≥) is a fundamental resource that has been argued to be a key driver of linguistic expressiveness. As such, it plays a central role in fundamental debates on what makes human language unique, and how they might have evolved. Empirical evidence on the prevalence and the limits of embeddings has however been based on either laboratory setups or corpus data of relatively limited size. We introduce here a collection of large, dependency-parsed written corpora in 17 languages, that allow us, for the first time, to capture clausal embedding through dependency graphs and assess their distribution. Our results indicate that there is no evidence for hard constraints on embedding depth: the tail of depth distributions is heavy. Moreover, although deeply embedded clauses tend to be shorter, suggesting processing load issues, complex sentences with many embeddings do not display a bias towards less deep embeddings. Taken together, the results suggest that deep embeddings are not disfavoured in written language. More generally, our study illustrates how resources and methods from latest-generation big-data NLP can provide new perspectives on fundamental questions in theoretical linguistics.","tags":null,"title":"On the distribution of deep clausal embeddings: A large cross-linguistic study","type":"publication"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"84cc0a9e2fb24ebde73ba952790fd562","permalink":"https://rycolab.github.io/publication/bjervaal-acl-19/","publishdate":"2020-09-03T06:45:26.012799Z","relpermalink":"/publication/bjervaal-acl-19/","section":"publication","summary":"The study of linguistic typology is rooted in the implications we find between linguistic features, such as the fact that languages with object-verb word ordering tend to have postpositions. Uncovering such implications typically amounts to time-consuming manual processing by trained and experienced linguists, which potentially leaves key linguistic universals unexplored. In this paper, we present a computational model which successfully identifies known universals, including Greenberg universals, but also uncovers new ones, worthy of further linguistic investigation. Our approach outperforms baselines previously used for this problem, as well as a strong baseline from knowledge base population.","tags":null,"title":"Uncovering Typological Implications with Belief Nets","type":"publication"},{"authors":["Alexander M. Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Isabelle Augenstein","Ryan Cotterell"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"88feb9579b10f90d0805e8ee27e849a8","permalink":"https://rycolab.github.io/publication/hoyleal-acl-19/","publishdate":"2020-09-03T06:45:29.078553Z","relpermalink":"/publication/hoyleal-acl-19/","section":"publication","summary":"Studying the ways in which language is gendered has long been an area of interest in sociolinguistics. Studies have explored, for example, the speech of male and female characters in film and the language used to describe male and female politicians. In this paper, we aim not to merely study this phenomenon qualitatively, but instead to quantify the degree to which the language used to describe men and women is different and, moreover, different in a positive or negative way. To that end, we introduce a generative latent-variable model that jointly represents adjective (or verb) choice, with its sentiment, given the natural gender of a head (or dependent) noun. We find that there are significant differences between descriptions of male and female nouns and that these differences align with common gender stereotypes: Positive adjectives used to describe women are more often related to their bodies than adjectives used to describe men.","tags":null,"title":"Unsupervised Discovery of Gendered Language through Latent-Variable Modeling","type":"publication"},{"authors":["Sabrina Mielke","Ryan Cotterell","Kyle Gorman","Brian Roark","Jason Eisner"],"categories":null,"content":"","date":1561939200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561939200,"objectID":"f4b1b6ea089e83dd43526718672dc3d0","permalink":"https://rycolab.github.io/publication/mielkeal-acl-19/","publishdate":"2020-09-03T06:45:25.564021Z","relpermalink":"/publication/mielkeal-acl-19/","section":"publication","summary":"How language-agnostic are current state-of-the-art NLP tools? Are there some types of language that are easier to model with current methods? In prior work (Cotterell et al., 2018) we attempted to address this question for language modeling, and observed that recurrent neural network language models do not perform equally well over all the high-resource European languages found in the Europarl corpus. We speculated that inflectional morphology may be the primary culprit for the discrepancy. In this paper, we extend these earlier experiments to cover 69 languages from 13 language families using a multilingual Bible corpus. Methodologically, we introduce a new paired-sample multiplicative mixed-effects model to obtain language difficulty coefficients from at-least-pairwise parallel corpora. In other words, the model is aware of inter-sentence variation and can handle missing data. Exploiting this model, we show that ``translationese‚Ä≥ is not any easier to model than natively written language in a fair comparison. Trying to answer the question of what features difficult languages have in common, we try and fail to reproduce our earlier (Cotterell et al., 2018) observation about morphological complexity and instead reveal far simpler statistics of the data that seem to drive complexity in a much larger sample.","tags":null,"title":"What Kind of Language Is Hard to Language-Model?","type":"publication"},{"authors":["Johannes Bjerva","Yova Kementchedjhieva","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"5d53e7002cc0533fd0478a4bc7673df1","permalink":"https://rycolab.github.io/publication/bjervaal-naacl-19/","publishdate":"2020-09-03T06:45:30.547034Z","relpermalink":"/publication/bjervaal-naacl-19/","section":"publication","summary":"In the principles-and-parameters framework, the structural features of languages depend on parameters that may be toggled on or off, with a single parameter often dictating the status of multiple features. The implied covariance between features inspires our probabilisation of this line of linguistic inquiry---we develop a generative model of language based on exponential-family matrix factorisation. By modelling all languages and features within the same architecture, we show how structural similarities between languages can be exploited to predict typological features with near-perfect accuracy, outperforming several baselines on the task of predicting held-out features. Furthermore, we show that language embeddings pre-trained on monolingual text allow for generalisation to unobserved languages. This finding has clear practical and also theoretical implications: the results confirm what linguists have hypothesised, i.e. that there are significant correlations between typological features and languages.","tags":null,"title":"A Probabilistic Generative Model of Linguistic Typology","type":"publication"},{"authors":["Chaitanya Malaviya","Shijie Wu","Ryan Cotterell"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"7cfbe19e49c0f05cd5a7b3beaa3f894e","permalink":"https://rycolab.github.io/publication/malaviyanaal-acl-19/","publishdate":"2020-09-03T06:45:29.540713Z","relpermalink":"/publication/malaviyanaal-acl-19/","section":"publication","summary":"English verbs have multiple forms. For instance, talk may also appear as talks, talked or talking, depending on the context. The NLP task of lemmatization seeks to map these diverse forms back to a canonical one, known as the lemma. We present a simple joint neural model for lemmatization and morphological tagging that achieves state-of-the-art results on 20 languages from the Universal Dependencies corpora. Our paper describes the model in addition to training and decoding procedures. Error analysis indicates that joint morphological tagging and lemmatization is especially helpful in low-resource lemmatization and languages that display a larger degree of morphological complexity.","tags":null,"title":"A Simple Joint Model for Improved Contextual Neural Lemmatization","type":"publication"},{"authors":["Alexander Hoyle","Lawrence Wolf-Sonkin","Hanna Wallach","Ryan Cotterell","Isabelle Augenstein"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"dc6d51404dc2effc57d9764c923031ec","permalink":"https://rycolab.github.io/publication/hoyleal-naacl-19/","publishdate":"2020-09-03T06:45:31.083172Z","relpermalink":"/publication/hoyleal-naacl-19/","section":"publication","summary":"When assigning quantitative labels to a dataset, different methodologies may rely on different scales. In particular, when assigning polarities to words in a sentiment lexicon, annotators may use binary, categorical, or continuous labels. Naturally, it is of interest to unify these labels from disparate scales to both achieve maximal coverage over words and to create a single, more robust sentiment lexicon while retaining scale coherence. We introduce a generative model of sentiment lexica to combine disparate scales into a common latent representation. We realize this model with a novel multi-view variational autoencoder (VAE), called SentiVAE. We evaluate our approach via a downstream text classification task involving nine English-Language sentiment analysis datasets; our representation outperforms six individual sentiment lexica, as well as a straightforward combination thereof.","tags":null,"title":"Combining Sentiment Lexica with a Multi-View Variational Autoencoder","type":"publication"},{"authors":["Ekaterina Vylomova","Ryan Cotterell","Timothy Baldwin","Trevor Cohn","Jason Eisner"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"3f8612ffd7dc7fc048ff998b6a81e076","permalink":"https://rycolab.github.io/publication/vylomovaal-naacl-19/","publishdate":"2020-09-03T06:45:32.100143Z","relpermalink":"/publication/vylomovaal-naacl-19/","section":"publication","summary":"Critical to natural language generation is the production of correctly inflected text. In this paper, we isolate the task of predicting a fully inflected sentence from its partially lemmatized version. Unlike traditional morphological inflection or surface realization, our task input does not provide ''gold'' tags that specify what morphological features to realize on each lemmatized word; rather, such features must be inferred from sentential context. We develop a neural hybrid graphical model that explicitly reconstructs morphological features before predicting the inflected forms, and compare this to a system that directly predicts the inflected forms without relying on any morphological annotation. We experiment on several typologically diverse languages from the Universal Dependencies treebanks, showing the utility of incorporating linguistically-motivated latent variables into NLP models.","tags":null,"title":"Contextualization of Morphological Inflection","type":"publication"},{"authors":["Jieyu Zhao","Tianlu Wang","Mark Yatskar","Ryan Cotterell","Vicente Ordonez","Kai-Wei Chang"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"07cd7213afdbf397a75bca00c039c3ee","permalink":"https://rycolab.github.io/publication/zhoual-naacl-19/","publishdate":"2020-09-03T06:45:31.597165Z","relpermalink":"/publication/zhoual-naacl-19/","section":"publication","summary":"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","tags":null,"title":"Gender Bias in Contextualized Word Embeddings","type":"publication"},{"authors":["Shijia Liu","Adina Williams","Hongyuan Mei","Ryan Cotterell"],"categories":null,"content":"","date":1559347200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559347200,"objectID":"06e41617c9ae911725b3fc26bd63b47d","permalink":"https://rycolab.github.io/publication/liual-naacl-19/","publishdate":"2020-09-03T06:45:30.043287Z","relpermalink":"/publication/liual-naacl-19/","section":"publication","summary":"While idiosyncrasies of the Chinese classifier system have been a richly studied topic among linguists (Adams and Conklin, 1973; Erbaugh, 1986; Lakoff, 1986), not much work has been done to quantify them with statistical methods. In this paper, we introduce an information-theoretic approach to measuring idiosyncrasy; we examine how much the uncertainty in Mandarin Chinese classifiers can be reduced by knowing semantic information about the nouns that the classifiers modify. Using the empirical distribution of classifiers from the parsed Chinese Gigaword corpus (Graff et al., 2005), we compute the mutual information (in bits) between the distribution over classifiers and distributions over other linguistic quantities. We investigate whether semantic classes of nouns and adjectives differ in how much they reduce uncertainty in classifier choice, and find that it is not fully idiosyncratic; while there are no obvious trends for the majority of semantic classes, shape nouns reduce uncertainty in classifier choice the most.","tags":null,"title":"On the Idiosyncrasies of the Mandarin Chinese Classifier System","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Mans Hulden","Jason Eisner"],"categories":null,"content":"","date":1551398400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1551398400,"objectID":"4df7e71c0378509ff7f0bb1850a5990f","permalink":"https://rycolab.github.io/publication/cotterellal-tacl-19/","publishdate":"2020-09-03T06:45:32.615369Z","relpermalink":"/publication/cotterellal-tacl-19/","section":"publication","summary":"We quantify the linguistic complexity of different languages' morphological systems. We verify that there is a statistically significant empirical trade-off between paradigm size and irregularity: A language's inflectional paradigms may be either large in size or highly irregular, but never both. We define a new measure of paradigm irregularity based on the conditional entropy of the surface realization of a paradigm---how hard it is to jointly predict all the word forms in a paradigm from the lemma. We estimate irregularity by training a predictive model. Our measurements are taken on large morphological paradigms from 36 typologically diverse languages.","tags":null,"title":"On the Complexity and Typology of Inflectional Morphological Systems","type":"publication"},{"authors":["Sebastian Ruder$^*$","Ryan Cotterell$^*$","Yova Kementchedjhieva","Anders S√∏gaard"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"2f067d72799894553dc0f72584bb1252","permalink":"https://rycolab.github.io/publication/ruderal-emnlp-18/","publishdate":"2020-03-13T16:20:59.654298Z","relpermalink":"/publication/ruderal-emnlp-18/","section":"publication","summary":"We introduce a novel discriminative latent-variable model for the task of bilingual lexicon induction. Our model combines the bipartite matching dictionary prior of Haghighi et al. (2008) with a state-of-the-art embedding-based approach. To train the model, we derive an efficient Viterbi EM algorithm. We provide empirical improvements on six language pairs under two metrics and show that the prior theoretically and empirically helps to mitigate the hubness problem. We also demonstrate how previous work may be viewed as a similarly fashioned latent-variable model, albeit with a different prior.","tags":null,"title":"A Discriminative Latent-Variable Model for Bilingual Lexicon Induction","type":"publication"},{"authors":["Yova Kementchedjhieva","Sebastian Ruder","Ryan Cotterell","Anders S√∏gaard"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"12558fbbd9611f5da65ff93738691b22","permalink":"https://rycolab.github.io/publication/kementchedjhievaal-conll-18/","publishdate":"2020-03-13T16:20:58.333468Z","relpermalink":"/publication/kementchedjhievaal-conll-18/","section":"publication","summary":"Most recent approaches to bilingual dictionary induction find a linear alignment between the word vector spaces of two languages. We show that projecting the two languages onto a third, latent space, rather than directly onto each other, while equivalent in terms of expressivity, makes it easier to learn approximate alignments. Our modified approach also allows for supporting languages to be included in the alignment process, to obtain an even better performance in low resource settings.","tags":null,"title":"Generalizing Procrustes Analysis for Better Bilingual Dictionary Induction","type":"publication"},{"authors":["Shijie Wu","Pamela Shapiro","Ryan Cotterell"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"898bd87faa75ca4cc9c323a70eb39ebf","permalink":"https://rycolab.github.io/publication/wual-emnlp-18/","publishdate":"2020-03-13T16:21:00.929123Z","relpermalink":"/publication/wual-emnlp-18/","section":"publication","summary":"Character-level string-to-string transduction is an important component of various NLP tasks. The goal is to map an input string to an output string, where the strings may be of different lengths and have characters taken from different alphabets. Recent approaches have used sequence-to-sequence models with an attention mechanism to learn which parts of the input string the model should focus on during the generation of the output string. Both soft attention and hard monotonic attention have been used, but hard non-monotonic attention has only been used in other sequence modeling tasks and has required a stochastic approximation to compute the gradient. In this work, we introduce an exact, polynomial-time algorithm for marginalizing over the exponential number of non-monotonic alignments between two strings, showing that hard attention models can be viewed as neural reparameterizations of the classical IBM Model 1. We compare soft and hard non-monotonic attention experimentally and find that the exact algorithm significantly improves performance over the stochastic approximation and outperforms soft attention.","tags":null,"title":"Hard Non-Monotonic Attention for Character-Level Transduction","type":"publication"},{"authors":["Arya D. McCarthy","Miikka Silfverberg","Ryan Cotterell","Mans Hulden","David Yarowsky"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"a150607e03d1cc2b9cc4a3a8da3fad7a","permalink":"https://rycolab.github.io/publication/mccarthyal-udw-18/","publishdate":"2020-03-13T16:21:06.861396Z","relpermalink":"/publication/mccarthyal-udw-18/","section":"publication","summary":"The Universal Dependencies (UD) and Universal Morphology (UniMorph) projects each present schemata for annotating the morphosyntactic details of language. Each project also provides corpora of annotated text in many languages---UD at the token level and UniMorph at the type level. As each corpus is built by different annotators, language-specific decisions hinder the goal of universal schemata. With compatibility of tags, each project's annotations could be used to validate the other's. Additionally, the availability of both type- and token-level resources would be a boon to tasks such as parsing and homograph disambiguation. To ease this interoperability, we present a deterministic mapping from Universal Dependencies v2 features into the UniMorph schema. We validate our approach by lookup in the UniMorph corpora and find a macro-average of 64.13% recall. We also note incompatibilities due to paucity of data on either side. Finally, we present a critical evaluation of the foundations, strengths, and weaknesses of the two annotation projects.","tags":null,"title":"Marrying Universal Dependencies and Universal Morphology","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","G√©raldine Walther","Ekaterina Vylomova","Arya D. McCarthy","Katharina Kann","Sabrina Mielke","Garrett Nicolai","Miikka Silfverberg","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1538352000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1538352000,"objectID":"3af314661f7a50d23092c996ac3531de","permalink":"https://rycolab.github.io/publication/cotterellal-conll-18/","publishdate":"2020-03-13T16:20:58.899131Z","relpermalink":"/publication/cotterellal-conll-18/","section":"publication","summary":"The CoNLL-SIGMORPHON 2018 shared task on supervised learning of morphological generation featured data sets from 103 typologically diverse languages. Apart from extending the number of languages involved in earlier supervised tasks of generating inflected forms, this year the shared task also featured a new second task which asked participants to inflect words in sentential context, similar to a cloze task. This second task featured seven languages. Task 1 received 27 submissions and task 2 received 6 submissions. Both tasks featured a low, medium, and high data condition. Nearly all submissions featured a neural component and built on highly-ranked systems from the earlier 2017 shared task. In the inflection task (task 1), 41 of the 52 languages present in last year‚Äôs inflection task showed improvement by the best systems in the low-resource setting. The cloze task (task 2) proved to be difficult, and few submissions managed to consistently improve upon both a simple neural baseline system and a lemmarepeating baseline.","tags":null,"title":"The CoNLL--SIGMORPHON 2018 Shared Task: Universal Morphological Reinflection","type":"publication"},{"authors":["Lawrence Wolf-Sonkin$^*$","Jason Naradowsky$^*$","Sabrina J. Mielke$^*$","Ryan Cotterell$^*$"],"categories":null,"content":"","date":1530403200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530403200,"objectID":"81c65001211ead5c4bae2ff60e6a56c7","permalink":"https://rycolab.github.io/publication/wolf-sonkin-acl-18/","publishdate":"2020-03-13T16:20:56.16673Z","relpermalink":"/publication/wolf-sonkin-acl-18/","section":"publication","summary":"Statistical morphological inflectors are typically trained on fully supervised, type-level data. One remaining open research question is the following: How can we effectively exploit raw, token-level data to improve their performance? To this end, we introduce a novel generative latent-variable model for the semi-supervised learning of inflection generation. To enable posterior inference over the latent variables, we derive an efficient variational inference procedure based on the wake-sleep algorithm. We experiment on 23 languages, using the Universal Dependencies corpora in a simulated low-resource setting, and find improvements of over 10% absolute accuracy in some cases.","tags":null,"title":"A Structured Variational Autoencoder for Contextual Morphological Inflection","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"3ae1326e83220b4c8f5d6afadaa7e6a8","permalink":"https://rycolab.github.io/publication/cotterelleisner-naacl-18/","publishdate":"2020-03-13T16:21:03.020539Z","relpermalink":"/publication/cotterelleisner-naacl-18/","section":"publication","summary":"What makes some types of languages more probable than others? For instance, we know that almost all spoken languages contain the vowel phoneme /i/; why should that be? The field of linguistic typology seeks to answer these questions and, thereby, divine the mechanisms that underlie human language. In our work, we tackle the problem of vowel system typology, i.e., we propose a generative probability model of which vowels a language contains. In contrast to previous work, we work directly with the acoustic information---the first two formant values---rather than modeling discrete sets of symbols from the international phonetic alphabet. We develop a novel generative probability model and report results on over 200 languages.","tags":null,"title":"A Deep Generative Model of Vowel Formant Typology","type":"publication"},{"authors":["Ryan Cotterell","Sabrina J. Mielke","Jason Eisner","Brian Roark"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"d436f65260058640e7930be367a0e8cb","permalink":"https://rycolab.github.io/publication/cotterellal-naacl-18-a/","publishdate":"2020-03-13T16:21:04.186643Z","relpermalink":"/publication/cotterellal-naacl-18-a/","section":"publication","summary":"For general modeling methods applied to diverse languages, a natural question is: how well should we expect our models to work on languages with differing typological profiles? In this work, we develop an evaluation framework for fair cross-linguistic comparison of language models, using translated text so that all models are asked to predict approximately the same information. We then conduct a study on 21 languages, demonstrating that in some languages, the textual expression of the information is harder to predict with both n-gram and LSTM language models. We show complex inflectional morphology to be a cause of performance differences among languages.","tags":null,"title":"Are All Languages Equally Hard to Language-Model?","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Sabrina J. Mielke","Jason Eisner"],"categories":null,"content":"","date":1527811200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1527811200,"objectID":"9ee0d90db50ad67fb3060518130a3c42","permalink":"https://rycolab.github.io/publication/cotterellal-naacl-18-b/","publishdate":"2020-03-13T16:21:04.74082Z","relpermalink":"/publication/cotterellal-naacl-18-b/","section":"publication","summary":"Lexical ambiguity makes it difficult to compute useful statistics of a corpus. A given word form might represent any of several morphological feature bundles. One can, however, use unsupervised learning (as in EM) to fit a model that probabilistically disambiguates word forms. We present such an approach, which employs a neural network to smoothly model a prior distribution over feature bundles (even rare ones). Although this basic model does not consider a token's context, that very property allows it to operate on a simple list of unigram type counts, partitioning each count among different analyses of that unigram. We discuss evaluation metrics for this novel task and report results on 5 languages.","tags":null,"title":"Unsupervised Disambiguation of Syncretism in Inflected Lexicons","type":"publication"},{"authors":["Christo Kirov","Ryan Cotterell","John Sylak-Glassman","G√©raldine Walther","Ekaterina Vylomova","Patrick Xia","Manaal Faruqui","Sabrina Mielke","Arya McCarthy","Sandra K√ºbler","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1525132800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525132800,"objectID":"89cfa7b59e28d9a972a4e49489e18e08","permalink":"https://rycolab.github.io/publication/kiroval-lrec-18/","publishdate":"2020-03-13T16:21:02.270668Z","relpermalink":"/publication/kiroval-lrec-18/","section":"publication","summary":"The Universal Morphology (UniMorph) project is a collaborative effort to improve how NLP handles complex morphology across the world‚Äôs languages. The project releases annotated morphological data using a universal tagset, the UniMorph schema. Each inflected form is associated with a lemma, which typically carries its underlying lexical meaning, and a bundle of morphological features from our schema. Additional supporting data and tools are also released on a per-language basis when available. UniMorph is based at the Center for Language and Speech Processing (CLSP) at Johns Hopkins University in Baltimore, Maryland. This paper details advances made to the collection, annotation, and dissemination of project resources since the initial UniMorph release described at LREC 2016.","tags":null,"title":"UniMorph 2.0: Universal Morphology","type":"publication"},{"authors":["Ryan Cotterell","Julia Kreutzer"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"8f9f94018a73e2ff526a65aeedc1d3ec","permalink":"https://rycolab.github.io/publication/cotterellkreutzer-arxiv-18/","publishdate":"2020-03-13T16:20:56.598395Z","relpermalink":"/publication/cotterellkreutzer-arxiv-18/","section":"publication","summary":"","tags":null,"title":"Explaining and Generalizing Back-Translation through Wake-Sleep","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1030c6c628ef205a3f67724e7d809180","permalink":"https://rycolab.github.io/publication/cotterellschuetze-tacl-18/","publishdate":"2020-03-13T16:21:05.340228Z","relpermalink":"/publication/cotterellschuetze-tacl-18/","section":"publication","summary":"Much like sentences are composed of words, words themselves are composed of smaller units. For example, the English word questionably can be analyzed as question+able+ly. However, this structural decomposition of the word does not directly give us a semantic representation of the word‚Ä≤s meaning. Since morphology obeys the principle of compositionality, the semantics of the word can be systematically derived from the meaning of its parts. In this work, we propose a novel probabilistic model of word formation that captures both the analysis of a word w into its constituent segments and the synthesis of the meaning of w from the meanings of those segments. Our model jointly learns to segment words into morphemes and compose distributional semantic vectors of those morphemes. We experiment with the model on English CELEX data and German DErivBase (Zeller et al., 2013) data. We show that jointly modeling semantics increases both segmentation accuracy and morpheme F1 by between 3% and 5%. Additionally, we investigate different models of vector composition, showing that recurrent neural networks yield an improvement over simple additive models. Finally, we study the degree to which the representations correspond to a linguist‚Ä≤s notion of morphological productivity.","tags":null,"title":"Joint Semantic Synthesis and Morphological Analysis of the Derived Word","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","Mans Hulden","Jason Eisner"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"1fa421ac33a261ae3d663c5a02cdb76a","permalink":"https://rycolab.github.io/publication/cotterellal-arxiv-18/","publishdate":"2020-03-13T16:20:57.089434Z","relpermalink":"/publication/cotterellal-arxiv-18/","section":"publication","summary":"","tags":null,"title":"On the Diachronic Stability of Irregularity in Inflectional Morphology","type":"publication"},{"authors":["Christo Kirov","Ryan Cotterell"],"categories":null,"content":"","date":1514764800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1514764800,"objectID":"15717ae85e8cb20882e5431a78d9e060","permalink":"https://rycolab.github.io/publication/kirovcotterell-tacl-18/","publishdate":"2020-03-13T16:21:07.99972Z","relpermalink":"/publication/kirovcotterell-tacl-18/","section":"publication","summary":"Can advances in NLP help advance cognitive modeling? We examine the role of artificial neural networks, the current state of the art in many common NLP tasks, by returning to a classic case study. In 1986, Rumelhart and McClelland famously introduced a neural architecture that learned to transduce English verb stems to their past tense forms. Shortly thereafter in 1988, Pinker and Prince presented a comprehensive rebuttal of many of Rumelhart and McClelland's claims. Much of the force of their attack centered on the empirical inadequacy of the Rumelhart and McClelland model. Today, however, that model is severely outmoded. We show that the Encoder-Decoder network architectures used in modern NLP systems obviate most of Pinker and Prince's criticisms without requiring any simplification of the past tense mapping problem. We suggest that the empirical performance of modern networks warrants a reexamination of their utility in linguistic and cognitive modeling.","tags":null,"title":"Recurrent Neural Networks in Linguistic Theory: Revisiting Pinker and Prince (1988) and the Past Tense Debate","type":"publication"},{"authors":["Ryan Cotterell","Kevin Duh."],"categories":null,"content":"","date":1509494400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1509494400,"objectID":"1f118c7c6d7e9f331838935ea2fed573","permalink":"https://rycolab.github.io/publication/cotterellduh-ijcnlp-17/","publishdate":"2020-04-20T10:10:30.834491Z","relpermalink":"/publication/cotterellduh-ijcnlp-17/","section":"publication","summary":"Low-resource named entity recognition is still an open problem in NLP. Most state-of-the-art systems require tens of thousands of annotated sentences in order to obtain high performance. However, for most of the world's languages it is unfeasible to obtain such annotation. In this paper, we present a transfer learning scheme, whereby we train character-level neural CRFs to predict named entities for both high-resource languages and low-resource languages jointly. Learning character representations for multiple related languages allows knowledge transfer from the high-resource languages to the low-resource ones, improving F1 by up to 9.8 points.","tags":null,"title":"Low-Resource Named Entity Recognition with Cross-lingual, Character-Level Neural Conditional Random Fields","type":"publication"},{"authors":["Ryan Cotterell","Georg Heigold"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"4174111fd88ca9bd66c5290666730be9","permalink":"https://rycolab.github.io/publication/cotterellheigold-emnlp-17/","publishdate":"2020-04-20T10:10:31.302764Z","relpermalink":"/publication/cotterellheigold-emnlp-17/","section":"publication","summary":"Even for common NLP tasks, sufficient supervision is not available in many languages--morphological tagging is no exception. In the work presented here, we explore a transfer learning scheme, whereby we train character-level recurrent neural taggers to predict morphological taggings for high-resource languages and low-resource languages together. Learning joint character representations among multiple related languages successfully enables knowledge transfer from the high-resource languages to the low-resource ones.","tags":null,"title":"Cross-lingual, Character-Level Neural Morphological Tagging","type":"publication"},{"authors":["Ryan Cotterell","Ekaterina Vylomova","Huda Khayrallah","Christo Kirov","David Yarowsky"],"categories":null,"content":"","date":1504224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1504224000,"objectID":"d1c98d2aefe8457ea6296afaa5dc2204","permalink":"https://rycolab.github.io/publication/cotterellal-emnlp-17/","publishdate":"2020-04-20T10:10:31.806004Z","relpermalink":"/publication/cotterellal-emnlp-17/","section":"publication","summary":"The generation of complex derived word forms has been an overlooked problem in NLP; we fill this gap by applying neural sequence-to-sequence models to the task. We overview the theoretical motivation for a paradigmatic treatment of derivational morphology, and introduce the task of derivational paradigm completion as a parallel to inflectional paradigm completion. State-of-the-art neural models adapted from the inflection task are able to learn the range of derivation patterns, and outperform a non-neural baseline by 16.4%. However, due to semantic, historical, and lexical considerations involved in derivational morphology, future work will be needed to achieve performance parity with inflection-generating systems.","tags":null,"title":"Paradigm Completion for Derivational Morphology","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","G√©raldine Walther","Ekaterina Vylomova","Patrick Xia","Manaal Faruqui","Sandra K√ºbler","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"c09bf1108b8a9f99886af0418cb4f453","permalink":"https://rycolab.github.io/publication/cotterellal-conll-17/","publishdate":"2020-04-20T10:10:34.320503Z","relpermalink":"/publication/cotterellal-conll-17/","section":"publication","summary":"The CoNLL-SIGMORPHON 2017 shared task on supervised morphological generation required systems to be trained and tested in each of 52 typologically diverse languages. In sub-task 1, submitted systems were asked to predict a specific inflected form of a given lemma. In sub-task 2, systems were given a lemma and some of its specific inflected forms, and asked to complete the inflectional paradigm by predicting all of the remaining inflected forms. Both sub-tasks included high, medium, and low-resource conditions. Sub-task 1 received 24 system submissions, while sub-task 2 received 3 system submissions. Following the success of neural sequence-to-sequence models in the SIGMORPHON 2016 shared task, all but one of the submissions included a neural component. The results show that high performance can be achieved with small training datasets, so long as models have appropriate inductive bias or make use of additional unlabeled data or synthetic data. However, different biasing and data augmentation resulted in non-identical sets of inflected forms being predicted correctly, suggesting that there is room for future improvement.","tags":null,"title":"CoNLL--SIGMORPHON 2017 Shared Task: Universal Morphological Reinflection in 52 Languages","type":"publication"},{"authors":["Francis Ferraro","Adam Poliak","Ryan Cotterell","Benjamin Van Durme"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"087c51922a795fbc97864e39def05304","permalink":"https://rycolab.github.io/publication/ferraroal-starsem-17/","publishdate":"2020-04-20T10:10:34.797399Z","relpermalink":"/publication/ferraroal-starsem-17/","section":"publication","summary":"We study how different frame annotations complement one another when learning continuous lexical semantics. We learn the representations from a tensorized skip-gram model that consistently encodes syntactic-semantic content better, with multiple 10% gains over baselines.","tags":null,"title":"Frame-Based Continuous Lexical Semantics through Exponential Family Tensor Factorization and Semantic Proto-Roles","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"bcf915476f31940a15d842647e3f1001","permalink":"https://rycolab.github.io/publication/kannal-acl-17/","publishdate":"2020-04-20T10:10:33.815305Z","relpermalink":"/publication/kannal-acl-17/","section":"publication","summary":"We present a novel cross-lingual transfer method for paradigm completion, the task of mapping a lemma to its inflected forms, using a neural encoder-decoder model, the state of the art for the monolingual task. We use labeled data from a high-resource language to increase performance on a low-resource language. In experiments on 21 language pairs from four different language families, we obtain up to 58% higher accuracy than without transfer and show that even zero-shot and one-shot learning are possible. We further find that the degree of language relatedness strongly influences the ability to transfer morphological knowledge.","tags":null,"title":"One-Shot Neural Cross-Lingual Transfer for Paradigm Completion","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1501545600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1501545600,"objectID":"71b13a4d0772b4d54b40f69f2e93feaf","permalink":"https://rycolab.github.io/publication/cotterelleisner-acl-17/","publishdate":"2020-04-20T10:10:32.755862Z","relpermalink":"/publication/cotterelleisner-acl-17/","section":"publication","summary":"Linguistic typology studies the range of structures present in human language. The main goal of the field is to discover which sets of possible phenomena are universal, and which are merely frequent. For example, all languages have vowels, while most---but not all---languages have an /u/ sound. In this paper we present the first probabilistic treatment of a basic question in phonological typology: What makes a natural vowel inventory? We introduce a series of deep stochastic point processes, and contrast them with previous computational, simulation-based approaches. We provide a comprehensive suite of experiments on over 200 distinct languages.","tags":null,"title":"Probabilistic Typology: Deep Generative Models of Vowel Inventories","type":"publication"},{"authors":["Christo Kirov","John Sylak-Glassman","Rebecca Knowles","Ryan Cotterell","Matt Post"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"cd8133a88495b62b72d13bc2400f38a2","permalink":"https://rycolab.github.io/publication/kiroval-eacl-17/","publishdate":"2020-04-20T10:10:35.714258Z","relpermalink":"/publication/kiroval-eacl-17/","section":"publication","summary":"A traditional claim in linguistics is that all human languages are equally expressive---able to convey the same wide range of meanings. Morphologically rich languages, such as Czech, rely on overt inflectional and derivational morphology to convey many semantic distinctions. Languages with comparatively limited morphology, such as English, should be able to accomplish the same using a combination of syntactic and contextual cues. We capitalize on this idea by training a tagger for English that uses syntactic features obtained by automatic parsing to recover complex morphological tags projected from Czech. The high accuracy of the resulting model provides quantitative confirmation of the underlying linguistic hypothesis of equal expressivity, and bodes well for future improvements in downstream HLT tasks including machine translation.","tags":null,"title":"A Rich Morphological Tagger for English: Exploring the Cross-Linguistic Tradeoff Between Morphology and Syntax","type":"publication"},{"authors":["Ekaterina Vylomova","Ryan Cotterell","Timothy Baldwin","Trevor Cohn"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"88dd0bbfa4129bc86b6c49d1d0fdf629","permalink":"https://rycolab.github.io/publication/vylomovaal-eacl-17/","publishdate":"2020-04-20T10:10:38.652566Z","relpermalink":"/publication/vylomovaal-eacl-17/","section":"publication","summary":"Derivational morphology is a fundamental and complex characteristic of language. In this paper we propose a new task of predicting the derivational form of a given base-form lemma that is appropriate for a given context. We present an encoder-decoder style neural network to produce a derived form character-by-character, based on its corresponding character-level representation of the base form and the context. We demonstrate that our model is able to generate valid context-sensitive derivations from known base forms, but is less accurate under lexicon agnostic setting.","tags":null,"title":"Context-Aware Prediction of Derivational Word-forms","type":"publication"},{"authors":["Ryan Cotterell","Adam Poliak","Ben Van Durme","Jason Eisner"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"62e68751e7f01daaccf799f3031f75b2","permalink":"https://rycolab.github.io/publication/cotterellalb-eacl-17/","publishdate":"2020-04-20T10:10:37.734145Z","relpermalink":"/publication/cotterellalb-eacl-17/","section":"publication","summary":"The popular skip-gram model induces word embeddings by exploiting the signal from word-context coocurrence. We offer a new interpretation of skip-gram based on exponential family PCA-a form of matrix factorization to generalize the skip-gram model to tensor factorization. In turn, this lets us train embeddings through richer higher-order coocurrences, e.g., triples that include positional information (to incorporate syntax) or morphological information (to share parameters across related words). We experiment on 40 languages and show our model improves upon skip-gram.","tags":null,"title":"Explaining and Generalizing Skip-Gram through Exponential Family Principal Component Analysis","type":"publication"},{"authors":["Arun Kumar","Ryan Cotterell","Llu√≠s Padr√≥","Antoni Oliver"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"950429cf9d367338236f6c9801bbdeb1","permalink":"https://rycolab.github.io/publication/kumaral-eacl-17/","publishdate":"2020-04-20T10:10:39.619718Z","relpermalink":"/publication/kumaral-eacl-17/","section":"publication","summary":"The Dravidian languages are one of the most widely spoken language families in the world, yet there are very few annotated resources available to NLP researchers. To remedy this, we create DravMorph, a corpus annotated for morphological segmentation and part-of-speech. Additionally, we exploit novel features and higher-order models to set state-of-the-art results on these corpora on both tasks, beating techniques proposed in the literature by as much as 4 points in segmentation F1.","tags":null,"title":"Morphological Analysis of the Dravidian Language Family","type":"publication"},{"authors":["Ryan Cotterell","John Sylak-Glassman","Christo Kirov"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"fc54a64fe12dd9d57beea964e7ce8876","permalink":"https://rycolab.github.io/publication/cotterellala-eacl-17/","publishdate":"2020-04-20T10:10:36.720999Z","relpermalink":"/publication/cotterellala-eacl-17/","section":"publication","summary":"Many of the world's languages contain an abundance of inflected forms for each lexeme. A critical task in processing such languages is predicting these inflected forms. We develop a novel statistical model for the problem, drawing on graphical modeling techniques and recent advances in deep learning. We derive a Metropolis-Hastings algorithm to jointly decode the model. Our Bayesian network draws inspiration from principal parts morphological analysis. We demonstrate improvements on 5 languages.","tags":null,"title":"Neural Graphical Models over Strings for Principal Parts Morphological Paradigm Completion","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1491004800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1491004800,"objectID":"919462c9664194568756fd3006526dba","permalink":"https://rycolab.github.io/publication/kannal-eacl-17/","publishdate":"2020-04-20T10:10:40.526493Z","relpermalink":"/publication/kannal-eacl-17/","section":"publication","summary":"We explore the task of multi-source morphological reinflection, which generalizes the standard, single-source version. The input consists of (i) a target tag and (ii) multiple pairs of source form and source tag for a lemma. The motivation is that it is beneficial to have access to more than one source form since different source forms can provide complementary information, e.g., different stems. We further present a novel extension to the encoder-decoder recurrent neural architecture, consisting of multiple encoders, to better solve the task. We show that our new architecture outperforms single-source reinflection models and publish our dataset for multi-source morphological reinflection to facilitate future research.","tags":null,"title":"Neural Multi-Source Morphological Reinflection","type":"publication"},{"authors":["Ryan Cotterell","Arun Kumar","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"a39aaa943290d5f6face02f3d9d47983","permalink":"https://rycolab.github.io/publication/cotterellal-emnlp-16/","publishdate":"2020-04-20T10:09:57.258238Z","relpermalink":"/publication/cotterellal-emnlp-16/","section":"publication","summary":"Morphological segmentation has traditionally been modeled with non-hierarchical models, which yield flat segmentations as output. In many cases, however, proper morphological analysis requires hierarchical structure---especially in the case of derivational morphology. In this work, we introduce a discriminative joint model of morphological segmentation along with the orthographic changes that occur during word formation. To the best of our knowledge, this is the first attempt to approach discriminative segmentation with a context-free model. Additionally, we release an annotated treebank of 7454 English words with constituency parses, encouraging future research in this area.","tags":null,"title":"Morphological Segmentation Inside-Out","type":"publication"},{"authors":["Katharina Kann","Ryan Cotterell","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"1feb9c000aedb9cbb4a319955397bc86","permalink":"https://rycolab.github.io/publication/kannal-emnlp-16/","publishdate":"2020-04-20T10:09:57.660609Z","relpermalink":"/publication/kannal-emnlp-16/","section":"publication","summary":"Canonical morphological segmentation aims to divide words into a sequence of standardized segments. In this work, we propose a character-based neural encoderdecoder model for this task. Additionally, we extend our model to include morphemelevel and lexical information through a neural reranker. We set the new state of the art for the task improving previous results by up to 21% accuracy. Our experiments cover three languages: English, German and Indonesian.","tags":null,"title":"Neural Morphological Analysis: Encoding-Decoding Canonical Segments","type":"publication"},{"authors":["Tim Vieira$^*$","Ryan Cotterell$^*$","Jason Eisner"],"categories":null,"content":"","date":1477958400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1477958400,"objectID":"1906bb6181348303c0d677575abad1ce","permalink":"https://rycolab.github.io/publication/vieiraal-emnlp-16/","publishdate":"2020-04-20T10:09:58.396743Z","relpermalink":"/publication/vieiraal-emnlp-16/","section":"publication","summary":"We propose a method for learning the structure of variable-order CRFs, a more flexible variant of higher-order linear-chain CRFs. Variableorder CRFs achieve faster inference by including features for only some of the tag ngrams. Our learning method discovers the useful higher-order features at the same time as it trains their weights, by maximizing an objective that combines log-likelihood with a structured-sparsity regularizer. An active-set outer loop allows the feature set to grow as far as needed. On part-of-speech tagging in 5 randomly chosen languages from the Universal Dependencies dataset, our method of shrinking the model achieved a 2‚Äì6x speedup over a baseline, with no significant drop in accuracy.","tags":null,"title":"Speed-Accuracy Tradeoffs in Tagging with Variable-Order CRFs and Structured Sparsity","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Sch√ºtze","Jason Eisner"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"d5e258b2cfce483a0a8e30a31119cd1d","permalink":"https://rycolab.github.io/publication/cotterellal-acl-16/","publishdate":"2020-04-20T10:09:59.062781Z","relpermalink":"/publication/cotterellal-acl-16/","section":"publication","summary":"Languages with rich inflectional morphology exhibit lexical data sparsity, since the word used to express a given concept will vary with the syntactic context. For instance, each count noun in Czech has 12 forms (where English uses only singular and plural). Even in large corpora, we are unlikely to observe all inflections of a given lemma. This reduces the vocabulary coverage of methods that induce continuous representations for words from distributional corpus information. We solve this problem by exploiting existing morphological resources that can enumerate a word‚Äôs component morphemes. We present a latent variable Gaussian graphical model that allows us to extrapolate continuous representations for words not observed in the training corpus, as well as smoothing the representations provided for the observed words. The latent variables represent embeddings of morphemes, which combine to create embeddings of words. Over several languages and training sizes, our model improves the embeddings for words, when evaluated on an analogy task, skip-gram predictive accuracy, and word similarity","tags":null,"title":"Morphological Smoothing and Extrapolation of Word Embeddings","type":"publication"},{"authors":["Ryan Cotterell","Christo Kirov","John Sylak-Glassman","David Yarowsky","Jason Eisner","Mans Hulden"],"categories":null,"content":"","date":1470009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1470009600,"objectID":"0dae3427f8a795d86396e714ddca0095","permalink":"https://rycolab.github.io/publication/cotterellal-sigmorphon-16/","publishdate":"2020-04-20T10:09:59.656993Z","relpermalink":"/publication/cotterellal-sigmorphon-16/","section":"publication","summary":"The 2016 SIGMORPHON Shared Task was devoted to the problem of morphological reinflection. It introduced morphological datasets for 10 languages with diverse typological characteristics. The shared task drew submissions from 9 teams representing 11 institutions reflecting a variety of approaches to addressing supervised learning of reinflection. For the simplest task, inflection generation from lemmas, the best system averaged 95.56% exact-match accuracy across all languages, ranging from Maltese (88.99%) to Hungarian (99.30%). With the relatively large training datasets provided, recurrent neural network architectures consistently performed best‚Äîin fact, there was a significant margin between neural and non-neural approaches. The best neural approach, averaged over all tasks and languages, outperformed the best nonneural one by 13.76% absolute; on individual tasks and languages the gap in accuracy sometimes exceeded 60%. Overall, the results show a strong state of the art, and serve as encouragement for future shared tasks that explore morphological analysis and generation with varying degrees of supervision.","tags":null,"title":"The SIGMORPHON 2016 Shared Task‚ÄîMorphological Reinflection","type":"publication"},{"authors":["Ryan Cotterell","Tim Vieira","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"7fa0b09b73010847f89c913ed7de741a","permalink":"https://rycolab.github.io/publication/cotterellal-naacl-16/","publishdate":"2020-04-20T10:10:00.37066Z","relpermalink":"/publication/cotterellal-naacl-16/","section":"publication","summary":"We present a model of morphological segmentation that jointly learns to segment and restore orthographic changes, e.g., funniest 7 ‚Üí fun-y-est. We term this form of analysis canonical segmentation and contrast it with the traditional surface segmentation, which segments a surface form into a sequence of substrings, e.g., funniest 7 ‚Üí funn-i-est. We derive an importance sampling algorithm for approximate inference in the model and report experimental results on English, German and Indonesian.","tags":null,"title":"A Joint Model of Orthography and Morphological Segmentation","type":"publication"},{"authors":["Pushpendre Rastogi","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1464739200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1464739200,"objectID":"bffcb6da09c3c39cf85ea1db79e3a5b8","permalink":"https://rycolab.github.io/publication/rastogial-naacl-16/","publishdate":"2020-04-20T10:10:00.914565Z","relpermalink":"/publication/rastogial-naacl-16/","section":"publication","summary":"How should one apply deep learning to tasks such as morphological reinflection, which stochastically edit one string to get another? A recent approach to such sequence-to-sequence tasks is to compress the input string into a vector that is then used to generate the output string, using recurrent neural networks. In contrast, we propose to keep the traditional architecture, which uses a finite-state transducer to score all possible output strings, but to augment the scoring function with the help of recurrent networks. A stack of bidirectional LSTMs reads the input string from leftto-right and right-to-left, in order to summarize the input context in which a transducer arc is applied. We combine these learned features with the transducer to define a probability distribution over aligned output strings, in the form of a weighted finite-state automaton. This reduces hand-engineering of features, allows learned features to examine unbounded context in the input string, and still permits exact inference through dynamic programming. We illustrate our method on the tasks of morphological reinflection and lemmatization.","tags":null,"title":"Weighting Finite-State Transductions With Neural Context","type":"publication"},{"authors":["Nanyun Peng","Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"27421098a0f9e3c451ea02557297cb9c","permalink":"https://rycolab.github.io/publication/pengal-emnlp-15/","publishdate":"2020-04-20T10:10:21.461087Z","relpermalink":"/publication/pengal-emnlp-15/","section":"publication","summary":"We investigate dual decomposition for joint MAP inference of many strings. Given an arbitrary graphical model, we decompose it into small acyclic sub-models, whose MAP configurations can be found by finite-state composition and dynamic programming. We force the solutions of these subproblems to agree on overlapping variables, by tuning Lagrange multipliers for an adaptively expanding set of variable-length n-gram count features.  This is the first inference method for arbitrary graphical models over strings that does not require approximations such as random sampling, message simplification, or a bound on string length. Provided that the inference method terminates, it gives a certificate of global optimality (though MAP inference in our setting is undecidable in general). On our global phonological inference problems, it always terminates, and achieves more accurate results than max-product and sum-product loopy belief propagation.","tags":null,"title":"Dual Decomposition Inference for Graphical Models over Strings","type":"publication"},{"authors":["Thomas M√ºller","Ryan Cotterell","Alexander Fraser","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"307e76fe83844df78b8722e4c4925865","permalink":"https://rycolab.github.io/publication/muelleral-conll-15/","publishdate":"2020-04-20T10:10:21.912734Z","relpermalink":"/publication/muelleral-conll-15/","section":"publication","summary":"We present Lemming, a modular log-linear model that jointly models lemmatization and tagging and supports the integration of arbitrary global features. It is trainable on corpora annotated with gold standard tags and lemmata and does not rely on morphological dictionaries or analyzers. Lemming sets the new state of the art in token-based statistical lemmatization on six languages; e.g., for Czech lemmatization, we reduce the error by 60%, from 4.05 to 1.58. We also give empirical evidence that jointly modeling morphological tags and lemmata is mutually beneficial.","tags":null,"title":"Joint Lemmatization and Morphological Tagging with Lemming","type":"publication"},{"authors":["Ryan Cotterell","Thomas M√ºller","Alexander Fraser","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"457b1282971eef22ec6477186cc6f4c1","permalink":"https://rycolab.github.io/publication/cotterellal-conll-15/","publishdate":"2020-04-20T10:10:22.689806Z","relpermalink":"/publication/cotterellal-conll-15/","section":"publication","summary":"We present labeled morphological segmentation‚Äîan alternative view of morphological processing that unifies several tasks. We introduce a new hierarchy of morphotactic tagsets and Chipmunk, a discriminative morphological segmentation system that, contrary to previous work, explicitly models morphotactics. We show improved performance on three tasks for all six languages: (i) morphological segmentation, (ii) stemming and (iii)  morphological tag classification. For morphological segmentation our method shows absolute improvements of 2-6 points F1 over a strong baseline.","tags":null,"title":"Labeled Morphological Segmentation with Semi-Markov Models","type":"publication"},{"authors":["Ryan Cotterell","Hinrich Sch√ºtze"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"58c9c1b17d1c3c1e7deb1d36d1765e88","permalink":"https://rycolab.github.io/publication/cotterellschuetze-naacl-15/","publishdate":"2020-04-20T10:10:25.459218Z","relpermalink":"/publication/cotterellschuetze-naacl-15/","section":"publication","summary":"Linguistic similarity is multi-faceted. For instance, two words may be similar with respect to semantics, syntax, or morphology inter alia. Continuous word-embeddings have been shown to capture most of these shades of similarity to some degree. This work considers guiding word-embeddings with morphologically annotated data, a form of semi-supervised learning, encouraging the vectors to encode a word‚Äôs morphology, i.e., words close in the embedded space share morphological features. We extend the log-bilinear model to this end and show that indeed our learned embeddings achieve this, using German as a case study.","tags":null,"title":"Morphological Word Embeddings","type":"publication"},{"authors":["Ryan Cotterell","Jason Eisner"],"categories":null,"content":"","date":1435708800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1435708800,"objectID":"db153a927a072c4049d322aa1f4dfc77","permalink":"https://rycolab.github.io/publication/cotterelleisner-naacl-15/","publishdate":"2020-04-20T10:10:24.471872Z","relpermalink":"/publication/cotterelleisner-naacl-15/","section":"publication","summary":"We present penalized expectation propagation (PEP), a novel algorithm for approximate inference in graphical models. Expectation propagation is a variant of loopy belief propagation that keeps messages tractable by projecting them back into a given family of functions. Our extension, PEP, uses a structuredsparsity penalty to encourage simple messages, thus balancing speed and accuracy. We specifically show how to instantiate PEP in the case of string-valued random variables, where we adaptively approximate finite-state distributions by variable-order n-gram models. On phonological inference problems, we obtain substantial speedup over previous related algorithms with no significant loss in accuracy.","tags":null,"title":"Penalized Expectation Propagation for Graphical Models over Strings","type":"publication"},{"authors":["Ryan Cotterell","Nanyun Peng","Jason Eisner"],"categories":null,"content":"","date":1420070400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1420070400,"objectID":"585f26f2d387f8189154c38ea4fd00a6","permalink":"https://rycolab.github.io/publication/cotterellal-tacl-15/","publishdate":"2020-04-20T10:10:23.509194Z","relpermalink":"/publication/cotterellal-tacl-15/","section":"publication","summary":"The observed pronunciations or spellings of words are often explained as arising from the ''underlying forms'' of their morphemes. These forms are latent strings that linguists try to reconstruct by hand. We propose to reconstruct them automatically at scale, enabling generalization to new words. Given some surface word types of a concatenative language along with the abstract morpheme sequences that they express, we show how to recover consistent underlying forms for these morphemes, together with the (stochastic) phonology that maps each concatenation of underlying forms to a surface form. Our technique involves loopy belief propagation in a natural directed graphical model whose variables are unknown strings and whose conditional distributions are encoded as finite-state machines with trainable weights. We define training and evaluation paradigms for the task of surface word prediction, and report results on subsets of 7 languages.","tags":null,"title":"Modeling Word Forms Using Latent Underlying Morphs and Phonology","type":"publication"},{"authors":["Ryan Cotterell","Nanyun Peng","Jason Eisner"],"categories":null,"content":"","date":1401580800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1401580800,"objectID":"98faa9c135408e6a0e6802b0a788931b","permalink":"https://rycolab.github.io/publication/cotterellal-acl-14/","publishdate":"2020-04-20T10:10:54.993291Z","relpermalink":"/publication/cotterellal-acl-14/","section":"publication","summary":"String similarity is most often measured by weighted or unweighted edit distance d(x, y). Ristad and Yianilos (1998) defined stochastic edit distance‚Äîa probability distribution p(y | x) whose parameters can be trained from data. We generalize this so that the probability of choosing each edit operation can depend on contextual features. We show how to construct and train a probabilistic finite-state transducer that computes our stochastic  ontextual edit distance. To illustrate the improvement from conditioning on context, we model typos found in social media text.","tags":null,"title":"Stochastic Contextual Edit Distance and Probabilistic FSTs","type":"publication"},{"authors":null,"categories":null,"content":" Time: Fri 15-16h via Zoom\nReading List   Date\u0026emsp; Presenter(s) Topic Title Authors Bib\u0026emsp;\u0026emsp;     27/11 Clara Mirror Descent Optimizing with constraints: reparametrization and geometry Vlad Niculae    6/11 Sankalan MC-Dropout Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Yarin Gal, Zoubin Ghahramani Cite   30/10 Marinela Pruning The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks Jonathan Frankle, Michael Carbin Cite   23/10 Afra Sampling without Replacement Incremental Sampling Without Replacement for Sequence Models Kensen Shi, David Bieber, Charles Sutton Cite   16/10 Shehzaad Critique of Leaderboards Utility is in the Eye of the User: A Critique of NLP Leaderboards Kawin Ethayarajh, Dan Jurafsky Cite   9/10 Jiaoda Dropout and Pruning Learning Sparse Networks Using Targeted Dropout Aidan N. Gomez, Ivan Zhang, Siddhartha Rao Kamalakara, Divyam Madaan, Kevin Swersky, Yarin Gal, Geoffrey E. Hinton Cite   2/10 Tiago V-Information A Theory of Usable Information under Computational Constraints Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, Stefano Ermon Cite   25/9 Tianyu Coreference Resolution End-to-end Neural Coreference Resolution Kenton Lee, Luheng He, Mike Lewis, Luke Zettlemoyer Cite   18/9 Eleanor Centering Theory Centering: A Framework for Modeling the Local Coherence of Discourse Barbara J. Grosz, Aravind K. Joshi, Scott Weinstein Cite   2/7 - 23/7  Reinforcement Learning Fundamentals of Reinforcement Learning Martha White and Adam White    1/5  Bayesian Non-Parametrics Bayesian density estimation and inference using mixtures Markov chain sampling methods for Dirichlet process mixture models     28/4  NLP Retrospective Experience Grounds Language Climbing towards NLU: On Meaning, Form, and Understanding in the Age of Data     24/4  Bayesian Non-Parametrics Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures     17/4  Bayesian Non-Parametrics Lecture Notes on Bayesian Nonparametrics - Ch.6: Exchangeability     15/4  Probing Information-Theoretic Probing with Minimum Description Length     14/4  Bayesian Non-Parametrics Lecture Notes on Bayesian Nonparametrics - Ch.2: Clustering and the Dirichlet Process Stat 362: Bayesian Nonparametrics - Lecture 1      ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"724bf24aaf13714b41e76aa01f2775a5","permalink":"https://rycolab.github.io/readinggroup/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/readinggroup/","section":"","summary":"Time: Fri 15-16h via Zoom\nReading List   Date\u0026emsp; Presenter(s) Topic Title Authors Bib\u0026emsp;\u0026emsp;     27/11 Clara Mirror Descent Optimizing with constraints: reparametrization and geometry Vlad Niculae    6/11 Sankalan MC-Dropout Dropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning Yarin Gal, Zoubin Ghahramani Cite   30/10 Marinela Pruning The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks Jonathan Frankle, Michael Carbin Cite   23/10 Afra Sampling without Replacement Incremental Sampling Without Replacement for Sequence Models Kensen Shi, David Bieber, Charles Sutton Cite   16/10 Shehzaad Critique of Leaderboards Utility is in the Eye of the User: A Critique of NLP Leaderboards Kawin Ethayarajh, Dan Jurafsky Cite   9/10 Jiaoda Dropout and Pruning Learning Sparse Networks Using Targeted Dropout Aidan N.","tags":null,"title":"ETHZ++ NLP Reading Group","type":"page"},{"authors":null,"categories":null,"content":" Course Description This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.e processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.\nThe objective of the course is to learn the basic concepts in the statistical processing of natural languages. The course will be project-oriented so that the students can also gain hands-on experience with state-of-the-art tools and techniques.\nGrading Marks for the course will be determined by the following formula:\n* 70% Final Exam\n* 30% Course Project/Assignment\nLectures: Mon 12-14h Zoom (link to be emailed and posted on piazza day of lecture)\nDiscussion Sections: Wednesday 13-14h Zoom (link to be emailed and posted on piazza day of discussion)\nTextbooks: Introduction to Natural Language Processing (Eisenstein)\n\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp;\u0026emsp; Deep Learning (Goodfellow, Bengio and Courville)\nNews 31.08 \u0026emsp; Class website is online!\n31.08 \u0026emsp; We are using piazza as our discussion forum. Please enroll here.\n21.09 \u0026emsp; First lecture.\n30.09 \u0026emsp; First discussion section.\n16.10 \u0026emsp; Project guidelines released.\n23.10 \u0026emsp; First part of course assignment released.\n1.11 \u0026emsp;\u0026ensp; Project proposals due for groups electing to do research project (submission instructions to come).\n4.11 \u0026emsp;\u0026ensp; LaTex template for course assignment released.\nSyllabus Disclaimer: This is the first year the class is being taught in this format. It will progress, and may change, as the semester carries on.     Week Date\u0026emsp;\u0026emsp; Topic Slides\u0026emsp;\u0026emsp; Readings Supplementary Material     - 14.09.20 Knabenschiessen (no class)      1 21.09.20 Introduction to Natural Language Lecture 1 Eisenstein Ch. 1    2 28.09.20 Backpropagation Lecture 2   Chris Olah\u0026rsquo;s Blog Justin Domke‚Äôs Notes Tim Vieira‚Äôs Blog Moritz Hardt‚Äôs Notes Baur and Strassen (1983) Griewank and Walter (2008) Eisner (2016) Computation Graph for MLP Computation Graph Example   3 5.10.20  Log-Linear Modeling\u0026mdash;Meet the Softmax Lecture 3 Tutorial Eisenstein Ch. 2 Ferraro and Eisner (2013) Jason Eisner‚Äôs list of further resources on log-linear modeling   4 12.10.20 Sentiment Analysis with Multi-layer Perceptrons Lecture 4 Tutorial Eisenstein Ch. 3 and Ch. 4Goodfellow, Bengio and Courville Ch. 6 Wikipedia Cybenko (1989) Hanin and Selke (2018) Pang and Lee (2008) Iyyer et al. (2015) word2vec Parameter Learning Explained word2vec Explained   5 19.10.20 Language Modeling with n-grams and LSTMs Lecture 5 Tutorial Eisenstein Ch. 6Goodfellow, Bengio and Courville Ch. 10 Good Tutorial on n-gram smoothing Good‚ÄìTuring Smoothing Kneser and Ney (1995) Bengio et al. (2003) Mikolov et al. (2010)   6 26.10.20 Part-of-Speech Tagging with CRFs Lecture 6 Tutorial Eisenstein Ch. 7 and 8 Tim Vieira\u0026rsquo;s Blog McCallum et al. (2000) Lafferty et al. (2001) Sutton and McCallum (2011) Koller and Friedman (2009)   7 2.11.20 Review      8 9.11.20 Class canceled      9 16.11.20 Context-Free Parsing with CKY Lecture 8 Eisenstein Ch. 10 The Inside-Outside Algorithm Jason Eisner‚Äôs Slides Kasami (1966) Younger (1967) Cocke and Schwartz (1970)   10 23.11.20 No Class (NAACL Deadline)      11 30.11.20 Dependency Parsing with the Matrix-Tree Theorem  Eisenstein Ch. 11 Koo et al. (2007) Smith and Smith (2007) McDonald and Satta (2007) McDonald, K√ºbler and Nivre (2009)   12 TBA Transliteration with WFSTs  Eisenstein Ch. 9 Knight and Graehl (1998) Mohri, Pereira and Riley (2008)   12 TBA Machine Translation with Transformers  Eisenstein Ch. 18 Neural Machine Translation Vaswani et al. (2017) Rush (2018)   13 14.12.20 Bias and Fairness in NLP   Bolukabasi et al. (2016) Gonen and Goldberg (2019) Hall Maudslay et al. (2019) Vargas and Cotterell (2020) A Course in Machine Learning Chapter 8 \n \nCourse Project/Assignment Every student has the option of completing either a research project or a structured assignment. The course project/assigment will be worth 30% of your final mark. The project would be an open-ended research project where students reimplement an existing research paper or perform novel research if they are so inclined. Please find the guidelines below. In the assignment, some of the questions would be more theoretical and resemble the questions you will see on the final exam. However, there may also be a large coding portion in the assignment, which would not look like the exam questions. For instance, we may ask you to implement a recurrent neural dependency parser. Please find the first portion of the assignment and the writeup template below.\nAssignments must be completed individually. Projects can be completed in groups of up to 4. If you choose to do the project, we require a proposal no later than November 1, 2020 23:59 CEST. Please see project guidelines for content/formatting instructions; email proposals to Clara (email address below) by the deadline. All projects/assigments will be due at the end of the semester.\nMaterials  Project Guidelines Course Assignment: Part 1 Course Assignment LaTex Template  Contact You can ask questions on piazza. Please post questions there, so others can see them and share in the discussion. If you have questions which are not of general interest, please don\u0026rsquo;t hesitate to contact us directly.\n  Lecturer Ryan Cotterell   Teaching Assistants Clara Meister, Niklas Stoehr, Pinjia He, Rita Kuznetsova    ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"d9362ba48d7ba4daca6705a375cfc5c6","permalink":"https://rycolab.github.io/classes/intro-nlp/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/classes/intro-nlp/","section":"classes","summary":"This course presents topics in natural language processing with an emphasis on modern techniques, primarily focusing on statistical and deep learning approaches. The course provides an overview of the primary areas of research in language processing as well as a detailed exploration of the models and techniques used both in research and in commercial natural language systems.","tags":null,"title":"Natural Language Processing","type":"classes"}]